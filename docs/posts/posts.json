[
  {
    "path": "posts/2021-12-04-cartography-in-r/",
    "title": "Cartography in R",
    "description": "Objectives:   \n1. Learn the (very) basics of map design  \n2. Learn how to load geospatial data  \n3. Learn how to inspect geospatial data  \n4. Plot geospatial data",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "R",
      "Ocean Data",
      "Remote Sensing",
      "Spatial Analysis"
    ],
    "contents": "\r\nLoad the neccessary libraries\r\n\r\n\r\nhide\r\n\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\nlibrary(raster)\r\nlibrary(here)\r\nlibrary(marmap)\r\nlibrary(ggspatial)\r\n\r\n\r\n\r\nLoad bathymetry data NOAA API\r\nUse marmap::getNOAA.bathy() to load New Zealand bathymetry and elevation data directly from NOAA database and convert bathy object to dataframe for ggplot.\r\n\r\n\r\nhide\r\n\r\nnz_df <- marmap::getNOAA.bathy(\r\n  lon1 = 162,\r\n  lon2 = 180,\r\n  lat1 = -33,\r\n  lat2 = -50,\r\n  resolution = 1\r\n) %>% \r\n  marmap::as.raster() %>% \r\n  raster::rasterToPoints() %>% \r\n  base::as.data.frame()%>%\r\n              filter(layer <= 0)\r\n\r\n\r\n\r\nRead in New Zealand coastlines\r\nUse sf::read_sf() to read the shapefile containing the New Zealand coastline.\r\n\r\n\r\nhide\r\n\r\nnz_coast <- sf::read_sf(\r\n    \"data/nz-coastlines-and-islands-polygons-topo-150k.shp\"\r\n  )\r\n\r\n\r\n\r\nPlot New Zealand Coastline\r\nUse ggplot() to plot New Zealand coastline, bathymetric raster, and depth contours.\r\n\r\n\r\nhide\r\n\r\nggplot() +\r\n  geom_raster(data = nz_df,\r\n              aes(x = x, y = y, fill = layer)) +\r\n  geom_contour(data = nz_df,\r\n               aes(x = x, y = y, z = layer), color = \"grey20\", size = .1) +\r\n  geom_sf(data = nz_coast,  fill = \"grey40\", \r\n          color = \"black\", size = .15) +\r\n  coord_sf(xlim = c(162, 180), ylim = c(-50, -33), expand = c(0, 0)) + \r\n  labs(fill = \"Depth (m)\",\r\n       x = \"Longitude\",\r\n       y = \"Latitude\",\r\n       title = \"Bathymetric Map of New Zealand\",\r\n       caption = \r\n\"This map highlights the bathymetric data surrounding the coast of New Zealand.\r\nNote that the scale on map varies by more than 10%, scale bar may be inaccurate.\r\nNA bathymetry values are reflected in white and can be seen in shallow coastal areas.\r\nSource: (A) NOAA National Geophysical Data Center. 2009: ETOPO1 1 Arc-Minute \r\nGlobal Relief Model. NOAA National Centers for Environmental Information. Accessed [2021-10-01]. \r\n(B) New Zealand Coastline shapefile downloaded from https://data.linz.govt.nz/data/. Accessed [2021-10-01].\r\nMap created by Cullen Molitor, Shelby Smith, and Amber McEldowney at the UCSB Bren School [2021-10-04].\") +\r\n  ggspatial::annotation_scale(location = \"br\") +\r\n  ggspatial::annotation_north_arrow(which_north=TRUE, location = \"tl\" ) +\r\n  theme_classic()+\r\n  theme(panel.border = element_rect(fill = NA, size = 2, color = \"black\"),\r\n        plot.caption = element_text(hjust = 0))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-04-cartography-in-r/cartography.png",
    "last_modified": "2021-12-04T15:56:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-04-effect-of-enso-on-coral-reefs/",
    "title": "Effect of ENSO on Coral Reefs",
    "description": "\"Our group discovered the Scott Reef and Rowley Shoals Coral Bleaching Data dataset while searching the DataOne repository for “coral bleaching.” This particular dataset focuses on long-term monitoring data from 1994 to 2017 at reef slope habitats off the coast of northwestern Australia.\"",
    "author": [
      {
        "name": "Cullen Molitor, Desik Somasundaram, Ryan Munnikhuis, and Julia Parish",
        "url": "https://github.com/desik23/eds-213-group-project"
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "R",
      "Ocean Data",
      "Ecology"
    ],
    "contents": "\r\n\r\n\r\nhide\r\n\r\nlibrary(metajam)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(janitor)\r\nlibrary(vegan)\r\nlibrary(zoo)\r\n\r\n\r\n\r\nEDS 213 Group Project\r\nOur group discovered the Scott Reef and Rowley Shoals Coral Bleaching Data dataset while searching the DataOne repository for “coral bleaching.” This particular dataset focuses on long-term monitoring data from 1994 to 2017 at reef slope habitats off the coast of northwestern Australia. Data Source: https://search.dataone.org/view/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fmetadata%2Feml%2Fedi%2F952%2F1. The metadata includes the reef system, date range, taxonomy, habitat type, coral coverage, and dataset methods.\r\nDownload data\r\n\r\n\r\nhide\r\n\r\n# assign data url to access coral data from DataOne then download\r\ndata_url <- \"https://cn.dataone.org/cn/v2/resolve/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F952%2F1%2Ff6212784c45d0a077f2c863868d22c4b\"\r\n# If data is akready up to date, this will throw an error\r\n# error=T in header will move past this issue\r\ndownload_d1_data(data_url, dir_name = \"data\", path = \".\")\r\n\r\n\r\nError in download_d1_data(data_url, dir_name = \"data\", path = \".\"): This dataset has already been downloaded here:\r\n./data\r\nPlease delete or move the folder to download the dataset again.\r\n\r\nLoad and tidy data\r\n\r\n\r\nhide\r\n\r\n# assign data path\r\ndata_path <- \"data\"\r\n# Read in data with metajam\r\ncoral_list <- read_d1_files(data_path)\r\n# Pick out the data read in above and clean it up\r\ncorals <- coral_list$data %>% \r\n  janitor::clean_names() %>% \r\n  dplyr::mutate(year_decimal = format(date_decimal(year_decimal), \"%Y-%m-%d\"),\r\n                month = month(year_decimal),\r\n                year = year(year_decimal)) %>% \r\n  dplyr::rename(date = year_decimal)\r\n# Read in  ONI to be joined to coral data\r\noni <- read.table(\r\n  \"https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/detrend.nino34.ascii.txt\",\r\n  header = T) %>%\r\n  dplyr::mutate(date = as.Date(ISOdate(YR, MON, 1)),\r\n                date_start = as.Date(ISOdate(YR, MON, 1)),\r\n                date_end = lubridate::ceiling_date(date_start, \"month\")) %>%\r\n  dplyr::rename(oni_anomaly = ANOM,\r\n                month = MON,\r\n                year = YR) %>% \r\n  dplyr::select(year, month, oni_anomaly, date_start, date_end) %>% \r\n  dplyr::mutate(roll_3_month_mean = zoo::rollmean(x = oni_anomaly, k = 3, fill = NA, align = \"right\")) %>% \r\n  dplyr::filter(date_start > lubridate::ymd(\"1994-09-01\"))\r\n# Calculate the Simpson's diversity index \r\n# join corals and ONI data\r\ncoral_oni <- corals %>% \r\n  tidyr::pivot_longer(cols = 5:18, names_to = \"species\", values_to = \"cover\") %>% \r\n  dplyr::group_by(system, reef, year, month, location) %>% \r\n  dplyr::summarise(simpson_index = vegan::diversity(cover, index = \"simpson\")) %>% \r\n  dplyr::left_join(oni %>% dplyr::select(-date_start, -date_end)) %>% \r\n  dplyr::mutate(date = lubridate::make_date(year = year, month = month, day = 1))\r\n\r\n\r\n\r\nPlot\r\n\r\n\r\nhide\r\n\r\n# Plot the coral diversity data with ONI color bar\r\nggplot2::ggplot() +\r\n  ggplot2::geom_rect(\r\n    data = oni,\r\n    aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = .3, fill = oni_anomaly)) +\r\n  ggplot2::scale_fill_viridis_c(\r\n    option = \"plasma\",\r\n    guide = guide_colorbar(direction = \"horizontal\", title.position = \"top\",\r\n                           order = 2, barheight = unit(.2, \"cm\"))) +\r\n  ggplot2::scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\", expand = expansion(mult = c(0,0))) +\r\n  ggplot2::geom_line(data = coral_oni, size = 1,\r\n                     aes(x = date, y = simpson_index, color = location)) +\r\n  ggplot2::scale_color_viridis_d() +\r\n  ggplot2::guides(color = guide_legend(order = 1)) +\r\n  ggplot2::labs(fill = \"Oceanic Ni\\u00f1o Index\", color = \"Sites\", \r\n                x = \"Date\", y = \"Simpsons Diversity Index\") +\r\n  ggplot2::theme_minimal()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-04-effect-of-enso-on-coral-reefs/effect-of-enso-on-coral-reefs_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-04T15:38:50-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-04-modeling-enso-and-wave-height/",
    "title": "Modeling ENSO and Wave Height",
    "description": "We are interested in investigating the effect of El Niño on our local oceanographic conditions. We used linear regression models to plot and compare equatorial sea surface temperature anomalies with local Santa Barbara temperature and wave height. Typically temperature anamolies are calcuated from a 30 year base period. Since we only had ~20 years of consistent data, we calculated monthly means since 1997 and used these as base values from which to compare monthly means for each year.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": "https://github.com/mk-waves/mk-waves-tutorial"
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "Python",
      "Ocean Data",
      "Remote Sensing"
    ],
    "contents": "\r\n\r\n↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\r\nClick the binder icon above to begin a code along tutorial. A remote python session will launch in your browser with the tutorial Be patient, sometimes it takes a while for the session to initialize the first few times.\r\nHW3 Making Waves: CDIP Dataset Tutorial and Use Case Examples\r\nEvaluating Data from the Coastal Data Information Program (CDIP)\r\nEDS 220, Fall 2021\r\nAuthors\r\nCullen Molitor, UC Santa Barbara (cullen_molitor@bren.ucsb.edu) https://cullen-molitor.github.io/\r\nJake Eisaguirre, UC Santa Barbara (eisaguirre@bren.ucsb.edu) https://jake-eisaguirrer.github.io/\r\nMarie Rivers, UC Santa Barbars (mrivers@bren.ucsb.edu) https://marierivers.github.io/\r\nTable of Contents\r\n1. Purpose\r\n2. Dataset Description\r\n3. Data Input/Output\r\n4. Metadata Display\r\n5. Basic Visualization\r\n6. Use Case Examples\r\n7. References\r\n\r\nNotebook Purpose\r\nThis notebook was created to provide an introduction to NetCDF4 files and data from the Coastal Data Information Program (CDIP) at the Scripps Institute of Oceanography (SIO). This tutorial is based on a python API which loads netCDF files. We also provide methods for using sea surface temperature and swell height to compare correlations with El Nino/Southern Oscillation (ENSO) behavior at a specific location.\r\nWhile these methods are applicable to other CDIP wave buoy stations, we chose the Harvest Buoy (CDIP site) to get information close to our local Santa Barabra, CA coast. The metadata is documented on the CDIP informational site.\r\n\r\n\r\n\r\n\r\nDataset Description\r\nFile format: NetCDFData retrieval source: CDIP portal and CDIP Python API\r\nOperator: CDIPCollaborators: CleanSeasFunding: California Division of Boating and Waterways (CDBW) & US Army Corps of Engineer (USACE)\r\nData Availability: CDIP data and products are free available for public useLicense: These data may be redistributed and used without restriction\r\nGlobal coverage of stations:\r\nHarvest buoy period of record: October 1991 - present\r\nMeasured wave parameters\r\nHs = wave height, 30-minute average of the ⅓ highest waves at a sensor*\r\nTp = peak period (most common period between consecutive waves)\r\nDp = peak direction (most common direction) *Statistically, highest wave during the measurement period ~1.8 x Hs\r\n\r\n\r\n\r\nTemporal notes\r\nWave calcs use ~30 minute data samples\r\nTime assigned to the data is start time\r\nAll data collected is archived by UTC time\r\nData Quality\r\nhigh quality publically released data excludes all records flagged by quality control procedures\r\nactivity log documents deployments, transmission problems, maintenance issues, and battery/power failures\r\n\r\nDataset Input/Output\r\nImport required packages\r\n\r\n\r\n\r\nimport netCDF4\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport datetime\r\nimport time\r\nimport calendar\r\nimport pandas as pd\r\nfrom matplotlib import gridspec\r\nfrom matplotlib import cm\r\nimport matplotlib as mpl\r\nSet parameters\r\nnames of any directories where data are stored\r\nranges of years over which data are valid\r\nany thresholds or latitude/longitude ranges to be used later (e.g. dimensions of NINO3.4 region, threshold SSTA values for El Nino, etc.)\r\nLink to other buoy stations\r\nstn = '071'\r\nRead in the data\r\nwe use the netCDF4 module to read in the archived buoy data.\r\n# CDIP Archived Dataset URL\r\ndata_url = 'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive/' + stn + 'p1/' + stn + 'p1_historic.nc'\r\ndata = netCDF4.Dataset(data_url)\r\n\r\nDisplay metadata\r\n# return the file type\r\ntype(data)\r\nnetCDF4._netCDF4.Dataset\r\n# return a summary of the dataset\r\ndata.summary\r\n'Directional wave and sea surface temperature measurements collected in situ by Datawell Waverider buoys located near HARVEST, CA from 1991/10/22 to 2019/06/17. This dataset includes publicly-released data only, excluding all records flagged bad by quality control procedures. A total of 372110 wave samples were analyzed for this area, where the water depth is approximately 183 to 549 meters.'\r\n# return all variables included in the dataset\r\nprint(data.variables.keys())\r\ndict_keys(['sourceFilename', 'waveTime', 'waveTimeBounds', 'waveFlagPrimary', 'waveFlagSecondary', 'waveHs', 'waveTp', 'waveTa', 'waveDp', 'wavePeakPSD', 'waveTz', 'waveSourceIndex', 'waveFrequency', 'waveFrequencyBounds', 'waveFrequencyFlagPrimary', 'waveFrequencyFlagSecondary', 'waveBandwidth', 'sstTime', 'sstTimeBounds', 'sstFlagPrimary', 'sstFlagSecondary', 'sstSeaSurfaceTemperature', 'sstSourceIndex', 'sstReferenceTemp', 'gpsTime', 'gpsTimeBounds', 'gpsStatusFlags', 'gpsLatitude', 'gpsLongitude', 'gpsSourceIndex', 'dwrTime', 'dwrTimeBounds', 'dwrSourceIndex', 'dwrBatteryLevel', 'dwrZAccelerometerOffset', 'dwrXAccelerometerOffset', 'dwrYAccelerometerOffset', 'dwrOrientation', 'dwrInclination', 'dwrBatteryWeeksOfLife', 'metaDeployLatitude', 'metaDeployLongitude', 'metaWaterDepth', 'metaDeclination', 'metaStationName', 'metaStationLatitude', 'metaStationLongitude', 'metaPlatform', 'metaInstrumentation', 'metaGridMapping', 'waveEnergyDensity', 'waveMeanDirection', 'waveA1Value', 'waveB1Value', 'waveA2Value', 'waveB2Value', 'waveCheckFactor', 'waveSpread', 'waveM2Value', 'waveN2Value'])\r\nThe Harvest buoy collects data in the following categories:\r\nSignificant Wave Height\r\nSwell Height\r\nSwell Period\r\nSwell Direction\r\nWind Wave Height\r\nWind Wave Period\r\nWind Wave Direction\r\nWave Steepness\r\nAverage Wave Period\r\nWater Temperature\r\nWe are interested in:\r\nSignificant Wave Height (waveHs)\r\nSea Surface Temperatuer (sstSeaSurfaceTemperature)\r\n# learn more about a variable including long name, units, valid min/max values\r\nprint(data['sstSeaSurfaceTemperature'])\r\n<class 'netCDF4._netCDF4.Variable'>\r\nfloat32 sstSeaSurfaceTemperature(sstTime)\r\n    long_name: sea surface temperature\r\n    units: Celsius\r\n    _FillValue: -999.99\r\n    standard_name: sea_surface_temperature\r\n    coordinates: metaStationLatitude metaStationLongitude\r\n    grid_mapping: metaGridMapping\r\n    valid_min: -5.0\r\n    valid_max: 46.15\r\n    ancillary_variables: sstFlagPrimary sstFlagSecondary\r\n    ncei_name: SEA SURFACE TEMPERATURE\r\n    cell_methods: sstTime: point\r\nunlimited dimensions: \r\ncurrent shape = (372015,)\r\nfilling off\r\n# Other Variables\r\n# data.variables\r\n# Hs = data.variables['waveHs']\r\n# Tp = data.variables['waveTp']\r\n# Dp = data.variables['waveDp'] \r\nClean data\r\nExample code found here\r\n# Get SST timestamp variable \r\nsst_time_var = data.variables['sstTime']\r\n\r\n# Get SST variable \r\nsst = data.variables['sstSeaSurfaceTemperature'][:]\r\n\r\n# Get wave height timestamp variable\r\nwave_time_var = data.variables['waveTime']\r\n\r\n# Get wave height variable \r\nwave = data.variables['waveHs'][:]\r\nWe used the cftime Python library for decoding time units and variable values in a netCDF file conforming to the Climate and Forecasting (CF) netCDF conventions.\r\ncftime documentation\r\nFirst we created dataframes for the sea surface temperature and wave data. We then aggregatted this data by month and joined the dataframes to create one dataframe of buoy variables.\r\n# Use num2date on sst_time_var\r\nsst_time = netCDF4.num2date(sst_time_var[:], sst_time_var.units, only_use_cftime_datetimes=False)\r\n# Make an empty pandas dataframe\r\nsst_df = pd.DataFrame()\r\n\r\n# Fill it with SST and the date time it was collected\r\nsst_df['sst'] = sst\r\nsst_df['date_time'] = sst_time\r\n# Make date_time column a pandas date_time\r\nsst_df['date_time'] = pd.to_datetime(sst_df['date_time']) \r\n\r\n# Pull out date from datetime\r\nsst_df['date'] = sst_df['date_time'].dt.date\r\n\r\n# Pull out month from datetime\r\nsst_df['month'] = sst_df['date_time'].dt.month\r\n\r\n# Pull out year from datetime\r\nsst_df['year'] = sst_df['date_time'].dt.year\r\n# Use num2date on wave_time_var\r\nwave_time = netCDF4.num2date(wave_time_var[:], wave_time_var.units, only_use_cftime_datetimes=False)\r\n# Make an empty pandas dataframe\r\nwave_df = pd.DataFrame()\r\n\r\n# Fill it with SST and the date time it was collected\r\nwave_df['wave'] = wave\r\nwave_df['date_time'] = wave_time\r\n# Make date_time column a pandas date_time\r\nwave_df['date_time'] = pd.to_datetime(wave_df['date_time']) \r\n\r\n# Pull out date from datetime\r\nwave_df['date'] = wave_df['date_time'].dt.date\r\n\r\n# Pull out month from datetime\r\nwave_df['month'] = wave_df['date_time'].dt.month\r\n\r\n# Pull out year from datetime\r\nwave_df['year'] = wave_df['date_time'].dt.year\r\n# Inspect data\r\nsst_df.head()\r\nwave_df.head()\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nwave\r\n\r\n\r\ndate_time\r\n\r\n\r\ndate\r\n\r\n\r\nmonth\r\n\r\n\r\nyear\r\n\r\n\r\n0\r\n\r\n\r\n1.50\r\n\r\n\r\n1991-10-22 07:52:00\r\n\r\n\r\n1991-10-22\r\n\r\n\r\n10\r\n\r\n\r\n1991\r\n\r\n\r\n1\r\n\r\n\r\n1.48\r\n\r\n\r\n1991-10-22 08:22:00\r\n\r\n\r\n1991-10-22\r\n\r\n\r\n10\r\n\r\n\r\n1991\r\n\r\n\r\n2\r\n\r\n\r\n1.54\r\n\r\n\r\n1991-10-22 08:52:00\r\n\r\n\r\n1991-10-22\r\n\r\n\r\n10\r\n\r\n\r\n1991\r\n\r\n\r\n3\r\n\r\n\r\n1.56\r\n\r\n\r\n1991-10-22 09:22:00\r\n\r\n\r\n1991-10-22\r\n\r\n\r\n10\r\n\r\n\r\n1991\r\n\r\n\r\n4\r\n\r\n\r\n1.59\r\n\r\n\r\n1991-10-22 09:52:00\r\n\r\n\r\n1991-10-22\r\n\r\n\r\n10\r\n\r\n\r\n1991\r\n\r\n\r\n\r\nVisualize data\r\nWe started by visualizing the full dataset to inform our further analysis. Based on the data gaps shown in the plot above, we decided to begin our analysis at 1997.\r\n# Plot here to see what the data looks like\r\nf, (pHs, pSst) = plt.subplots(2, 1, sharex=True, figsize=(15,10)) \r\npSst.plot(sst_df.date, sst_df.sst, linewidth = 0.5)\r\npHs.plot(wave_df.date, wave_df.wave, linewidth = 0.5)\r\nplt.title(\"Harvest Buoy\", fontsize=30, y = 2.3)\r\npHs.set_ylabel('Wave Height, m', fontsize=18)\r\npSst.set_ylabel('SST, C', fontsize=18)\r\nText(0, 0.5, 'SST, C')\r\npng\r\nUse Cases Examples\r\nObservational Data vs Modeled Data\r\nThe CDIP buoy network is a valuable resource for marine scientists, coastal managers, and mariners. The historic data (observational) is archived and is of great use for understanding past patterns and provides context for understanding the oceanographic conditions of our local ocean. Real time data (observaltional) gives an better idea of what to expect as wave energy moves past the buoy and towards our shorline. It is also incredibly useful for mariners navigating these sometimes harsh and volotile waters. Forecasted data (modeled) is perhaps the most practical use case for mariners, as they not only need to know the current conditions but they need to know how those conditions are changing in order to navigate safely.\r\nCDIP provide what they call ‘nowcasted’ data and ‘forecasted’ data. - Nowcasted provides insight at a higher resolution for the upcoming 6-hour period. These predictions tend to be more accurate and are generally fairly trustworthy. - Forecasted data provide a look further into the future, but with a greater amount of uncertainty\r\nOur interest\r\nWe are interested in investigating the effect of El Niño on our local oceanographic conditions. We used linear regression models to plot and compare equatorial sea surface temperature anomalies with local Santa Barbara temperature and wave height. Typically temperature anamolies are calcuated from a 30 year base period. Since we only had ~20 years of consistent data, we calculated monthly means since 1997 and used these as base values from which to compare monthly means for each year. The results of this analysis are useful to scientist, marine managers, marine conservation groups and coastal communities.\r\nOther General Use Cases\r\nModeling coastal errosion\r\nInvestigating storm frequecny and intensity over time\r\nInvestigating changes in SST over time\r\nDetecting upwelling events\r\nCalculate SST and wave height anomalies\r\n# Filter data to be greater than 1997 to dump missing values\r\nsst_df = sst_df[sst_df['year'] > 1997]\r\n\r\nwave_df = wave_df[wave_df['year'] > 1997]\r\n# Group by date and summarise with mean SST and Wave Height\r\nsst_monthly = sst_df.groupby(['month', 'year']).agg({'sst': 'mean'})\r\n\r\nwave_monthly = wave_df.groupby(['month', 'year']).agg({'wave': 'mean'})\r\n# Inspect Data\r\nprint(sst_monthly.head())\r\nprint(wave_monthly.head())\r\n                  sst\r\nmonth year           \r\n1     1999  13.194914\r\n      2000  13.023686\r\n      2001  13.454234\r\n      2002  12.896236\r\n      2003  14.313378\r\n                wave\r\nmonth year          \r\n1     1999  2.467315\r\n      2000  2.126410\r\n      2001  2.971848\r\n      2002  2.609128\r\n      2003  2.432630\r\n# join monthly sst and wave data into a buoy dataframe\r\nbuoy_df = sst_monthly.join(wave_monthly)\r\nbuoy_df\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\n\r\n\r\nsst\r\n\r\n\r\nwave\r\n\r\n\r\nmonth\r\n\r\n\r\nyear\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n1999\r\n\r\n\r\n13.194914\r\n\r\n\r\n2.467315\r\n\r\n\r\n2000\r\n\r\n\r\n13.023686\r\n\r\n\r\n2.126410\r\n\r\n\r\n2001\r\n\r\n\r\n13.454234\r\n\r\n\r\n2.971848\r\n\r\n\r\n2002\r\n\r\n\r\n12.896236\r\n\r\n\r\n2.609128\r\n\r\n\r\n2003\r\n\r\n\r\n14.313378\r\n\r\n\r\n2.432630\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n12\r\n\r\n\r\n2014\r\n\r\n\r\n16.786358\r\n\r\n\r\n2.756734\r\n\r\n\r\n2015\r\n\r\n\r\n15.884072\r\n\r\n\r\n3.330739\r\n\r\n\r\n2016\r\n\r\n\r\n13.688676\r\n\r\n\r\n2.353851\r\n\r\n\r\n2017\r\n\r\n\r\n15.000236\r\n\r\n\r\n2.022722\r\n\r\n\r\n2018\r\n\r\n\r\n15.179164\r\n\r\n\r\n2.877026\r\n\r\n\r\n252 rows × 2 columns\r\n\r\n\r\n# Next we used `reset_index` to ungroup the data\r\nbuoy_df = buoy_df.reset_index()\r\nbuoy_df\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nmonth\r\n\r\n\r\nyear\r\n\r\n\r\nsst\r\n\r\n\r\nwave\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1999\r\n\r\n\r\n13.194914\r\n\r\n\r\n2.467315\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n2000\r\n\r\n\r\n13.023686\r\n\r\n\r\n2.126410\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2001\r\n\r\n\r\n13.454234\r\n\r\n\r\n2.971848\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n2002\r\n\r\n\r\n12.896236\r\n\r\n\r\n2.609128\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n2003\r\n\r\n\r\n14.313378\r\n\r\n\r\n2.432630\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n247\r\n\r\n\r\n12\r\n\r\n\r\n2014\r\n\r\n\r\n16.786358\r\n\r\n\r\n2.756734\r\n\r\n\r\n248\r\n\r\n\r\n12\r\n\r\n\r\n2015\r\n\r\n\r\n15.884072\r\n\r\n\r\n3.330739\r\n\r\n\r\n249\r\n\r\n\r\n12\r\n\r\n\r\n2016\r\n\r\n\r\n13.688676\r\n\r\n\r\n2.353851\r\n\r\n\r\n250\r\n\r\n\r\n12\r\n\r\n\r\n2017\r\n\r\n\r\n15.000236\r\n\r\n\r\n2.022722\r\n\r\n\r\n251\r\n\r\n\r\n12\r\n\r\n\r\n2018\r\n\r\n\r\n15.179164\r\n\r\n\r\n2.877026\r\n\r\n\r\n252 rows × 4 columns\r\n\r\n\r\n# calculate the anomalies\r\nbuoy_df = buoy_df.groupby(['month', 'year']).agg({'sst': 'mean', 'wave': 'mean'}) - \\\r\n    buoy_df.groupby(['month']).agg({'sst': 'mean', 'wave': 'mean'}) \r\nbuoy_df.head()\r\n\r\n# since this line of code was so long, use used a `\\` to break up the line. You can also split up lines at commas\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\n\r\n\r\nsst\r\n\r\n\r\nwave\r\n\r\n\r\nmonth\r\n\r\n\r\nyear\r\n\r\n\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n1999\r\n\r\n\r\n-0.458099\r\n\r\n\r\n-0.088380\r\n\r\n\r\n2000\r\n\r\n\r\n-0.629327\r\n\r\n\r\n-0.429286\r\n\r\n\r\n2001\r\n\r\n\r\n-0.198779\r\n\r\n\r\n0.416153\r\n\r\n\r\n2002\r\n\r\n\r\n-0.756777\r\n\r\n\r\n0.053433\r\n\r\n\r\n2003\r\n\r\n\r\n0.660365\r\n\r\n\r\n-0.123065\r\n\r\n\r\n# Note: You need to reset the index each time you group the data\r\nbuoy_df = buoy_df.reset_index()\r\nbuoy_df\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nmonth\r\n\r\n\r\nyear\r\n\r\n\r\nsst\r\n\r\n\r\nwave\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1999\r\n\r\n\r\n-0.458099\r\n\r\n\r\n-0.088380\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n2000\r\n\r\n\r\n-0.629327\r\n\r\n\r\n-0.429286\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2001\r\n\r\n\r\n-0.198779\r\n\r\n\r\n0.416153\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n2002\r\n\r\n\r\n-0.756777\r\n\r\n\r\n0.053433\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n2003\r\n\r\n\r\n0.660365\r\n\r\n\r\n-0.123065\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n247\r\n\r\n\r\n12\r\n\r\n\r\n2014\r\n\r\n\r\n2.599428\r\n\r\n\r\n0.120727\r\n\r\n\r\n248\r\n\r\n\r\n12\r\n\r\n\r\n2015\r\n\r\n\r\n1.697143\r\n\r\n\r\n0.694733\r\n\r\n\r\n249\r\n\r\n\r\n12\r\n\r\n\r\n2016\r\n\r\n\r\n-0.498254\r\n\r\n\r\n-0.282156\r\n\r\n\r\n250\r\n\r\n\r\n12\r\n\r\n\r\n2017\r\n\r\n\r\n0.813306\r\n\r\n\r\n-0.613285\r\n\r\n\r\n251\r\n\r\n\r\n12\r\n\r\n\r\n2018\r\n\r\n\r\n0.992234\r\n\r\n\r\n0.241020\r\n\r\n\r\n252 rows × 4 columns\r\n\r\n\r\n# rename columns\r\nbuoy_df = buoy_df.rename(columns = {'sst':'buoy_sst_anom', 'wave':'buoy_wave_anom'})\r\n\r\n# Assign arbitrary day value\r\nbuoy_df = buoy_df.assign(day = 1) \r\n\r\n# set datetime \r\nbuoy_df['date'] = pd.to_datetime(buoy_df[['year', 'month', 'day']])\r\n\r\n# sort values\r\nbuoy_df = buoy_df.sort_values(by=['date'])\r\n\r\n# set the index for plotting\r\nbuoy_df = buoy_df.set_index('date')\r\n# Inspect\r\nbuoy_df.head()\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nmonth\r\n\r\n\r\nyear\r\n\r\n\r\nbuoy_sst_anom\r\n\r\n\r\nbuoy_wave_anom\r\n\r\n\r\nday\r\n\r\n\r\ndate\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1998-03-01\r\n\r\n\r\n3\r\n\r\n\r\n1998\r\n\r\n\r\n1.834283\r\n\r\n\r\n0.105816\r\n\r\n\r\n1\r\n\r\n\r\n1998-04-01\r\n\r\n\r\n4\r\n\r\n\r\n1998\r\n\r\n\r\n0.914924\r\n\r\n\r\n-0.130027\r\n\r\n\r\n1\r\n\r\n\r\n1998-05-01\r\n\r\n\r\n5\r\n\r\n\r\n1998\r\n\r\n\r\n0.674051\r\n\r\n\r\n-0.090042\r\n\r\n\r\n1\r\n\r\n\r\n1998-06-01\r\n\r\n\r\n6\r\n\r\n\r\n1998\r\n\r\n\r\n0.614153\r\n\r\n\r\n-0.300137\r\n\r\n\r\n1\r\n\r\n\r\n1998-07-01\r\n\r\n\r\n7\r\n\r\n\r\n1998\r\n\r\n\r\n1.214789\r\n\r\n\r\n0.132242\r\n\r\n\r\n1\r\n\r\n\r\nRead in ENSO data\r\nERSST5 Extended Reconstructed Sea Surface Temperature (SST) V5Nino Regions\r\npath = \"https://www.cpc.ncep.noaa.gov/data/indices/ersst5.nino.mth.91-20.ascii\"\r\nenso = pd.read_csv(path, sep = '\\s{2,}', engine = 'python')\r\nenso\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nYR\r\n\r\n\r\nMON\r\n\r\n\r\nNINO1+2\r\n\r\n\r\nANOM\r\n\r\n\r\nNINO3\r\n\r\n\r\nANOM.1\r\n\r\n\r\nNINO4\r\n\r\n\r\nANOM.2\r\n\r\n\r\nNINO3.4\r\n\r\n\r\nANOM.3\r\n\r\n\r\n0\r\n\r\n\r\n1950\r\n\r\n\r\n1\r\n\r\n\r\n23.01\r\n\r\n\r\n-1.55\r\n\r\n\r\n23.56\r\n\r\n\r\n-2.10\r\n\r\n\r\n26.94\r\n\r\n\r\n-1.38\r\n\r\n\r\n24.55\r\n\r\n\r\n-1.99\r\n\r\n\r\n1\r\n\r\n\r\n1950\r\n\r\n\r\n2\r\n\r\n\r\n24.32\r\n\r\n\r\n-1.78\r\n\r\n\r\n24.89\r\n\r\n\r\n-1.52\r\n\r\n\r\n26.67\r\n\r\n\r\n-1.53\r\n\r\n\r\n25.06\r\n\r\n\r\n-1.69\r\n\r\n\r\n2\r\n\r\n\r\n1950\r\n\r\n\r\n3\r\n\r\n\r\n25.11\r\n\r\n\r\n-1.38\r\n\r\n\r\n26.36\r\n\r\n\r\n-0.84\r\n\r\n\r\n26.52\r\n\r\n\r\n-1.80\r\n\r\n\r\n25.87\r\n\r\n\r\n-1.42\r\n\r\n\r\n3\r\n\r\n\r\n1950\r\n\r\n\r\n4\r\n\r\n\r\n23.63\r\n\r\n\r\n-1.90\r\n\r\n\r\n26.44\r\n\r\n\r\n-1.14\r\n\r\n\r\n26.90\r\n\r\n\r\n-1.73\r\n\r\n\r\n26.28\r\n\r\n\r\n-1.54\r\n\r\n\r\n4\r\n\r\n\r\n1950\r\n\r\n\r\n5\r\n\r\n\r\n22.68\r\n\r\n\r\n-1.74\r\n\r\n\r\n25.69\r\n\r\n\r\n-1.57\r\n\r\n\r\n27.73\r\n\r\n\r\n-1.18\r\n\r\n\r\n26.18\r\n\r\n\r\n-1.75\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n858\r\n\r\n\r\n2021\r\n\r\n\r\n7\r\n\r\n\r\n21.74\r\n\r\n\r\n-0.22\r\n\r\n\r\n25.49\r\n\r\n\r\n-0.32\r\n\r\n\r\n28.67\r\n\r\n\r\n-0.22\r\n\r\n\r\n26.91\r\n\r\n\r\n-0.38\r\n\r\n\r\n859\r\n\r\n\r\n2021\r\n\r\n\r\n8\r\n\r\n\r\n20.71\r\n\r\n\r\n-0.30\r\n\r\n\r\n24.65\r\n\r\n\r\n-0.47\r\n\r\n\r\n28.53\r\n\r\n\r\n-0.26\r\n\r\n\r\n26.34\r\n\r\n\r\n-0.51\r\n\r\n\r\n860\r\n\r\n\r\n2021\r\n\r\n\r\n9\r\n\r\n\r\n20.05\r\n\r\n\r\n-0.67\r\n\r\n\r\n24.45\r\n\r\n\r\n-0.45\r\n\r\n\r\n28.22\r\n\r\n\r\n-0.54\r\n\r\n\r\n26.16\r\n\r\n\r\n-0.56\r\n\r\n\r\n861\r\n\r\n\r\n2021\r\n\r\n\r\n10\r\n\r\n\r\n20.17\r\n\r\n\r\n-0.84\r\n\r\n\r\n24.21\r\n\r\n\r\n-0.77\r\n\r\n\r\n28.04\r\n\r\n\r\n-0.72\r\n\r\n\r\n25.77\r\n\r\n\r\n-0.95\r\n\r\n\r\n862\r\n\r\n\r\n2021\r\n\r\n\r\n11\r\n\r\n\r\n20.67\r\n\r\n\r\n-0.98\r\n\r\n\r\n24.17\r\n\r\n\r\n-0.93\r\n\r\n\r\n28.01\r\n\r\n\r\n-0.68\r\n\r\n\r\n25.81\r\n\r\n\r\n-0.89\r\n\r\n\r\n863 rows × 10 columns\r\n\r\n\r\n# clean ENSO data \r\nenso = enso.drop(enso.columns[[2, 3, 4, 5, 6, 7, 8]], axis=1)\r\n\r\n# rename columns\r\nenso = enso.rename(columns={\"YR\":\"year\", \"MON\":\"month\", \"ANOM.3\": \"enso_anom\"})\r\nenso\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nyear\r\n\r\n\r\nmonth\r\n\r\n\r\nenso_anom\r\n\r\n\r\n0\r\n\r\n\r\n1950\r\n\r\n\r\n1\r\n\r\n\r\n-1.99\r\n\r\n\r\n1\r\n\r\n\r\n1950\r\n\r\n\r\n2\r\n\r\n\r\n-1.69\r\n\r\n\r\n2\r\n\r\n\r\n1950\r\n\r\n\r\n3\r\n\r\n\r\n-1.42\r\n\r\n\r\n3\r\n\r\n\r\n1950\r\n\r\n\r\n4\r\n\r\n\r\n-1.54\r\n\r\n\r\n4\r\n\r\n\r\n1950\r\n\r\n\r\n5\r\n\r\n\r\n-1.75\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n858\r\n\r\n\r\n2021\r\n\r\n\r\n7\r\n\r\n\r\n-0.38\r\n\r\n\r\n859\r\n\r\n\r\n2021\r\n\r\n\r\n8\r\n\r\n\r\n-0.51\r\n\r\n\r\n860\r\n\r\n\r\n2021\r\n\r\n\r\n9\r\n\r\n\r\n-0.56\r\n\r\n\r\n861\r\n\r\n\r\n2021\r\n\r\n\r\n10\r\n\r\n\r\n-0.95\r\n\r\n\r\n862\r\n\r\n\r\n2021\r\n\r\n\r\n11\r\n\r\n\r\n-0.89\r\n\r\n\r\n863 rows × 3 columns\r\n\r\n\r\nenso = enso.assign(day = 1)\r\nenso['date'] = pd.to_datetime(enso[['year', 'month', 'day']])\r\nPlot ENSO anomaly\r\n# Choose color gradient here: https://matplotlib.org/stable/tutorials/colors/colormaps.html \r\n\r\ncolourmap = cm.get_cmap('magma')\r\n\r\nxx = enso.date\r\nyy =  enso.enso_anom\r\n\r\nplt.figure(figsize = (15, 5))\r\nplt.plot(xx,yy, color = 'none')\r\n\r\nnormalize = mpl.colors.Normalize(vmin=yy.min(), vmax=yy.max())\r\nnpts = len(enso)\r\nfor i in range(npts - 1):\r\n    plt.fill_between([xx[i], xx[i+1]],\r\n                     [yy[i], yy[i+1]],\r\n                     color=colourmap(normalize(yy[i]))\r\n                     ,alpha=0.6)\r\nplt.show()\r\npngJoining ENSO and buoy data\r\nbuoy_df = buoy_df.drop([\"month\", \"year\", \"day\"], axis=1).reset_index()\r\nenso = enso.drop([\"year\", \"month\", \"day\"], axis = 1)\r\nanom_df = buoy_df.set_index('date').join(enso.set_index('date')).reset_index()\r\n# Plot sst and wave height anomaly from buoy along with ENSO sst anomaly\r\nf, (bwa, bsa, esa ) = plt.subplots(3, 1, sharex=True, figsize=(15,10))\r\n\r\nbwa.plot(anom_df.date, anom_df.buoy_wave_anom)\r\nbwa.fill_between(anom_df.date, anom_df.buoy_wave_anom, 0, alpha=0.30)\r\nbwa.axhline(0,color='red')\r\nbwa.set_ylabel('Buoy Wave Height Anomaly')\r\n\r\nbsa.plot(anom_df.date, anom_df.buoy_sst_anom)\r\nbsa.fill_between(anom_df.date, anom_df.buoy_sst_anom,  0, alpha=0.30)\r\nbsa.axhline(0,color='red')\r\nbsa.set_ylabel('Buoy SST Anomaly')\r\n\r\nesa.plot(anom_df.date, anom_df.enso_anom)\r\nesa.fill_between(anom_df.date, anom_df.enso_anom,  0, alpha=0.30)\r\nesa.axhline(0,color='red')\r\nesa.set_ylabel('ENSO Anomaly')\r\n\r\nText(0, 0.5, 'ENSO Anomaly')\r\npngModel\r\nimport statsmodels.api as sm\r\n# Simple linear model: buoy sst anomaly vs enso sst anomaly\r\nmodel = sm.OLS(anom_df.buoy_sst_anom, anom_df.enso_anom)\r\n\r\n# Model results\r\nresults = model.fit()\r\n\r\n# Model summary\r\nprint(results.summary())\r\n                                 OLS Regression Results                                \r\n=======================================================================================\r\nDep. Variable:          buoy_sst_anom   R-squared (uncentered):                   0.282\r\nModel:                            OLS   Adj. R-squared (uncentered):              0.279\r\nMethod:                 Least Squares   F-statistic:                              98.58\r\nDate:                Sat, 04 Dec 2021   Prob (F-statistic):                    8.24e-20\r\nTime:                        11:29:08   Log-Likelihood:                         -312.83\r\nNo. Observations:                 252   AIC:                                      627.7\r\nDf Residuals:                     251   BIC:                                      631.2\r\nDf Model:                           1                                                  \r\nCovariance Type:            nonrobust                                                  \r\n==============================================================================\r\n                 coef    std err          t      P>|t|      [0.025      0.975]\r\n------------------------------------------------------------------------------\r\nenso_anom      0.5964      0.060      9.929      0.000       0.478       0.715\r\n==============================================================================\r\nOmnibus:                       11.182   Durbin-Watson:                   0.808\r\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               12.718\r\nSkew:                           0.402   Prob(JB):                      0.00173\r\nKurtosis:                       3.751   Cond. No.                         1.00\r\n==============================================================================\r\n\r\nNotes:\r\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\r\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\r\n# Simple linear model: buoy wave anomaly vs enso sst anomaly\r\nmodel = sm.OLS(anom_df.buoy_wave_anom, anom_df.enso_anom)\r\n\r\n# Model results\r\nresults = model.fit()\r\n\r\n# Model summary\r\nprint(results.summary())\r\n                                 OLS Regression Results                                \r\n=======================================================================================\r\nDep. Variable:         buoy_wave_anom   R-squared (uncentered):                   0.000\r\nModel:                            OLS   Adj. R-squared (uncentered):             -0.004\r\nMethod:                 Least Squares   F-statistic:                           0.006389\r\nDate:                Sat, 04 Dec 2021   Prob (F-statistic):                       0.936\r\nTime:                        11:29:08   Log-Likelihood:                         -11.128\r\nNo. Observations:                 252   AIC:                                      24.26\r\nDf Residuals:                     251   BIC:                                      27.79\r\nDf Model:                           1                                                  \r\nCovariance Type:            nonrobust                                                  \r\n==============================================================================\r\n                 coef    std err          t      P>|t|      [0.025      0.975]\r\n------------------------------------------------------------------------------\r\nenso_anom      0.0015      0.018      0.080      0.936      -0.034       0.037\r\n==============================================================================\r\nOmnibus:                        1.416   Durbin-Watson:                   1.619\r\nProb(Omnibus):                  0.493   Jarque-Bera (JB):                1.162\r\nSkew:                           0.011   Prob(JB):                        0.559\r\nKurtosis:                       3.332   Cond. No.                         1.00\r\n==============================================================================\r\n\r\nNotes:\r\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\r\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\r\n# Simple linear model: buoy wave anomaly vs buoy sst anomaly\r\nmodel = sm.OLS(anom_df.buoy_wave_anom, anom_df.buoy_sst_anom)\r\n\r\n# Model results\r\nresults = model.fit()\r\n\r\n# Model summary\r\nprint(results.summary())\r\n                                 OLS Regression Results                                \r\n=======================================================================================\r\nDep. Variable:         buoy_wave_anom   R-squared (uncentered):                   0.035\r\nModel:                            OLS   Adj. R-squared (uncentered):              0.031\r\nMethod:                 Least Squares   F-statistic:                              9.167\r\nDate:                Sat, 04 Dec 2021   Prob (F-statistic):                     0.00272\r\nTime:                        11:29:08   Log-Likelihood:                         -6.6118\r\nNo. Observations:                 252   AIC:                                      15.22\r\nDf Residuals:                     251   BIC:                                      18.75\r\nDf Model:                           1                                                  \r\nCovariance Type:            nonrobust                                                  \r\n=================================================================================\r\n                    coef    std err          t      P>|t|      [0.025      0.975]\r\n---------------------------------------------------------------------------------\r\nbuoy_sst_anom    -0.0480      0.016     -3.028      0.003      -0.079      -0.017\r\n==============================================================================\r\nOmnibus:                        2.296   Durbin-Watson:                   1.645\r\nProb(Omnibus):                  0.317   Jarque-Bera (JB):                2.133\r\nSkew:                           0.093   Prob(JB):                        0.344\r\nKurtosis:                       3.410   Cond. No.                         1.00\r\n==============================================================================\r\n\r\nNotes:\r\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\r\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\r\nVisualize model results\r\nm, b = np.polyfit(anom_df.enso_anom, anom_df.buoy_sst_anom, 1)\r\nplt.figure(figsize = (10, 5))\r\nplt.scatter(anom_df.enso_anom, anom_df.buoy_sst_anom)\r\nplt.plot(anom_df.enso_anom, m * anom_df.enso_anom + b, color = 'red')\r\nplt.title('Buoy SST Anomaly ~ ENSO SST Anomaly')\r\nplt.ylabel('Buoy SST Anomaly')\r\nplt.xlabel('ENSO SST Anomaly')\r\nText(0.5, 0, 'ENSO SST Anomaly')\r\npngm, b = np.polyfit(anom_df.enso_anom, anom_df.buoy_wave_anom, 1)\r\nplt.figure(figsize = (10, 5))\r\nplt.scatter(anom_df.enso_anom, anom_df.buoy_wave_anom)\r\nplt.plot(anom_df.enso_anom, m * anom_df.enso_anom + b, color = 'red')\r\nplt.title('Buoy Wave Anomaly ~ ENSO SST Anomaly')\r\nplt.ylabel('Buoy Wave Anomaly')\r\nplt.xlabel('ENSO SST Anomaly')\r\nText(0.5, 0, 'ENSO SST Anomaly')\r\npngm, b = np.polyfit(anom_df.buoy_sst_anom, anom_df.buoy_wave_anom, 1)\r\nplt.figure(figsize = (10, 5))\r\nplt.scatter(anom_df.buoy_sst_anom, anom_df.buoy_wave_anom)\r\nplt.plot(anom_df.buoy_sst_anom, m * anom_df.buoy_sst_anom + b, color = 'red')\r\nplt.title('Buoy Wave Anomaly ~ Buoy SST Anomaly')\r\nplt.ylabel('Buoy Wave Anomaly')\r\nplt.xlabel('Buoy SST Anomaly')\r\nText(0.5, 0, 'Buoy SST Anomaly')\r\npngResults\r\nOur model shows a positive correlation between ENSO sea surface temperature anomaly and Santa Barbara sea surface temperature anomaly. Buoy wave anomaly only has a slight positive correlation with ENSO sea surface temperature anomaly.\r\nFuture Work\r\nFurther analyses with these datasets could look at correlations on a larger spatial scale by comparing results for other buoy stations closer to and further from the equator than Santa Barbara. The CDIP datasets can also be used to model the effect of ENSO on wave direction.\r\n\r\nReferences\r\nReferences used to create this tutorial include:\r\nCoastal Data Information Program (CDIP): https://cdip.ucsd.edu/\r\nCoastal Data Information Program (CDIP) data access documentation: https://cdip.ucsd.edu/m/documents/data_access.html\r\nHarvest bouy, station 071: https://cdip.ucsd.edu/m/products/?stn=071p1\r\nData citation: Data furnished by the Coastal Data Information Program (CDIP), Integrative Oceanography Division, operated by the Scripps Institution of Oceanography, under the sponsorship of the U.S. Army Corps of Engineers and the California Department of Parks and Recreation\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-04-modeling-enso-and-wave-height/output_52_0.png",
    "last_modified": "2021-12-04T11:51:08-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-04-precipitation-and-ndvi/",
    "title": "Precipitation and NDVI",
    "description": "Using Google Earth Engine to work with satellite imagery. We find the total precipitation for a coastal region in Northern Califnia as well as calculate the normalized difference vegetation index (NDVI), a simple graphical indicator that can be used to analyze remote sensing measurements. We use ERA5 Daily Aggregates data for precipitation and Landsat 8 data for NDVI.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "Python",
      "Spatial Analysis",
      "Google Earth Engine",
      "Remote Sensing"
    ],
    "contents": "\r\nHomework 1, EDS 220\r\nFun With Data Manipulation in Google Earth Engine\r\nThis template provides the instructions for EDS 220 HW1, and some initial environment loading/package configuration commands which you are welcome to modify as needed.\r\nYou will need to supply your own code for this assignment! Feel free to cut and paste relevant lines from the course materials or elsewhere online, that’s what coding is all about.\r\nWe’ll assume that you need to load the Google Earth Engine package, and will also include several other common Python packages: - pandas - matplotlib (the plotting functionality “matplotlib.pyplot” to be precise) - numpy\r\n# Import packages\r\nimport ee\r\nimport geemap\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom IPython.display import Image\r\n1. Choose A Study Region\r\nThink about places on Earth that you’re interested in. This could be anywhere: Santa Barbara, your hometown, a place you always wanted to visit, you name it.\r\nFor that place, then consider what it is like. Is it urban? Rural? Temperate? Tropical? Write a brief (1-2 paragraph) description of the local climate and population (if applicable).\r\nFor this project I am choosing Arcata California. This is where I attended college, and I chose to go there for it’s rural location and its proximity to a variety of outdoor recreation areas. Arcata is a situated along the northern coast of California. The coastal zone surrounding the town includes beautiful coast redwoods (Sequoia sempervirens), Humboldt Bay, and low-lying areas around Humboldt Bay. Going east from the coastal zone, you quickly gain elevation and enter the southern Cascade mountain range and the Trinity Alps. This region contains the volcanoes Mt. Shasta and Mt Lassen, formed by the Cascadia subduction zone. Arcata’s climate falls into the temperate rainforest category, and is dominated by a rainy season and a dry season. Arcata’s population is also somewhat seasonal thanks to the college population leaving for the summer months.\r\n2. Locate A Precipitation Dataset\r\nLook through the Google Earth Engine data catalog: https://developers.google.com/earth-engine/datasets\r\nTo keep things simple (ish), we’ll stick with precipitation as our major climate variable. Precip is nice because there are lots of interesting spatial patterns that are easy to see without doing too much data processing!\r\nChoose one of the precipitation datasets available in GEE, any one you like. Explain briefly (1-2 paragraphs) why you chose it, and what you think its major advantages/properties might be and how they compare with other available data products. You can also explore the metadata associated with your dataset using the print() and getInfo() methods, or viewing on the description page for your dataset. See the example ERA5 analysis code provided in this repo for more details!\r\nI chose the the ERA5 Daily Aggregates dataset because my other choice Global Precipitation Measurement (GPM) v6 kept exceeding my user memory limit. I also investigated many other datasets (test_1 to test_5 files) but I did not really like how a few of them looked in comparison. I think the ERA5 Daily Aggregates suited my needs quite well and showed the scope of the storm event I was looking at quite well. It feels a bit like cheating, but I really did try to use other datasets. This dataset also gives reasonable spatial resolution.\r\nee.Initialize()\r\ngdat = ee.ImageCollection('ECMWF/ERA5/DAILY')\r\ngdat_prop = gdat.propertyNames().getInfo()\r\npr = gdat.select('total_precipitation')\r\nprdflt = pr.filter(ee.Filter.date('2019-02-24', '2019-02-28')).sum();\r\nprdflt.getInfo()\r\n{'type': 'Image',\r\n 'bands': [{'id': 'total_precipitation',\r\n   'data_type': {'type': 'PixelType', 'precision': 'double'},\r\n   'crs': 'EPSG:4326',\r\n   'crs_transform': [1, 0, 0, 0, 1, 0]}]}\r\n3. Plot a Time Series of Precipitation\r\nLet’s do a new type of analysis on the data: the creation of a time series. The “ExampleCommands_HW1” notebook should give you a starting point for how to do this - pull out data for your region of interest, and plot a time series.\r\nar_lon = -124\r\nar_lat = 40.8\r\nar_poi = ee.Geometry.Point(ar_lon, ar_lat)\r\nscale = 1000   # scale in m\r\nar_pr_ts = pr.getRegion(ar_poi, scale).getInfo()\r\ndf = pd.DataFrame(ar_pr_ts)\r\n\r\nprint(df)\r\n              0          1          2              3                    4\r\n0            id  longitude   latitude           time  total_precipitation\r\n1      19790102 -123.99895  40.796989   284083200000                    0\r\n2      19790103 -123.99895  40.796989   284169600000                    0\r\n3      19790104 -123.99895  40.796989   284256000000             0.002583\r\n4      19790105 -123.99895  40.796989   284342400000                    0\r\n...         ...        ...        ...            ...                  ...\r\n15161  20200705 -123.99895  40.796989  1593907200000             0.000057\r\n15162  20200706 -123.99895  40.796989  1593993600000             0.000298\r\n15163  20200707 -123.99895  40.796989  1594080000000             0.000005\r\n15164  20200708 -123.99895  40.796989  1594166400000              0.00015\r\n15165  20200709 -123.99895  40.796989  1594252800000             0.000015\r\n\r\n[15166 rows x 5 columns]\r\nheaders = df.loc[0]     # Assign the first entry in the data frame to a variable called \"headers\"\r\nprint(headers) \r\n0                     id\r\n1              longitude\r\n2               latitude\r\n3                   time\r\n4    total_precipitation\r\nName: 0, dtype: object\r\ndf = pd.DataFrame(df.values[1:], columns=headers)      \r\n# Make a new data frame out of the old one, but assigning the names we just retrieved as actual column headers\r\nprint(df)  \r\n0            id  longitude   latitude           time total_precipitation\r\n0      19790102 -123.99895  40.796989   284083200000                   0\r\n1      19790103 -123.99895  40.796989   284169600000                   0\r\n2      19790104 -123.99895  40.796989   284256000000            0.002583\r\n3      19790105 -123.99895  40.796989   284342400000                   0\r\n4      19790106 -123.99895  40.796989   284428800000                   0\r\n...         ...        ...        ...            ...                 ...\r\n15160  20200705 -123.99895  40.796989  1593907200000            0.000057\r\n15161  20200706 -123.99895  40.796989  1593993600000            0.000298\r\n15162  20200707 -123.99895  40.796989  1594080000000            0.000005\r\n15163  20200708 -123.99895  40.796989  1594166400000             0.00015\r\n15164  20200709 -123.99895  40.796989  1594252800000            0.000015\r\n\r\n[15165 rows x 5 columns]\r\ndf['datetime'] = pd.to_datetime(df['time'], unit = 'ms')\r\ndf\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nid\r\n\r\n\r\nlongitude\r\n\r\n\r\nlatitude\r\n\r\n\r\ntime\r\n\r\n\r\ntotal_precipitation\r\n\r\n\r\ndatetime\r\n\r\n\r\n0\r\n\r\n\r\n19790102\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n284083200000\r\n\r\n\r\n0\r\n\r\n\r\n1979-01-02\r\n\r\n\r\n1\r\n\r\n\r\n19790103\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n284169600000\r\n\r\n\r\n0\r\n\r\n\r\n1979-01-03\r\n\r\n\r\n2\r\n\r\n\r\n19790104\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n284256000000\r\n\r\n\r\n0.002583\r\n\r\n\r\n1979-01-04\r\n\r\n\r\n3\r\n\r\n\r\n19790105\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n284342400000\r\n\r\n\r\n0\r\n\r\n\r\n1979-01-05\r\n\r\n\r\n4\r\n\r\n\r\n19790106\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n284428800000\r\n\r\n\r\n0\r\n\r\n\r\n1979-01-06\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n15160\r\n\r\n\r\n20200705\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n1593907200000\r\n\r\n\r\n0.000057\r\n\r\n\r\n2020-07-05\r\n\r\n\r\n15161\r\n\r\n\r\n20200706\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n1593993600000\r\n\r\n\r\n0.000298\r\n\r\n\r\n2020-07-06\r\n\r\n\r\n15162\r\n\r\n\r\n20200707\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n1594080000000\r\n\r\n\r\n0.000005\r\n\r\n\r\n2020-07-07\r\n\r\n\r\n15163\r\n\r\n\r\n20200708\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n1594166400000\r\n\r\n\r\n0.00015\r\n\r\n\r\n2020-07-08\r\n\r\n\r\n15164\r\n\r\n\r\n20200709\r\n\r\n\r\n-123.99895\r\n\r\n\r\n40.796989\r\n\r\n\r\n1594252800000\r\n\r\n\r\n0.000015\r\n\r\n\r\n2020-07-09\r\n\r\n\r\n15165 rows × 6 columns\r\n\r\n\r\nplt.figure(figsize = (10, 6), dpi = 300)    # create a new figure, set size and resolution (dpi)\r\nplt.plot(df['datetime'],df['total_precipitation'])   # add data to the plot\r\nplt.title('Arcata Rainfall', fontsize=16)\r\nplt.xlabel('Date', fontsize=14)\r\nplt.ylabel('Precip (m)', fontsize=14)\r\nplt.ylim(0, 0.12)\r\n(0.0, 0.12)\r\npng4. Make Some Maps of Interesting Times\r\nBased on the time series of precipitation you just created, look for a few interesting time periods. This could be days when extreme storms happened, or seasons (or years) that were above or below average precipitation.\r\nExplain briefly why you chose these particular time periods.\r\nI started by looking at local river gauges to try to find a storm event. I was able to see a recent crest of the Mad River by looking here https://water.weather.gov/ahps2/hydrograph.php?gage=arcc1&wfo=eka.\r\nUsing the GEE Map method that we worked on in class, plot maps of those time periods. Things to think about:\r\nchoosing an appropriate zoom level to highlight the precipitation features you’re interested in\r\nsetting appropriate min/max ranges for your plot, to allow the major features of regional precipitation to be easily visualized\r\nDescribe briefly what is interesting about your maps!\r\nI chose to take the sum of the time period I examined in order to look at how much precipitation there was over the Mad River watershed. The sum of the values also lets you see the total amount of precipitation over the entire time period of the storm event. My map shows heavy precipitation over the Pacific, making landfall in northern California and Oregon, while staying to the north and largely avoiding Nevada.\r\n# Base map\r\nMap = geemap.Map(center=[40.8,-124], zoom=6)\r\n# Min and Max\r\nVIS_PREC = {\r\n    'min':0,\r\n    'max':.02,\r\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']\r\n}\r\n# Add layer\r\nMap.addLayer(prdflt, VIS_PREC,'total precipitation',opacity=0.3)\r\n# Plot map\r\nMap\r\nMap(center=[40.8, -124], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=…\r\npt = ee.Geometry.Point([-124.3652, 40.2922])\r\nroi = pt.buffer(1600000)\r\n# Create a URL to the styled image for a region around Mendocino County CA.\r\nurl = prdflt.getThumbUrl({\r\n    'min': 0, 'max': .02, 'dimensions': 512, 'region': roi, 'opacity': 0.3,\r\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']})\r\nImage(url=url, embed=True, format = 'png')\r\npng5. Plot Landsat NDVI for the Region\r\nThe precipitation analysis discussed above provides useful context for identifying interesting weather events for a particular region. Now let’s see what (if any!) effect changes in weather patterns have had on the landscape, using Landsat imagery.\r\nMake a long-term average NDVI map for your region: say, a 10- or 20-year mean. This will give you a baseline to compare against.\r\nMake an NDVI map centered on one of your ‘interesting’ time periods from the precipitation time series. How are the patterns different from the long-term average? And how do you think this relates to precipitation during this time period?\r\nI was able to see a slight difference in the two images. It is unclear to me if the long term average includes more cloud cover and is therfore less bright/vivid, or if the rain increased the vegetation. If I had to guess I would say that there was not a very noticable change because I chose an area that is already very wet and productive.\r\nNote I was not able to run my last bit until 10/18 (thanks to Desik’s suggestion). I therfore only answered this on 10/18 as well.\r\nland_8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA')\r\npt = ee.Geometry.Point([-124, 40.8])   # Point corresponding to Arcata, CA\r\nland_8_pt = land_8.filterBounds(pt)\r\nland_8_least_cld = land_8_pt.filter('CLOUD_COVER < 20').mean();\r\nred = land_8_least_cld.select('B4')\r\nnir = land_8_least_cld.select('B5')\r\n\r\nndvi=(nir.subtract(red)).divide((nir.add(red))).rename('NDVI')\r\n\r\nndvi\r\n<ee.image.Image at 0x22d22b1b280>\r\nndviParams = {'min': -1, \r\n              'max': 1, \r\n              'palette': ['blue', 'white', 'green']\r\n             }\r\n#  Interactive only, no saved output\r\nMap_NDVI = geemap.Map(center=[40.8, -124], zoom=8)\r\nMap_NDVI.addLayer(ndvi, ndviParams,'NDVI')\r\nMap_NDVI\r\nMap(center=[40.8, -124], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=…\r\npt = ee.Geometry.Point([-124.3652, 40.2922])\r\nroi = pt.buffer(120000)\r\n# Create a URL to the styled image for a region around Mendocino County CA.\r\nurl = ndvi.getThumbUrl({\r\n    'min': -1, 'max': 1, 'dimensions': 512, 'region': roi,\r\n    'palette': ['blue', 'white', 'green']})\r\nImage(url=url, embed=True, format = 'png')\r\npngland_8_least_cld_flt = land_8_pt.filter('CLOUD_COVER < 20')\r\nland_8_least_cld_flt_dt = land_8_least_cld_flt.filter(ee.Filter.date('2019-01-01', '2019-04-28')).mean();\r\nred_2 = land_8_least_cld_flt_dt.select('B4')\r\nnir_2 = land_8_least_cld_flt_dt.select('B5')\r\n\r\nndvi_2 = (nir_2.subtract(red_2)).divide((nir_2.add(red_2))).rename('NDVI')\r\n\r\nndvi_2\r\n<ee.image.Image at 0x22d220f6820>\r\n#  Interactive only, no saved output\r\nMap_NDVI_flt = geemap.Map(center=[40.8, -124], zoom=8)\r\nMap_NDVI_flt.addLayer(ndvi_2, ndviParams,'NDVI')\r\nMap_NDVI_flt\r\nMap(center=[40.8, -124], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=…\r\npt = ee.Geometry.Point([-124.3652, 40.2922])\r\nroi = pt.buffer(120000)\r\n# Create a URL to the styled image for a region around Mendocino County CA.\r\nurl = ndvi_2.getThumbUrl({\r\n    'min': -1, 'max': 1, 'dimensions': 512, 'region': roi,\r\n    'palette': ['blue', 'white', 'green']})\r\nImage(url=url, embed=True, format = 'png')\r\npng\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-04-precipitation-and-ndvi/output_42_0.png",
    "last_modified": "2021-12-04T15:25:29-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-03-houston-blackout-analysis-2021/",
    "title": "Houston Blackout Analysis (2021)",
    "description": "This analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite.The task is to answer the questions: How many residential buildings were without power on 2021-02-16? Is there a socioeconomic metric that predicts being affected by the power outage?",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "R",
      "Remote Sensing",
      "Spatial Analysis"
    ],
    "contents": "\r\nLoad libraries\r\n\r\n\r\nhide\r\n\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\nlibrary(here)\r\nlibrary(stars)\r\nlibrary(rosm)\r\nlibrary(tmap)\r\n\r\n\r\n\r\nFunction to load the DNB dataset from VNP46A1 granules\r\n\r\n\r\nhide\r\n\r\nread_dnb <- function(file_name) {\r\n  # Reads the \"DNB_At_Sensor_Radiance_500m\" dataset from a VNP46A1 granule into a STARS object.\r\n  # Then read the sinolsoidal tile x/y positions and adjust the STARS dimensions (extent+delta)\r\n\r\n  # The name of the dataset holding the nightlight band in the granule\r\n  dataset_name <- \"//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m\"\r\n\r\n  # From the metadata, we pull out a string containing the horizontal and vertical tile index\r\n  h_string <- gdal_metadata(file_name)[199]\r\n  v_string <- gdal_metadata(file_name)[219]\r\n  \r\n  # We parse the h/v string to pull out the integer number of h and v\r\n  tile_h <- as.integer(str_split(h_string, \"=\", simplify = TRUE)[[2]])\r\n  tile_v <- as.integer(str_split(v_string, \"=\", simplify = TRUE)[[2]])\r\n\r\n  # From the h/v tile grid position, we get the offset and the extent\r\n  west <- (10 * tile_h) - 180\r\n  north <- 90 - (10 * tile_v)\r\n  east <- west + 10\r\n  south <- north - 10\r\n\r\n  # A tile is 10 degrees and has 2400x2400 grid cells\r\n  delta <- 10 / 2400\r\n\r\n  # Reading the dataset\r\n  dnb <- read_stars(file_name, sub = dataset_name)\r\n\r\n  # Setting the CRS and applying offsets and deltas\r\n  st_crs(dnb) <- st_crs(4326)\r\n  st_dimensions(dnb)$x$delta <- delta\r\n  st_dimensions(dnb)$x$offset <- west\r\n  st_dimensions(dnb)$y$delta <- -delta\r\n  st_dimensions(dnb)$y$offset <- north\r\n  \r\n  return(dnb)\r\n}\r\n\r\n\r\n\r\nRead in day night band (DNB) data\r\n\r\n\r\nhide\r\n\r\nFeb_07_v5 <- read_dnb(file_name = \"data/VNP46A1.A2021038.h08v05.001.2021039064328.h5\")\r\n\r\n\r\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \r\n\r\nhide\r\n\r\nFeb_07_v6 <- read_dnb(file_name = \"data/VNP46A1.A2021038.h08v06.001.2021039064329.h5\")\r\n\r\n\r\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \r\n\r\nhide\r\n\r\nFeb_16_v5 <- read_dnb(file_name = \"data/VNP46A1.A2021047.h08v05.001.2021048091106.h5\")\r\n\r\n\r\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \r\n\r\nhide\r\n\r\nFeb_16_v6 <- read_dnb(file_name = \"data/VNP46A1.A2021047.h08v06.001.2021048091105.h5\")\r\n\r\n\r\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \r\n\r\nCombine adjacent tiles for each date\r\n\r\n\r\nhide\r\n\r\nFeb_07_v5_v6 <- st_mosaic(Feb_07_v5, Feb_07_v6)\r\nFeb_16_v5_v6 <- st_mosaic(Feb_16_v5, Feb_16_v6)\r\n\r\n\r\n\r\nCreate blackout Mask\r\n\r\n\r\nhide\r\n\r\n## Take the difference of light data from before the storm and after the storm to make a mask of values with a difference greater than 200 nW cm-2 sr-1\r\ndiff <- (Feb_07_v5_v6 - Feb_16_v5_v6) > 200\r\n\r\n## Convert values with a difference of less than 200 to NA\r\ndiff[diff == F] <- NA\r\n\r\n\r\n\r\nVectorize blackout mask\r\n\r\n\r\nhide\r\n\r\nblackout_mask <- st_as_sf(diff)\r\n\r\n## Fix invalid geometries \r\nblackout_mask_fixed <- st_make_valid(blackout_mask)\r\n\r\nrm(diff, blackout_mask)\r\ngc()\r\n\r\n\r\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\r\nNcells  3047944 162.8    4503170  240.5   4503170  240.5\r\nVcells 27482570 209.7  132454492 1010.6 156217946 1191.9\r\n\r\nCrop the vectorized blackout mask to the region of interest\r\n\r\n\r\nhide\r\n\r\n## Set region of interest\r\nhouston <- st_polygon(\r\n  list(\r\n    rbind(\r\n      c(-96.5, 29), \r\n      c(-96.5, 30.5), \r\n      c(-94.5, 30.5), \r\n      c(-94.5, 29), \r\n      c(-96.5, 29)\r\n    )\r\n  )\r\n) %>% \r\n  st_sfc(crs = 4326)\r\n\r\n## Crop night lights data\r\nintersects <- st_intersects(blackout_mask_fixed, houston, sparse = FALSE)\r\nblackout_cropped <- blackout_mask_fixed[intersects,]\r\n\r\n## Transform cropped blackout mask back to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\r\nblackout_cropped_NAD83 <- st_transform(blackout_cropped, 3083)\r\n\r\nrm(blackout_cropped)\r\ngc()\r\n\r\n\r\n           used  (Mb) gc trigger  (Mb)  max used   (Mb)\r\nNcells  3020703 161.4    4503170 240.5   4503170  240.5\r\nVcells 27554636 210.3  105963594 808.5 156217946 1191.9\r\n\r\nSanity check plot\r\n\r\n\r\nhide\r\n\r\nggplot() +\r\n  geom_sf(data = blackout_cropped_NAD83) +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nRoads data\r\n\r\n\r\nhide\r\n\r\nquery <- \r\n  \"SELECT * \r\n   FROM gis_osm_roads_free_1 \r\n   WHERE fclass='motorway'\"\r\n\r\nhighways <- \r\n  st_read(\r\n    \"data/gis_osm_roads_free_1.gpkg\", \r\n    query = query) %>% \r\n  st_transform(crs = 3083) %>% \r\n  st_buffer(dist = 200) %>% \r\n  st_union()\r\n\r\n\r\nReading query `SELECT * \r\n   FROM gis_osm_roads_free_1 \r\n   WHERE fclass='motorway'' from data source `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\gis_osm_roads_free_1.gpkg' \r\n  using driver `GPKG'\r\nSimple feature collection with 6085 features and 10 fields\r\nGeometry type: LINESTRING\r\nDimension:     XY\r\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\r\nGeodetic CRS:  WGS 84\r\n\r\nhide\r\n\r\ncat(\"\\n\\n\\nAfter Transforming\\n\\n\")\r\n\r\n\r\n\r\n\r\n\r\nAfter Transforming\r\n\r\nhide\r\n\r\nhighways\r\n\r\n\r\nGeometry set for 1 feature \r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 1837420 ymin: 7218299 xmax: 2040621 ymax: 7387049\r\nProjected CRS: NAD83 / Texas Centric Albers Equal Area\r\n\r\nBasic highways plot\r\n\r\n\r\nhide\r\n\r\nggplot() +\r\n  geom_sf(data = highways) +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nBuildings data\r\n\r\n\r\nhide\r\n\r\nquery <- \r\n  \"SELECT * \r\n   FROM gis_osm_buildings_a_free_1\r\n   WHERE (type IS NULL AND name IS NULL)\r\n   OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\r\n\r\nbuildings <- \r\n  st_read(\r\n    \"data/gis_osm_buildings_a_free_1.gpkg\", \r\n    query = query) %>% \r\n  st_transform(crs = 3083) \r\n\r\n\r\nReading query `SELECT * \r\n   FROM gis_osm_buildings_a_free_1\r\n   WHERE (type IS NULL AND name IS NULL)\r\n   OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')' from data source `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\gis_osm_buildings_a_free_1.gpkg' \r\n  using driver `GPKG'\r\nSimple feature collection with 475941 features and 5 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\r\nGeodetic CRS:  WGS 84\r\n\r\nhide\r\n\r\ncat(\"\\n\\n\\nAfter Transforming\\n\\n\")\r\n\r\n\r\n\r\n\r\n\r\nAfter Transforming\r\n\r\nhide\r\n\r\nbuildings\r\n\r\n\r\nSimple feature collection with 475941 features and 5 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 1838180 ymin: 7216470 xmax: 2027040 ymax: 7386914\r\nProjected CRS: NAD83 / Texas Centric Albers Equal Area\r\nFirst 10 features:\r\n     osm_id code   fclass name       type\r\n1  15289727 1500 building <NA>       <NA>\r\n2  15289869 1500 building <NA>       <NA>\r\n3  15299261 1500 building <NA> apartments\r\n4  15331425 1500 building <NA>       <NA>\r\n5  15349970 1500 building <NA>       <NA>\r\n6  20868178 1500 building <NA>       <NA>\r\n7  20871848 1500 building <NA>       <NA>\r\n8  20871948 1500 building <NA>       <NA>\r\n9  20876080 1500 building <NA>       <NA>\r\n10 20877241 1500 building <NA>       <NA>\r\n                             geom\r\n1  MULTIPOLYGON (((1948074 728...\r\n2  MULTIPOLYGON (((1927251 732...\r\n3  MULTIPOLYGON (((1984491 730...\r\n4  MULTIPOLYGON (((1932443 731...\r\n5  MULTIPOLYGON (((1925238 732...\r\n6  MULTIPOLYGON (((1922763 727...\r\n7  MULTIPOLYGON (((1922899 727...\r\n8  MULTIPOLYGON (((1922788 727...\r\n9  MULTIPOLYGON (((1922699 727...\r\n10 MULTIPOLYGON (((1922807 727...\r\n\r\nCensus data\r\n\r\n\r\nhide\r\n\r\n# st_layers(\"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\r\n\r\nacs_geoms <- \r\n  st_read(\r\n    \"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\r\n    layer = \"ACS_2019_5YR_TRACT_48_TEXAS\"\r\n  )\r\n\r\n\r\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \r\n  `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \r\n  using driver `OpenFileGDB'\r\nSimple feature collection with 5265 features and 15 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\r\nGeodetic CRS:  NAD83\r\n\r\nhide\r\n\r\nacs_income <- \r\n  st_read(\r\n    \"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\r\n    layer = \"X19_INCOME\"\r\n  ) %>% \r\n  select(GEOID, B19013e1) %>% \r\n  rename(GEOID_Data = GEOID,\r\n         median_income = B19013e1)\r\n\r\n\r\nReading layer `X19_INCOME' from data source \r\n  `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \r\n  using driver `OpenFileGDB'\r\n\r\n\r\n\r\nhide\r\n\r\nacs_geoms_med <- left_join(acs_geoms, acs_income)  %>% \r\n  st_transform(crs = 3083) \r\n\r\n\r\n\r\nMerge datasets\r\n\r\n\r\nhide\r\n\r\nblackout_no_hwy <- st_difference(blackout_cropped_NAD83, highways)\r\nrm(highways)\r\ngc()\r\n\r\n\r\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\r\nNcells  7126649 380.7   20004878 1068.4  20004878 1068.4\r\nVcells 56415877 430.5  105963594  808.5 156217946 1191.9\r\n\r\n\r\n\r\nhide\r\n\r\nhouston_res_wo_power <- buildings[blackout_no_hwy, op = st_intersects]\r\nnumber_houses_wo_power <- length(houston_res_wo_power$osm_id)\r\n\r\n\r\n\r\nThe number of buildings that were left without power is 157411.\r\n\r\n\r\nhide\r\n\r\nacs_building <- st_join(houston_res_wo_power, acs_geoms_med, join = st_intersects)\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\nacs_polygon <- st_join(blackout_no_hwy, acs_geoms_med, join = st_intersects)\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\nhouston_bbox <- st_bbox(houston)\r\nhouston_map <- osm.raster(houston_bbox)\r\n\r\n\r\n\r\nArea and Median Incomes of Residences Affected by Houston Blackout in February, 2021\r\n\r\n\r\nhide\r\n\r\ntm_shape(houston_map) +\r\n  tm_rgb(alpha = .75) +\r\n  # tm_shape(acs_geoms_med,\r\n  #          border.alpha = 0) +\r\n  # tm_polygons(col = \"median_income\") +\r\n  tm_shape(blackout_no_hwy) +\r\n  tm_polygons(border.alpha = 1) +\r\n  tm_shape(acs_polygon) + \r\n  tm_fill(\"median_income\", \r\n          n = 5, \r\n          style = \"pretty\",\r\n          title = \"Median Income ($)\") +\r\n  tm_compass()+\r\n  tm_scale_bar()\r\n\r\n\r\n\r\n\r\nThis map was created by Amber McEldowney and Cullen Molitor 2021-10-24. Sources: Socioeconomic data: U.S. Census Bureau’s American Community Survey for Texas census tracts in 2019  Light data: NASA’s Level-1 and Atmosphere Archive & Distribution System Distributed Active Archive Center (LAADS DAAC)  Spatial & Buildings Data: OpenStreetMap\r\n\r\n\r\nhide\r\n\r\nrm(blackout_cropped_NAD83, acs_geoms, acs_income, buildings)\r\ngc()\r\n\r\n\r\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\r\nNcells  4929805 263.3   16003903  854.8  20004878 1068.4\r\nVcells 46664452 356.1  152763574 1165.5 156217946 1191.9\r\n\r\nHistogram of Median Income for Houses Affected by Blackout\r\n\r\n\r\nhide\r\n\r\nMedian_Income <- acs_polygon$median_income\r\nhist(Median_Income,\r\nmain=\"Median Income in Regions Affected by Blackout\",\r\nxlab=\"Median Income ($)\",\r\nylab=\"Number of Households\",\r\ncol=\"grey\",\r\nfreq=FALSE\r\n)\r\n\r\n\r\n\r\n\r\nHistogram of Median Income in Houston\r\n\r\n\r\nhide\r\n\r\nhouston_nad <- houston %>%\r\n  st_as_sf() %>%\r\n  st_transform(houston, crs = 3083)\r\n\r\n# acs_geoms_med_bb <- st_join(acs_geoms_med, houston_nad, join = st_intersects)\r\n\r\nintersects <- st_intersects(acs_geoms_med, houston_nad, sparse = FALSE)\r\nacs_geoms_med_bb <- acs_geoms_med[intersects,]\r\n\r\nMedian_Income_Houston <- acs_geoms_med_bb$median_income\r\nhist(Median_Income_Houston,\r\nmain=\"Median Income in Houston\",\r\nxlab=\"Median Income ($)\",\r\nylab=\"Number of Households\",\r\ncol=\"grey\",\r\nfreq=FALSE\r\n)\r\n\r\n\r\n\r\n\r\nWe thought it would be interesting to compare the median incomes of households affected by the blackout, to median incomes in Houston in general, but because the blackouts seem to have occured in a more metropolitan area, it is likely the incomes are skewed higher for that area, and it does not give a clear indication of whether median income had an effect on whether or not a household experienced a blackout.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-03-houston-blackout-analysis-2021/houston-blackout-analysis-2021_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-12-04T10:11:04-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-03-iowas-wind-power-potential/",
    "title": "Iowa's Wind Power Potential",
    "description": "This analysis is used to determine locations in Iowa that would be suitible for wind turbine power plant installation as well as to calculate the hypothetical potential wind energy production capacity of these areas. This type of analysis can be used as a baseline in determining the maximum cost and benefits for potential renewable energy projects.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "Python",
      "SQL",
      "Spatial Analysis"
    ],
    "contents": "\r\nHomework # 4\r\nWind Power Potential\r\nGroup # 2\r\nCullen Molitor\r\nAmber McEldowney\r\n# Setup ## Import Modules\r\nimport sqlalchemy as sa\r\nimport geopandas as gpd\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport contextily as ctx\r\nimport math\r\n%matplotlib inline\r\nDatabase Connection\r\npg_uri_template = 'postgresql+psycopg2://{user}:{pwd}@{host}/{db_name}'\r\ndb_uri = pg_uri_template.format(\r\n    drivername='postgresql+psycopg2',\r\n    host = '{ip address}',\r\n    user = '{username}',\r\n    pwd = '{pwd}',\r\n    db_name = 'osmiowa'\r\n)\r\nengine = sa.create_engine(db_uri)\r\nData Details\r\nDatabase Name: - osmiowa\r\nDatabase Tables: - planet_osm_line - planet_osm_point - planet_osm_polygon - planet_osm_roads - spatial_ref_sys - wind - wind_cells\r\nGeometry: - Projected in EPSG:26975 - Geodetic CRS: NAD83 - Units: meters - Coordinate system: Cartesian 2D CS - Axes: easting, northing (X,Y) - Orientations: east, north\r\nQuery Database\r\nBuildings\r\nFrom planet_osm_polygon table ### Residential Scenario 1 (3H)\r\nsql_buildings_residential_1 = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 450) as way\r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    building IN ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\r\nOR \r\n    landuse = 'residential'\r\nOR \r\n    place = 'town'\r\n\"\"\"\r\n# buildings_residential = gpd.read_postgis(sql_buildings_residential_1, con = engine, geom_col = 'way')\r\nResidential Scenario 2 (10H)\r\nsql_buildings_residential_2 = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 1500) as way \r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    building \r\nIN \r\n    ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\r\nOR \r\n    landuse = 'residential'\r\nOR\r\n    place = 'town'\r\n\"\"\"\r\n# buildings_residential = gpd.read_postgis(sql_buildings_residential_2, con = engine, geom_col = 'way')\r\nNon-residential (3H)\r\nsql_buildings_nonresidential = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 450) as way\r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    building \r\nNOT IN \r\n    ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\r\n\"\"\"\r\n# nonresidential_buffer = gpd.read_postgis(sql_buildings_nonresidential, con = engine, geom_col = 'way')\r\nAirports\r\nFrom planet_osm_polygon table\r\nCreate Airports (7500m) Buffer\r\nsql_airports = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 7500) as way\r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    aeroway IS NOT NULL\r\n\"\"\"\r\n# airports = gpd.read_postgis(sql_airports, con = engine, geom_col = 'way')\r\nMilitary Bases\r\nFrom planet_osm_polygon table\r\nsql_military = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 0) as way \r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    military IS NOT NULL\r\nOR \r\n    landuse = 'military'\r\n\"\"\"\r\n# military = gpd.read_postgis(sql_military, con = engine, geom_col = 'way')\r\nRailways and Roads\r\nFrom planet_osm_line table\r\nCreate Railways and Roads (2H) Buffer\r\nsql_railways_n_roads = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 300) as way\r\nFROM \r\n    planet_osm_line \r\nWHERE \r\n    (railway IS NOT NULL\r\n     AND railway \r\n     NOT IN ('abandoned', 'disused', 'razed', 'dismantled'))\r\nOR \r\n    (highway IS NOT NULL\r\nAND \r\n    highway \r\nIN \r\n    ('motorway', 'motorway_link', 'trunk', 'trunk_link', 'road',\r\n    'primary', 'primary_link', 'secondary', 'secondary_link'))\r\n\"\"\"\r\n# railways_n_roads = gpd.read_postgis(sql_railways_n_roads, con = engine, geom_col = 'way')\r\nNature Reserves, Parks, and Wetlands\r\nFrom planet_osm_polygon table\r\nsql_nature = \\\r\n\"\"\"\r\nSELECT \r\n    way \r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    leisure \r\nIN \r\n    ('nature_reserve', 'park')\r\nOR\r\n    \"natural\" \r\nIN \r\n    ('wetland')\r\n\"\"\"\r\n# nature = gpd.read_postgis(sql_nature, con = engine, geom_col = 'way')\r\nRivers\r\nFrom planet_osm_line\r\nCreate Rivers (1H) Buffer\r\nsql_rivers = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 150) as way \r\nFROM \r\n    planet_osm_line \r\nWHERE \r\n    waterway \r\nIN \r\n    ('river')\r\n\"\"\"\r\n# rivers = gpd.read_postgis(sql_rivers, con = engine, geom_col = 'way')\r\nLakes\r\nFrom planet_osm_polygon\r\nsql_lakes = \\\r\n\"\"\"\r\nSELECT\r\n    way\r\nFROM \r\n    planet_osm_polygon\r\nWHERE \r\n    water \r\nIN \r\n    ('lake', 'reservoir', 'pond')\r\n\"\"\"\r\n# lakes = gpd.read_postgis(sql_lakes, con = engine, geom_col = 'way')\r\nPower Lines\r\nFrom planet_osm_line table\r\nCreate Powerlines (2H) Buffer\r\nsql_powerlines = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 300) as way\r\nFROM \r\n    planet_osm_line \r\nWHERE \r\n    power IS NOT NULL\r\n\"\"\"\r\n# powerlines = gpd.read_postgis(sql_powerlines, con = engine, geom_col = 'way')\r\nPower Plants\r\nFrom planet_osm_polygon table\r\nCreate Power Plants (1H) Buffer\r\nsql_powerplants = \\\r\n\"\"\"\r\nSELECT \r\n    ST_BUFFER(way, 150) as way\r\nFROM \r\n    planet_osm_polygon \r\nWHERE \r\n    power IS NOT NULL\r\n\"\"\"\r\n# powerplants = gpd.read_postgis(sql_powerplants, con = engine, geom_col = 'way')\r\nWind Turbines\r\nFrom planet_osm_point table\r\nsql_turbines = \\\r\n\"\"\"\r\nSELECT\r\n    ST_BUFFER(way, 680) as way\r\nFROM \r\n    planet_osm_point \r\nWHERE \r\n    \"generator:source\" IS NOT NULL\r\nAND \r\n    \"generator:source\" IN ('wind')\r\n\"\"\"\r\n# turbines_buffer = gpd.read_postgis(sql_turbines_buffer, con = engine, geom_col = 'way')\r\nMerge Subqueries\r\nScenario 1\r\nsql_siting_constraints = \\\r\nf\"\"\"\r\n{sql_buildings_residential_1} \r\nUNION\r\n{sql_buildings_nonresidential} \r\nUNION\r\n{sql_airports} \r\nUNION\r\n{sql_military} \r\nUNION\r\n{sql_railways_n_roads} \r\nUNION\r\n{sql_nature} \r\nUNION\r\n{sql_rivers} \r\nUNION\r\n{sql_lakes} \r\nUNION\r\n{sql_powerlines}\r\nUNION\r\n{sql_powerplants}\r\nUNION\r\n{sql_turbines}\r\n\"\"\"\r\nsiting_constraints = gpd.read_postgis(sql_siting_constraints, con = engine, geom_col = 'way')\r\nScenario 2\r\nsql_siting_constraints_2 = \\\r\nf\"\"\"\r\n{sql_buildings_residential_2} \r\nUNION\r\n{sql_buildings_nonresidential} \r\nUNION\r\n{sql_airports} \r\nUNION\r\n{sql_military} \r\nUNION\r\n{sql_railways_n_roads} \r\nUNION\r\n{sql_nature} \r\nUNION\r\n{sql_rivers} \r\nUNION\r\n{sql_lakes} \r\nUNION\r\n{sql_powerlines}\r\nUNION\r\n{sql_powerplants}\r\nUNION\r\n{sql_turbines}\r\n\"\"\"\r\nsiting_constraints_2 = gpd.read_postgis(sql_siting_constraints_2, con = engine, geom_col = 'way')\r\nExtra Credit\r\n# sql_siting_constraints = \\\r\n#     \"\"\"\r\n# SELECT \r\n#     ST_BUFFER(way, 450) as way\r\n#     FROM planet_osm_polygon \r\n#     WHERE building IN ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\r\n#     OR landuse = 'residential'\r\n#     OR place = 'town'\r\n# UNION\r\n# SELECT \r\n#     ST_BUFFER(way, 450) as way\r\n#     FROM planet_osm_polygon \r\n#     WHERE building NOT IN ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\r\n# UNION\r\n# SELECT \r\n#     ST_BUFFER(way, 7500) as way\r\n#     FROM planet_osm_polygon \r\n#     WHERE aeroway IS NOT NULL\r\n# UNION\r\n# SELECT \r\n#     ST_BUFFER(way, 0) as way\r\n#     FROM planet_osm_polygon \r\n#     WHERE military IS NOT NULL\r\n#     OR landuse = 'military'\r\n# UNION\r\n# SELECT  \r\n#     ST_BUFFER(way, 300) as way\r\n#     FROM planet_osm_line \r\n#     WHERE (railway IS NOT NULL\r\n#     AND railway NOT IN ('abandoned', 'disused', \r\n#                         'razed', 'dismantled'))\r\n#     OR (highway IS NOT NULL\r\n#     AND highway IN ('motorway', 'motorway_link',\r\n#                     'trunk', 'trunk_link', 'road',\r\n#                     'primary', 'primary_link',\r\n#                     'secondary', 'secondary_link'))\r\n# UNION\r\n# SELECT  \r\n#     ST_BUFFER(way, 150) as way\r\n#     FROM planet_osm_line \r\n#     WHERE waterway IN ('river')\r\n# UNION\r\n# SELECT  \r\n#     ST_BUFFER(way, 0) as way\r\n#     FROM planet_osm_polygon \r\n#     WHERE leisure IN ('nature_reserve', 'park')\r\n#     OR \"natural\" IN ('wetland')\r\n# UNION\r\n# SELECT \r\n#     ST_BUFFER(way, 0) as way\r\n#     FROM planet_osm_polygon\r\n#     WHERE water IN ('lake')\r\n# UNION\r\n# SELECT  \r\n#     ST_BUFFER(way, 300) as way\r\n#     FROM planet_osm_line \r\n#     WHERE power IS NOT NULL \r\n# UNION\r\n# SELECT  \r\n#     ST_BUFFER(way, 150) as way\r\n#     FROM planet_osm_polygon \r\n#     WHERE power IS NOT NULL\r\n# UNION\r\n# SELECT  \r\n#     ST_BUFFER(way, 680) as way\r\n#     FROM planet_osm_point \r\n#     WHERE \"generator:source\" IS NOT NULL\r\n#     AND \"generator:source\" IN ('wind')\r\n#     \"\"\"\r\n# siting_constraints = gpd.read_postgis(sql_siting_constraints, con = engine, geom_col = 'way')\r\nfig, ax = plt.subplots(figsize=(8, 8))\r\nsiting_constraints.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\r\nax.grid(True, color = 'dimgray')\r\nax.set_title('Siting Constraints (Scenario 1)', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs=\"EPSG:26975\")\r\npngfig, ax = plt.subplots(figsize=(8, 8))\r\nsiting_constraints_2.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\r\nax.grid(True, color = 'dimgray')\r\nax.set_title('Siting Constraints (Scenario 2)', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs=\"EPSG:26975\")\r\npngWind Data\r\nThe table wind_cells_10000 contains 10 km2 square polygons with associated average annual wind speeds (m s-1), arranged to cover Iowa in a ragged grid.\r\nsql_wind_speeds = \\\r\n\"\"\"\r\nSELECT * \r\nFROM \r\n    wind_cells_10000 \r\nWHERE \r\n    wind_speed IS NOT NULL\r\n\"\"\"\r\nwind_speeds = gpd.read_postgis(sql_wind_speeds, con = engine, geom_col = 'geom')\r\nResults\r\nScenario #1\r\nSubtract Siting Constraints from Wind Cells\r\nsuitable_cells = wind_speeds.overlay(siting_constraints, how = 'difference', keep_geom_type = False)\r\nPlot Suitable Wind Cells\r\nfig, ax = plt.subplots(figsize=(8, 8))\r\nsuitable_cells.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\r\nax.grid(True, color = 'dimgray')\r\nax.set_title('Suitable Areas (Scenario 1)', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs=\"EPSG:26975\")\r\npngFind Number of Turbines\r\nrotor_5x = math.pi * (136 * 5) ** 2\r\nsuitable_cells[\"area\"] = suitable_cells['geom'].area\r\nsuitable_cells[\"number_wind_turbines\"] = suitable_cells['geom'].area / rotor_5x\r\ntotal_wind_turbines = suitable_cells[\"number_wind_turbines\"].sum()\r\nprint(\"The total number of turbines in scenario 1 is:\", round(total_wind_turbines))\r\nThe total number of turbines in scenario 1 is: 57286\r\nPower production\r\nsuitable_cells['energy_per_turbine'] = 2.6 * suitable_cells['wind_speed'] - 5\r\nsuitable_cells['power_per_cell'] = suitable_cells['energy_per_turbine'] * suitable_cells['number_wind_turbines']\r\nTotal Power Production\r\ntotal_scenario_1 = suitable_cells['power_per_cell'].sum()\r\nprint(\"The total power produced in scenario 1 is:\", round(total_scenario_1), \"GWH/year\")\r\nThe total power produced in scenario 1 is: 1064159 GWH/year\r\nScenario #2\r\nSubtract Siting Constraints from Wind Cells\r\nsuitable_cells_2 = wind_speeds.overlay(siting_constraints_2, how = 'difference', keep_geom_type = False)\r\nPlot Suitable Wind Cells\r\nfig, ax = plt.subplots(figsize=(8, 8))\r\nsuitable_cells_2.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\r\nax.grid(True, color = 'dimgray')\r\nax.set_title('Suitable Areas (Scenario 2)', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs=\"EPSG:26975\")\r\npngFind Number of Turbines\r\nrotor_5x = math.pi * (136 * 5) ** 2\r\nsuitable_cells_2[\"area\"] = suitable_cells_2['geom'].area\r\nsuitable_cells_2[\"number_wind_turbines\"] = suitable_cells_2['geom'].area / rotor_5x\r\ntotal_wind_turbines_2 = suitable_cells_2[\"number_wind_turbines\"].sum()\r\nprint(\"The total number of turbines in scenario 2 is:\", round(total_wind_turbines_2))\r\nThe total number of turbines in scenario 2 is: 52055\r\nPower Production\r\nsuitable_cells_2['energy_per_turbine'] = 2.6 * suitable_cells_2['wind_speed'] - 5\r\nsuitable_cells_2['power_per_cell'] = suitable_cells_2['energy_per_turbine'] * suitable_cells_2['number_wind_turbines']\r\nwind_speeds\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nid\r\n\r\n\r\ngeom\r\n\r\n\r\nwind_speed\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPOLYGON ((1256222.769 1212179.582, 1266222.769…\r\n\r\n\r\n9.336039\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nPOLYGON ((1266222.769 1212179.582, 1276222.769…\r\n\r\n\r\n9.097315\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\nPOLYGON ((1276222.769 1212179.582, 1286222.769…\r\n\r\n\r\n8.984566\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\nPOLYGON ((1286222.769 1212179.582, 1296222.769…\r\n\r\n\r\n9.266137\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\nPOLYGON ((1296222.769 1212179.582, 1306222.769…\r\n\r\n\r\n9.296747\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n1437\r\n\r\n\r\n1438\r\n\r\n\r\nPOLYGON ((1686222.769 902179.582, 1696222.769 …\r\n\r\n\r\n8.678687\r\n\r\n\r\n1438\r\n\r\n\r\n1439\r\n\r\n\r\nPOLYGON ((1426222.769 892179.582, 1436222.769 …\r\n\r\n\r\n9.114730\r\n\r\n\r\n1439\r\n\r\n\r\n1440\r\n\r\n\r\nPOLYGON ((1656222.769 892179.582, 1666222.769 …\r\n\r\n\r\n8.237385\r\n\r\n\r\n1440\r\n\r\n\r\n1441\r\n\r\n\r\nPOLYGON ((1666222.769 892179.582, 1676222.769 …\r\n\r\n\r\n8.423034\r\n\r\n\r\n1441\r\n\r\n\r\n1442\r\n\r\n\r\nPOLYGON ((1666222.769 882179.582, 1676222.769 …\r\n\r\n\r\n8.109468\r\n\r\n\r\n1442 rows × 3 columns\r\n\r\n\r\nTotal Power Production\r\ntotal_scenario_2 = suitable_cells_2['power_per_cell'].sum()\r\nprint(\"The total power produced in scenario 2 is:\", round(total_scenario_2), \" GWH/year\")\r\nThe total power produced in scenario 2 is: 967009  GWH/year\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-03-iowas-wind-power-potential/output_58_0.png",
    "last_modified": "2021-12-04T10:11:28-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-03-protecting-whales-from-ships/",
    "title": "Protecting Whales From Ships",
    "description": "A vessel speed reduction zone is a potential solution to reducing ship strikes with whales near Dominica. The goal of this   analysis is to look at sperm whale sighting data, and AIS vessel data, to determine a vessel speed reduction zone, and the effects implementing a vessel speed reduction zone would have on shipping activities in the zone.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "Python",
      "Spatial Analysis"
    ],
    "contents": "\r\nHomework # 3\r\nProtecting Whales From Ships\r\nGroup # 2\r\nCullen Molitor\r\nAmber McEldowney\r\nSetup\r\nImport Modules\r\nimport geopandas as gpd \r\nimport rasterio as rio\r\nfrom rasterio.plot import show\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.image as mpimg\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport glob\r\nimport contextily as ctx\r\nimport shapely\r\nfrom shapely.geometry import Polygon\r\n%matplotlib inline\r\nParameters\r\nshoreline_shapefile  = 'data/dominica/dma_admn_adm0_py_s1_dominode_v2.shp'\r\nwhales_CSV           = 'data/sightings2005_2018.csv'\r\nvessels_CSV          = 'data/station1249.csv'\r\nConstants\r\nten_kn_in_ms = 5.14\r\nprojected_EPSG = 2002   # Dominica 1945 / British West Indies Grid\r\ngeodetic_EPSG  = 4326   # WGS 84 (use as default CRS for incoming latlon)\r\nLoad Dominica Shape File\r\ndominica_shp = gpd.read_file(shoreline_shapefile)\r\ndominica_shp.head()\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nADM0_PCODE\r\n\r\n\r\nADM0_EN\r\n\r\n\r\ngeometry\r\n\r\n\r\n0\r\n\r\n\r\nDM\r\n\r\n\r\nDominica\r\n\r\n\r\nPOLYGON ((-61.43023 15.63952, -61.43019 15.639…\r\n\r\n\r\nInspect CRS\r\ndominica_shp.crs\r\n<Geographic 2D CRS: EPSG:4326>\r\nName: WGS 84\r\nAxis Info [ellipsoidal]:\r\n- Lat[north]: Geodetic latitude (degree)\r\n- Lon[east]: Geodetic longitude (degree)\r\nArea of Use:\r\n- name: World.\r\n- bounds: (-180.0, -90.0, 180.0, 90.0)\r\nDatum: World Geodetic System 1984 ensemble\r\n- Ellipsoid: WGS 84\r\n- Prime Meridian: Greenwich\r\nSet CRS\r\nEPSG 2002\r\ndominica_shp = dominica_shp.to_crs(projected_EPSG)\r\ndominica_shp.crs\r\n<Projected CRS: EPSG:2002>\r\nName: Dominica 1945 / British West Indies Grid\r\nAxis Info [cartesian]:\r\n- E[east]: Easting (metre)\r\n- N[north]: Northing (metre)\r\nArea of Use:\r\n- name: Dominica - onshore.\r\n- bounds: (-61.55, 15.14, -61.2, 15.69)\r\nCoordinate Operation:\r\n- name: British West Indies Grid\r\n- method: Transverse Mercator\r\nDatum: Dominica 1945\r\n- Ellipsoid: Clarke 1880 (RGS)\r\n- Prime Meridian: Greenwich\r\nPlot Dominica\r\nfig, ax = plt.subplots(figsize=(5, 5))\r\nax.grid(True, color = 'dimgray')\r\nax.set(ylim=(1.67e6,1.74e6), xlim=(445000, 490000))\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\ndominica_shp.plot(ax = ax, edgecolor = \"k\", facecolor=\"None\")\r\nctx.add_basemap(ax, crs=projected_EPSG)\r\npngLoad Whale Sightings Data\r\nwhales = gpd.read_file(whales_CSV)\r\nwhales\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nfield_1\r\n\r\n\r\nGPStime\r\n\r\n\r\nLat\r\n\r\n\r\nLong\r\n\r\n\r\ngeometry\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n2005-01-15 07:43:27\r\n\r\n\r\n15.36977117\r\n\r\n\r\n-61.49328433\r\n\r\n\r\nNone\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n2005-01-15 08:07:13\r\n\r\n\r\n15.3834075\r\n\r\n\r\n-61.503702\r\n\r\n\r\nNone\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n2005-01-15 08:31:17\r\n\r\n\r\n15.38106333\r\n\r\n\r\n-61.50486067\r\n\r\n\r\nNone\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n2005-01-15 09:19:10\r\n\r\n\r\n15.33532083\r\n\r\n\r\n-61.46858117\r\n\r\n\r\nNone\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\n2005-01-15 10:08:00\r\n\r\n\r\n15.294224\r\n\r\n\r\n-61.45318517\r\n\r\n\r\nNone\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n4888\r\n\r\n\r\n4888\r\n\r\n\r\n2018-05-25 12:01:25\r\n\r\n\r\n15.39195\r\n\r\n\r\n-61.572\r\n\r\n\r\nNone\r\n\r\n\r\n4889\r\n\r\n\r\n4889\r\n\r\n\r\n2018-05-25 13:08:29\r\n\r\n\r\n15.4189\r\n\r\n\r\n-61.5833\r\n\r\n\r\nNone\r\n\r\n\r\n4890\r\n\r\n\r\n4890\r\n\r\n\r\n2018-05-25 14:50:00\r\n\r\n\r\n15.443483\r\n\r\n\r\n-61.60995\r\n\r\n\r\nNone\r\n\r\n\r\n4891\r\n\r\n\r\n4891\r\n\r\n\r\n2018-05-25 15:57:34\r\n\r\n\r\n15.499866\r\n\r\n\r\n-61.638333\r\n\r\n\r\nNone\r\n\r\n\r\n4892\r\n\r\n\r\n4892\r\n\r\n\r\n2018-05-25 16:17:10\r\n\r\n\r\n15.494783\r\n\r\n\r\n-61.6482\r\n\r\n\r\nNone\r\n\r\n\r\n4893 rows × 5 columns\r\n\r\n\r\nCreate GeoDataFrame\r\npoints = gpd.points_from_xy(whales['Long'], whales['Lat'], crs = geodetic_EPSG)\r\nwhales_gdf = gpd.GeoDataFrame(whales, geometry = points)\r\nSet CRS\r\nwhales_gdf = whales_gdf.to_crs(projected_EPSG)\r\nPlot Whale Sightings\r\nfig, ax = plt.subplots(figsize = (10, 10))\r\nax.set_aspect('equal')\r\nax.grid(True, color = 'dimgray')\r\nax.set(xlim=(4e5, 5e5), ylim=(1.5e6, 1.8e6))\r\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\r\nwhales_gdf.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .25)\r\nax.set_title('Whale Sightings', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs = projected_EPSG)\r\npngCreate Grid\r\nxmin, ymin, xmax, ymax = whales_gdf.total_bounds\r\ncell_size = 2000\r\nxs = list(np.arange(xmin, xmax + cell_size, cell_size))\r\nys = list(np.arange(ymin, ymax + cell_size, cell_size))\r\ndef make_cell(x, y, cell_size):\r\n    ring = [\r\n        (x, y),\r\n        (x + cell_size, y),\r\n        (x + cell_size, y + cell_size),\r\n        (x, y + cell_size)\r\n    ]\r\n    cell = shapely.geometry.Polygon(ring)\r\n    return cell\r\ncells = []\r\nfor x in xs:\r\n    for y in ys:\r\n        cell = make_cell(x, y, cell_size)\r\n        cells.append(cell)\r\nCreate GeoDataFrame\r\ngrid = gpd.GeoDataFrame({'geometry': cells}, crs = geodetic_EPSG)\r\nSet CRS\r\ngrid = grid.set_geometry(crs = projected_EPSG, col = grid['geometry'])\r\nPlot Grid\r\ngrid.plot(facecolor = \"none\", figsize = (10, 10), linewidth = 1)\r\n<AxesSubplot:>\r\npngExtract Whale Habitat\r\nwhale_grid = grid.sjoin(whales_gdf, how=\"inner\")\r\nPlot Whale Grid\r\nwhale_grid.plot(facecolor = \"none\", figsize = (10, 10), linewidth = .75)\r\n<AxesSubplot:>\r\npngSummarise Whale Counts\r\ngrid['count'] = whale_grid.groupby(whale_grid.index).count()['index_right']\r\nFind Primary Whale Habitat\r\ngrid = grid[grid['count'] > 20]\r\nPlot Whale Habitat\r\ngrid.plot(facecolor = \"none\", figsize = (10, 10), linewidth = .75)\r\n<AxesSubplot:>\r\npngCreate Unary Union\r\ngrid_union = grid.unary_union\r\ngrid_union\r\nsvgCreate Vessel Speed Reduction Zone\r\nThe vessel speed reduction zone is determined by the whale habitat areas in which more than 20 whales were observed per 2,000 m².\r\ngrid_convex = grid_union.convex_hull\r\ngrid_convex\r\nsvgMake GeoDataFrame\r\nwhale_habitat = gpd.GeoDataFrame(index = [0], geometry = [grid_convex], crs = projected_EPSG)\r\nPlot Whale Habitat\r\nfig, ax = plt.subplots(figsize = (10, 10))\r\nax.grid(True, color = 'dimgray')\r\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\r\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\r\nwhale_habitat.plot(ax = ax, edgecolor = \"purple\", facecolor = 'none')\r\nax.set_title('Whale Habitat', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs = projected_EPSG)\r\npngwhale_habitat_diff = whale_habitat.difference(dominica_shp)\r\nwhale_habitat = gpd.GeoDataFrame(geometry = whale_habitat_diff)\r\nwhale_habitat\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\ngeometry\r\n\r\n\r\n0\r\n\r\n\r\nPOLYGON ((456480.652 1682792.746, 454480.652 1…\r\n\r\n\r\nfig, ax = plt.subplots(figsize = (10, 10))\r\nax.grid(True, color = 'dimgray')\r\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\r\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\r\nwhale_habitat.plot(ax = ax, edgecolor = \"purple\", facecolor = 'none')\r\nax.set_title('Whale Habitat', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs = projected_EPSG)\r\npngwhale_habitat\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\ngeometry\r\n\r\n\r\n0\r\n\r\n\r\nPOLYGON ((456480.652 1682792.746, 454480.652 1…\r\n\r\n\r\nLoad AIS Vessel Data\r\nvessels = gpd.read_file(vessels_CSV)\r\npoints = gpd.points_from_xy(vessels['LON'], vessels['LAT'], crs = geodetic_EPSG)\r\nCreate GeoDataFrame\r\nvessels_gdf = gpd.GeoDataFrame(vessels, geometry = points)\r\nvessels_gdf\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nfield_1\r\n\r\n\r\nMMSI\r\n\r\n\r\nLON\r\n\r\n\r\nLAT\r\n\r\n\r\nTIMESTAMP\r\n\r\n\r\ngeometry\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n233092000\r\n\r\n\r\n-61.84788\r\n\r\n\r\n15.23238\r\n\r\n\r\n2015-05-22 13:53:26\r\n\r\n\r\nPOINT (-61.84788 15.23238)\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n255803280\r\n\r\n\r\n-61.74397\r\n\r\n\r\n15.96114\r\n\r\n\r\n2015-05-22 13:52:57\r\n\r\n\r\nPOINT (-61.74397 15.96114)\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n329002300\r\n\r\n\r\n-61.38968\r\n\r\n\r\n15.29744\r\n\r\n\r\n2015-05-22 13:52:32\r\n\r\n\r\nPOINT (-61.38968 15.29744)\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n257674000\r\n\r\n\r\n-61.54395\r\n\r\n\r\n16.2334\r\n\r\n\r\n2015-05-22 13:52:24\r\n\r\n\r\nPOINT (-61.54395 16.23340)\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\n636092006\r\n\r\n\r\n-61.52401\r\n\r\n\r\n15.81954\r\n\r\n\r\n2015-05-22 13:51:23\r\n\r\n\r\nPOINT (-61.52401 15.81954)\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n617257\r\n\r\n\r\n238722\r\n\r\n\r\n256525000\r\n\r\n\r\n-61.40679\r\n\r\n\r\n15.36907\r\n\r\n\r\n2015-05-21 21:34:59\r\n\r\n\r\nPOINT (-61.40679 15.36907)\r\n\r\n\r\n617258\r\n\r\n\r\n238723\r\n\r\n\r\n311077100\r\n\r\n\r\n-61.37539\r\n\r\n\r\n15.27406\r\n\r\n\r\n2015-05-21 21:34:55\r\n\r\n\r\nPOINT (-61.37539 15.27406)\r\n\r\n\r\n617259\r\n\r\n\r\n238724\r\n\r\n\r\n377907247\r\n\r\n\r\n-61.39461\r\n\r\n\r\n15.30672\r\n\r\n\r\n2015-05-21 21:34:46\r\n\r\n\r\nPOINT (-61.39461 15.30672)\r\n\r\n\r\n617260\r\n\r\n\r\n238725\r\n\r\n\r\n253365000\r\n\r\n\r\n-61.49001\r\n\r\n\r\n16.14007\r\n\r\n\r\n2015-05-21 21:34:46\r\n\r\n\r\nPOINT (-61.49001 16.14007)\r\n\r\n\r\n617261\r\n\r\n\r\n238726\r\n\r\n\r\n329002300\r\n\r\n\r\n-61.48073\r\n\r\n\r\n15.44751\r\n\r\n\r\n2015-05-21 21:34:45\r\n\r\n\r\nPOINT (-61.48073 15.44751)\r\n\r\n\r\n617262 rows × 6 columns\r\n\r\n\r\nSet CRS\r\nvessels_gdf = vessels_gdf.to_crs(projected_EPSG)\r\nMake TIMESTAMP datetime\r\nvessels_gdf['TIMESTAMP'] = pd.to_datetime(vessels_gdf['TIMESTAMP'])\r\nSelect Vessels Inside Whale Habitat\r\nvessels_whale = vessels_gdf.sjoin(whale_habitat, how = 'inner')\r\nvessels_whale\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\nfield_1\r\n\r\n\r\nMMSI\r\n\r\n\r\nLON\r\n\r\n\r\nLAT\r\n\r\n\r\nTIMESTAMP\r\n\r\n\r\ngeometry\r\n\r\n\r\nindex_right\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n329002300\r\n\r\n\r\n-61.38968\r\n\r\n\r\n15.29744\r\n\r\n\r\n2015-05-22 13:52:32\r\n\r\n\r\nPOINT (464555.392 1690588.725)\r\n\r\n\r\n0\r\n\r\n\r\n7\r\n\r\n\r\n7\r\n\r\n\r\n338143127\r\n\r\n\r\n-61.39575\r\n\r\n\r\n15.33418\r\n\r\n\r\n2015-05-22 13:50:54\r\n\r\n\r\nPOINT (463892.452 1694650.397)\r\n\r\n\r\n0\r\n\r\n\r\n13\r\n\r\n\r\n13\r\n\r\n\r\n329002300\r\n\r\n\r\n-61.38968\r\n\r\n\r\n15.29745\r\n\r\n\r\n2015-05-22 13:48:32\r\n\r\n\r\nPOINT (464555.389 1690589.831)\r\n\r\n\r\n0\r\n\r\n\r\n15\r\n\r\n\r\n15\r\n\r\n\r\n338143015\r\n\r\n\r\n-61.39558\r\n\r\n\r\n15.33423\r\n\r\n\r\n2015-05-22 13:47:31\r\n\r\n\r\nPOINT (463910.683 1694655.978)\r\n\r\n\r\n0\r\n\r\n\r\n16\r\n\r\n\r\n16\r\n\r\n\r\n338143127\r\n\r\n\r\n-61.39757\r\n\r\n\r\n15.33139\r\n\r\n\r\n2015-05-22 13:47:25\r\n\r\n\r\nPOINT (463697.964 1694341.275)\r\n\r\n\r\n0\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\n617252\r\n\r\n\r\n238717\r\n\r\n\r\n329002300\r\n\r\n\r\n-61.4885\r\n\r\n\r\n15.4706\r\n\r\n\r\n2015-05-21 21:37:45\r\n\r\n\r\nPOINT (453901.647 1709712.916)\r\n\r\n\r\n0\r\n\r\n\r\n617253\r\n\r\n\r\n238718\r\n\r\n\r\n338143015\r\n\r\n\r\n-61.39553\r\n\r\n\r\n15.33448\r\n\r\n\r\n2015-05-21 21:37:14\r\n\r\n\r\nPOINT (463915.972 1694683.643)\r\n\r\n\r\n0\r\n\r\n\r\n617255\r\n\r\n\r\n238720\r\n\r\n\r\n338143127\r\n\r\n\r\n-61.39563\r\n\r\n\r\n15.33468\r\n\r\n\r\n2015-05-21 21:35:12\r\n\r\n\r\nPOINT (463905.177 1694705.734)\r\n\r\n\r\n0\r\n\r\n\r\n617259\r\n\r\n\r\n238724\r\n\r\n\r\n377907247\r\n\r\n\r\n-61.39461\r\n\r\n\r\n15.30672\r\n\r\n\r\n2015-05-21 21:34:46\r\n\r\n\r\nPOINT (464023.288 1691613.624)\r\n\r\n\r\n0\r\n\r\n\r\n617261\r\n\r\n\r\n238726\r\n\r\n\r\n329002300\r\n\r\n\r\n-61.48073\r\n\r\n\r\n15.44751\r\n\r\n\r\n2015-05-21 21:34:45\r\n\r\n\r\nPOINT (454741.236 1707161.130)\r\n\r\n\r\n0\r\n\r\n\r\n167402 rows × 7 columns\r\n\r\n\r\nPlot Vessel Data\r\nfig, ax = plt.subplots(figsize = (10, 10))\r\nax.grid(True, color = 'dimgray')\r\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\r\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\r\nvessels_whale.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .1)\r\nax.set_title('Vessel Data Inside Whale Habitat', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs = projected_EPSG)\r\npngCalculate Vessel Speeds\r\nSort\r\nvessels_whale_sorted = vessels_whale.sort_values(['MMSI', 'TIMESTAMP'])\r\nMake Copy and Shift\r\nvessels_whale_shifted = vessels_whale_sorted.shift(periods = 1)\r\nJoin Sorted With Shifted\r\nvw_sorted_shifted = vessels_whale_sorted.join(vessels_whale_shifted, lsuffix = \"_original\", rsuffix = \"_copy\")\r\nMatch MMSI Numbers\r\nvw_matched = vw_sorted_shifted[vw_sorted_shifted['MMSI_original'] == vw_sorted_shifted['MMSI_copy']]\r\nSet Geometires\r\nvw_matched = vw_matched.set_geometry(col = vw_matched['geometry_original'], crs = projected_EPSG)\r\nFind the Distance (in meters - m)\r\nvw_matched['distance_m'] = vw_matched['geometry_original'].distance(vw_matched['geometry_copy']) \r\nFind the Elapsed Time (seconds - s)\r\nvw_matched['time_s'] = vw_matched['TIMESTAMP_original'] - vw_matched['TIMESTAMP_copy']\r\nvw_matched['time_s'] = vw_matched['time_s'].dt.total_seconds() \r\nFind the Speed (meters per seconds - ms)\r\nvw_matched['speed_ms'] = vw_matched['distance_m'] / vw_matched['time_s']\r\nvw_matched['10_kn_time'] = vw_matched['distance_m'] / ten_kn_in_ms\r\nvw_matched['time_diff'] = vw_matched['10_kn_time'].sub(vw_matched['time_s'])\r\nvw_matched = vw_matched[vw_matched['time_diff'] > 0]\r\nPlot Vessels travelling over 10 knots\r\nfig, ax = plt.subplots(figsize = (10, 10))\r\nax.grid(True, color = 'dimgray')\r\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\r\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\r\nvw_matched.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .25)\r\nax.set_title('Vessel Data Inside Whale Habitat', fontsize=12)\r\nax.ticklabel_format(scilimits =  [-5, 5])\r\nctx.add_basemap(ax, crs = projected_EPSG)\r\npngView Summary Stats\r\nvw_matched[['distance_m', 'time_s', 'speed_ms']].describe()\r\n\r\n\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\r\n\r\n\r\ndistance_m\r\n\r\n\r\ntime_s\r\n\r\n\r\nspeed_ms\r\n\r\n\r\ncount\r\n\r\n\r\n21282.000000\r\n\r\n\r\n21282.000000\r\n\r\n\r\n2.128200e+04\r\n\r\n\r\nmean\r\n\r\n\r\n1602.087151\r\n\r\n\r\n198.250728\r\n\r\n\r\ninf\r\n\r\n\r\nstd\r\n\r\n\r\n1473.334485\r\n\r\n\r\n187.465984\r\n\r\n\r\nNaN\r\n\r\n\r\nmin\r\n\r\n\r\n64.542545\r\n\r\n\r\n0.000000\r\n\r\n\r\n5.140343e+00\r\n\r\n\r\n25%\r\n\r\n\r\n838.258302\r\n\r\n\r\n120.000000\r\n\r\n\r\n5.949960e+00\r\n\r\n\r\n50%\r\n\r\n\r\n1150.938066\r\n\r\n\r\n141.000000\r\n\r\n\r\n6.826952e+00\r\n\r\n\r\n75%\r\n\r\n\r\n1805.137371\r\n\r\n\r\n181.000000\r\n\r\n\r\n8.696041e+00\r\n\r\n\r\nmax\r\n\r\n\r\n28330.585863\r\n\r\n\r\n4085.000000\r\n\r\n\r\ninf\r\n\r\n\r\nImpact of a 10 knot Speed Reduction Zone\r\nWe calculated that ship vessel traffic would be slowed by 2 minutes per mile on average. Over the course of the year, this equated to approximately 28 days of increased shipping time.\r\nnum_days = vw_matched['time_diff'].sum() / 60 / 60 /24\r\nnum_days\r\n27.9423247732646\r\nmean_time_lost = vw_matched['time_diff'].mean() / 60\r\nmean_time_lost\r\n1.8906563139508044\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-03-protecting-whales-from-ships/output_74_0.png",
    "last_modified": "2021-12-04T15:25:08-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-01-eds-222-final-project-species-density-sst/",
    "title": "Species Density ~ SST + lag(SST)\n",
    "description": "This analysis takes a shotgun approach to investigate which species are sensitive to either warm-water or cold-water events by using a series of species-level dynamic linear models with lag SST anomalies as predictor variables. These models will provide a broad overview of which species are affected, how they are affected, and whether or not the affect is statistically significant. This will inform our understanding of how species distribution and population ranges might be affected by changing oceanographic conditions due to climate change.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-01",
    "categories": [
      "R",
      "Ecology"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nData\r\nAnalysis\r\nResults\r\nCold-water species\r\nWarm-water species\r\n\r\nFuture Work\r\nBetter Models\r\nMore Protocols/Species\r\nIntegrate Program Data\r\n\r\nCode Availability\r\nLiterature Cited\r\n\r\nMotivation\r\nCalifornia’s Northern Channel Islands sit at the transition between two biogeographic provinces, the cold-water North Pacific and the warm-water Gulf of California. San Miguel and Santa Rosa Islands have species representative of the North Pacific province while Anacapa and Santa Barbara Islands sit firmly in the Gulf of California province. Santa Cruz Island lies in the transition zone with the western end of the island favoring North Pacific species and the eastern end favoring Gulf of California species. This makes the Northern Channel Islands a unique place to study how species distribution and recruitment respond to ocean temperatures.\r\n\r\n\r\nhide\r\n\r\nknitr::include_graphics(\"biogeo.png\")\r\n\r\n\r\n\r\n\r\n      Figure 1. The Southern California Bight (SCB) shown with a composite sea\r\n      surface temperatures (SST) color gradient for 2009. This SST is typical of\r\n      the region and illustrates the transition between the North Pacific and Gulf\r\n      of California biogegeographic provinces (Image taken from Freedman 2020).\r\n\r\nThe El Niño Southern Oscillation (ENSO) is measured primarily by the Oceanic Niño Index which calculates a SST anomaly value value each month going back to 1950. These values are derived from SST in the equatorial pacific (\\(\\pm5^{\\circ}\\) latitude, \\(120^{\\circ}-170^{\\circ}W\\) longitude). The patterns of SST in this region are responsible for driving certain global weather and climate patterns. Despite the distance from this region, ENSO has a large effect on the oceanographic conditions of the SCB and the Channel Islands. This effect includes an influence on local SST as well as the strength and direction of ocean currents. These effects are known to influence the distribution and abundance of certain marine species (Day 2018, Freedman 2020).\r\nThis analysis takes a shotgun approach to investigate which species are sensitive to either warm-water or cold-water events by using a series of species-level dynamic linear models with lag SST anomalies as predictor variables. These models will provide a broad overview of which species are affected, how they are affected, and whether or not the affect is statistically significant. This will inform our understanding of how species distribution and population ranges might be affected by changing oceanographic conditions due to climate change.\r\nData\r\nThe Kelp Forest Monitoring (KFM) program at Channel Islands National Park has been collecting abundance and size distribution data of more than 170 species since 1982 (sampled annually). The park currently samples 33 sites at the five islands in the National Park. This data is publicly available and was requested and issued under a scientific research and collecting permit (permit #: CHIS-2020-SCI-0006). These data are collected using scuba surveys where the biologists attempt to minimize their impact on the reef by using non-invasive sampling methods. This means that animals are counted in situ and no rocks are overturned or species removed. This inevitably biases the data towards adult populations of certain invertebrates that utilize crevice habitat during development as they will be hidden from direct observation.\r\nThe fish data is collected in (#\\(/m^3\\)). In order to assure that all data are in the same units, only measurements of invertebrate and algae density (#\\(/m^2\\)) are retained. This takes the total number of species with density data from 151 down to 35. Using species density, we can identify trends in abundance observed over multiple El Niño (warm-water) and La Niña (cold-water) events. Using a dynamic linear model with lag independent variables we can see the effect of sea surface temperature anomalies during and in the years following these events on species density.\r\nBelow we read in ENSO data and calculate yearly mean anomalies before making 5 new columns with corresponding lag values. We then load in the species density data and filter out fish and 2 species which are exceedingly rare and do not have enough data to model. This leaves us with 33 species to model, all with the same units.\r\n\r\n\r\nhide\r\n\r\n# Load libraries\r\nlibrary(tidyverse)\r\nlibrary(here)\r\nlibrary(lubridate)\r\nlibrary(broom)\r\nlibrary(arrow)\r\nlibrary(plotly)\r\n\r\n# Load and Tidy Data\r\noni_yearly <-  # Read in Oceanic Nino Index region 3.4 data\r\n  read.table(\r\n    \"https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/detrend.nino34.ascii.txt\",\r\n    header = T) %>%\r\n  # rename YR for joining tables\r\n  dplyr::rename(SurveyYear = YR) %>% \r\n  dplyr::group_by(SurveyYear) %>% \r\n  # Calculate yearly SST Anom\r\n  dplyr::summarise(SST_Anom = mean(ANOM)) %>% \r\n  # Create lagged SST\r\n  dplyr::mutate(SST_Anom_1 = lag(SST_Anom, n = 1),\r\n                SST_Anom_2 = lag(SST_Anom, n = 2),\r\n                SST_Anom_3 = lag(SST_Anom, n = 3),\r\n                SST_Anom_4 = lag(SST_Anom, n = 4),\r\n                SST_Anom_5 = lag(SST_Anom, n = 5))\r\n\r\n\r\ndensity <-  # Read in Density data\r\n  arrow::read_feather(\"Density.feather\") %>%\r\n  dplyr::filter(Classification != \"Fish\",\r\n                !CommonName %in% c(\"wakame, adult\", \r\n                                   \"wakame, juvenile\", \r\n                                   \"white abalone\")) %>% \r\n  dplyr::left_join(oni_yearly)\r\n\r\n\r\n\r\nAnalysis\r\nThis analysis will use a general model formula of lm(Mean_Density ~ SST_Anom + SST_Anom_1 + SST_Anom_2 + SST_Anom_3 + SST_Anom_4 + SST_Anom_5) applied to density data grouped by each species scientific name. These five lag periods will show how a species is affected within the same year, and for the next five years following anomalous warm or cold water. We expect recruitment of certain species will either increases or decreases following warmer than usual or colder than usual ocean temperatures. Given what we know about the life cycle of many marine species, it is reasonable to assume these effects would not be detected by non-invasive sampling techniques for 1-5 years following anomalous temperatures.\r\nThis analysis is simplified to detect trends in a large number of species and does not take into account the effect of biogeography. This also does not consider the impact of marine protected areas (MPAs) which potentially buffer the ecological community from the effects of abiotic factors such as SST. MPAs could also have an effect from increased predation to some of these species, potentially limiting their abundance.\r\nThe cumulative effect on all species will be plotted. Then the results will be filtered to only include significant affects and sorted by the estimated slope of their relationship. The two species with the most positive slope and the two with the most negative slope will have their coefficients plotted over the lag periods.\r\n\r\n\r\nhide\r\n\r\nResults_lag <- density %>% \r\n  dplyr::group_by(ScientificName) %>%\r\n  dplyr::summarise(\r\n    generics::tidy(\r\n      stats::lm(\r\n        Mean_Density ~ SST_Anom + SST_Anom_1 + SST_Anom_2 + SST_Anom_3 + SST_Anom_4 + SST_Anom_5\r\n        ))) %>% \r\n  dplyr::filter(term != \"(Intercept)\") %>% \r\n  dplyr::mutate(significant = ifelse(p.value <= .05, \"yes\", \"no\"))\r\n\r\n\r\n\r\nResults\r\n\r\n\r\nhide\r\n\r\nResults_filtered <- Results_lag %>%\r\n  dplyr::filter(p.value <= .05) %>% \r\n  dplyr::arrange(estimate) %>%\r\n  dplyr::mutate(statistic = round(statistic, 3),\r\n                p.value = round(p.value, 3),\r\n                p.value = ifelse(p.value < 0.001, \"< 0.001\", as.character(p.value))) \r\ncold_sp <- Results_filtered %>% \r\n  arrange(estimate) %>% \r\n  distinct(ScientificName) %>% \r\n  head(3) %>% \r\n  pull()\r\n\r\nwarm_sp <- Results_filtered %>% \r\n  arrange(desc(estimate)) %>% \r\n  distinct(ScientificName) %>% \r\n  head(3) %>% \r\n  pull()\r\n\r\ncold <- Results_lag %>% \r\n  group_by(ScientificName) %>% \r\n  mutate(cum_est = cumsum(estimate)) %>% \r\n  filter(ScientificName %in% cold_sp)\r\n\r\nwarm <- Results_lag %>% \r\n  group_by(ScientificName) %>% \r\n  mutate(cum_est = cumsum(estimate)) %>% \r\n  filter(ScientificName %in% warm_sp)\r\n\r\nplt <- function(.data = cold) {\r\n  ggplot(data = .data, aes(x = term, y = cum_est)) +\r\n    geom_line(aes(color = ScientificName, group = ScientificName), size = 1) +\r\n    geom_point(aes(shape = significant), size = 4) +\r\n    scale_y_continuous(expand = c(0, 1)) +\r\n    scale_x_discrete(labels = c(\"Current Year\", \"+1\", \"+2\", \"+3\", \"+4\", \"+5\")) +\r\n    scale_color_viridis_d(option = \"D\", end = .75, direction = -1) +\r\n    labs(x = \"Years relative to SST anomaly exposure\", y = \"Cumulative effect (sum of coefficients)\",\r\n         shape = \"P-Val < .05?\", color = \"Species\") +\r\n    theme_classic() \r\n}\r\n\r\n\r\n\r\nCold-water species\r\nThe two species with the largest negative cumulative effect were identified as Strongylocentrotus purpuratus (purple sea urchin) and Strongylocentrotus franciscanus (red sea urchin). Purple sea urchin seem to have the most extreme negative coefficients, suggesting that this species thrives under cold-water conditions. Red sea urchins have a similar though less extreme response to cold-water conditions. In both urchin species, the effect is significant in the same year, as well as the year following cold-water conditions. This makes sense, because despite non-invasive sampling, juvenile sea urchins as small as 1 mm have been recorded by KFM observers.\r\nPatiria miniata (bat star) had the third most negative cumulative effect. This species is particularly affected by warm water, which tends to cause a form of sea star wasting disease (SSWD). Bat star populations take years to rebound following SSWD events. This slow recovery is seen in the cumulative effect which is statistically significant for the current period and for the following four years.\r\n\r\n\r\nhide\r\n\r\nplt()\r\n\r\n\r\n\r\n\r\n      Figure 2. The cumulative effect that SST anomalies have on the three species\r\n      which have the largest negative slope estimates of all statistically significant\r\n      results. The effect accumulates from the current year to the following 5 years.\r\n\r\nWarm-water species\r\nSargassum horneri (devil weed) had the single most positive cumulative effect of any species. This is an invasive algae that has spread rapidly at the Channel Islands since its introduction to Catalina island in 2006 (Miller 2007). Predicting what factors contribute to the spread of S. horneri will help scientist understand where to expect it next and what impacts it might have on the ecologic communities it settles in. Only the current year is seen to be positive, suggesting that S. horneri juveniles are largely responsible for the effect.\r\nMegastaea undosa (wavy turban snail) is an important grazing snail that is known to spawn in warm-water conditions which means that ENSO may be helpful in setting harvest quotas (Zacharias 2006). The effect is only significant 2, 3, and 4 years following a warm water event. I would argue that these snails spawn late in the year when the water is warmest and the sampling season is coming to a close. The following year, the juvenile snails are most likely missed as they are too cryptic. Then during the 2nd, 3rd, and 4th year, these animals are largest enough to be observed reliably.\r\nLytechinus anamesus likely follow the same pattern as M. undosa. I would be interested to investigate MPA effects on these lobster and sheephead snack-sized sea urchin (<40 mm typically).\r\n\r\n\r\nhide\r\n\r\nplt(.data = warm)\r\n\r\n\r\n\r\n\r\n      Figure 3. The cumulative effect that SST anomalies have on the three species\r\n      which have the largest positive slope estimates of all statistically significant\r\n      results. The effect accumulates from the current year to the following 5 years.\r\n\r\nFuture Work\r\nBetter Models\r\nInclude biogeogrphic regions to see how species abundance is affected by region. I would also like to include MPA status to identify if there are any buffer effects provided by the MPA, or impacts to prey species from increased preddation.\r\nMore Protocols/Species\r\nThis analysis only uses 2 out of 13 sampling protocols of the KFM program. The analysis could be expanded to include data from the two fish surveys, as well as percent cover data from random point contacts survey.\r\nIntegrate Program Data\r\nThis analysis could be made more robust by including data from the Partnership for Interdisciplinary Study of Coastal Oceans (PISCO), which conducts similar monitoring efforts around the Channel Islands.\r\nCode Availability\r\nGitHub Repository with all code and data available at this link.\r\nLiterature Cited\r\nCostello, M.J., Tsai, P., Wong, P.S. et al. Marine biogeographic realms and species endemicity. Nat Commun 8, 1057 (2017). https://doi.org/10.1038/s41467-017-01121-2\r\nDay, P. B., Stuart-Smith, R. D., Edgar, G. J. & Bates, A. E. Species’ thermal ranges predict changes in reef fish community structure during 8 years of extreme temperature variation. Divers. Distrib. 24, 1036–1046 (2018).\r\nFreedman, R.M., Brown, J.A., Caldow, C. et al. Marine protected areas do not prevent marine heatwave-induced fish community structure changes in a temperate transition zone. Sci Rep 10, 21081 (2020). https://doi.org/10.1038/s41598-020-77885-3\r\nHorta e Costa, B. Tropicalization of fish assemblages in temperate biogeographic transition zones. Mar. Ecol. Prog. Ser. 504, 241–252 (2014).\r\nMiller, K. A., Engle, J. M., Uwai, S., & Kawai, H. (2007). First report of the asian seaweed sargassum filicinum harvey (fucales) in california, usa. Biological Invasions, (9), 609-613.\r\nWernberg, T. S. et al. Climate-driven regime shift of a temperate marine ecosystem. Science 353, 169–172 (2016).\r\nZacharias, Mark, and David J. Kushner. 2006. “Sea temperature and wave height as predictors of population size structure and density of Megastraea (Lithopoma) undosa: Implications for fishery management.” Bulletin of Marine Science 79.1: 71-82.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-01-eds-222-final-project-species-density-sst/eds-222-final-project-species-density-sst_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-12-04T10:14:42-08:00",
    "input_file": {}
  }
]
