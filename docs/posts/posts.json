[
  {
    "path": "posts/2022-04-23-new-horizons/",
    "title": "New Horizons in Conservation Conference Poster Submission",
    "description": "Our capstone group attended the New Horizons in Conservation Conference, 2022, organized through the Yale School of the Environments Justice, Equity, Diversity, and Sustainability Initiative. The 2022 conference was held virtually, and our group contributed a poster and video explaing our capstone project including our preliminary results.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": "https://github.com/cropmosaiks/Modeling"
      }
    ],
    "date": "2022-04-23",
    "categories": [
      "Python",
      "Remote Sensing",
      "Spatial Analysis",
      "Africa",
      "Agriculture"
    ],
    "contents": "\n\nContents\nAn open-source pipleline for remote sensing of crop yields under environmental change in sub-Saharan Africa\n\n\n\n\nAn open-source pipleline for remote sensing of crop yields under environmental change in sub-Saharan Africa\nABSTRACT:\nThe environmental and social impacts of climate change are disproportionately distributed worldwide. Many highly impacted regions lack the assets to monitor and generate resource predictions, and therefore lack high-quality environmental and social data. As a result, it is difficult to make predictions about the impacts of climate change for these regions using conventional modeling. Recently, machine learning approaches applied to high-resolution satellite imagery have been successful in making predictions of a wide range of social and environmental variables. However, generating these predictions comes with significant barriers, including high computational, data storage, expertise, and financial resource costs. Reducing the financial and computational burden of machine learning approaches is essential to increasing the equity of environmental monitoring processes and outputs. Sub-Saharan Africa is one of these data-limited regions and is likely to suffer some of the largest impacts from climate change globally. To enable increased monitoring and data access across the sub-continent, we apply the novel machine learning approach, MOSAIKS, to create tabular features for sub-Saharan Africa using satellite imagery. These features, paired with ground-truth crop data from three countries, will be used to build a model that predicts crop yields over time in regions without crop data. This model will provide new insights into the historical crop yields of the region. Furthermore, these tabular features of satellite imagery, and the methodology developed to create them, will enable more people around the globe to build models and generate predictions of other social and environmental variables in this region.\nVideo submission\n\nVideo\n\nPoster submission\n\n\n\nPlease feel free to contact my team at cp-cropmosaiks.bren.ucsb.edu or inspect our code in our github organization, cropMOSAIKS. Thanks!\n\n\n\n",
    "preview": "posts/2022-04-23-new-horizons/NHC.png",
    "last_modified": "2022-05-29T16:39:55+00:00",
    "input_file": {},
    "preview_width": 758,
    "preview_height": 439
  },
  {
    "path": "posts/2022-01-23-cropmosaiks-animated-ndvi/",
    "title": "CropMOSAIKS Animated NDVI",
    "description": "This notebook extends the code used to create the static images of the NDVI across Africa in 2013 for the MEDS capstone group CropMOSAIKS. This uses Google Earth Engine Python API to create an animated gif of NDVI over Africa from 2000 to 2021.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": "https://github.com/cropmosaiks/NDVI_Images"
      }
    ],
    "date": "2022-01-23",
    "categories": [
      "Python",
      "Spatial Analysis",
      "Remote Sensing",
      "Google Earth Engine"
    ],
    "contents": "\nNDVI Animated Images\nThe following code is adapted from a Google Earth Engine JavaScript tutorial. The code was translated into python and reproduced with several color palettes. The python code is found on the CropMOSAIKS GitHub.\nimport ee\nimport geemap\nimport os\nimport glob\nfrom PIL import Image\n# Initialize google earth engine\nee.Initialize()\n\n# Fetch a MODIS NDVI collection and select NDVI.\nimg_collection = ee.ImageCollection('MODIS/006/MOD13A2').select('NDVI')\n\n# Define a mask to clip the NDVI data by.\nmask = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('wld_rgn', 'Africa'))\n\n# Define the regional bounds of animation frames.\nregion = ee.Geometry.Polygon(\n  [[[-18.698368046353494, 38.1446395611524],\n    [-18.698368046353494, -36.16300755581617],\n    [52.229366328646506, -36.16300755581617],\n    [52.229366328646506, 38.1446395611524]]], None, False\n)\n# Add day-of-year (DOY) property to each image.\ndef clip_and_get_day_of_year(img):\n    img = img.clip(mask)\n    doy = ee.Date(img.get('system:time_start')).getRelative('day', 'year')\n    return img.set('doy', doy)\n# Apply median reduction among matching DOY collections.\ndef match_day_of_year_and_reduce(img):\n    doyCol = ee.ImageCollection.fromImages(img.get('doy_matches'))\n    return doyCol.reduce(ee.Reducer.median())\nnatural = [\n    'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n    '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n    '012E01', '011D01', '011301'\n]\nviridis = [\n    \"#440154FF\", \"#48186AFF\", \"#472D7BFF\", \"#424086FF\", \"#3B528BFF\", \"#33638DFF\", \n    \"#2C728EFF\", \"#26828EFF\", \"#21908CFF\", \"#1F9F88FF\", \"#27AD81FF\", \"#3EBC74FF\",\n    \"#5DC863FF\", \"#82D34DFF\", \"#AADC32FF\", \"#D5E21AFF\", \"#FDE725FF\"\n]\nmagma = [\n    \"#000004FF\", \"#0B0724FF\", \"#210C4AFF\", \"#3D0965FF\", \"#56106EFF\", \"#71196EFF\", \n    \"#89226AFF\", \"#A32C61FF\", \"#BB3754FF\", \"#D14545FF\", \"#E35932FF\", \"#F1721EFF\",\n    \"#F98C0AFF\", \"#FCAA0FFF\", \"#F9C932FF\", \"#F2E865FF\", \"#FCFFA4FF\"\n]\ncividis = [\n    \"#00204DFF\", \"#002C69FF\", \"#05366EFF\", \"#2D426CFF\", \"#414D6BFF\", \"#52596CFF\", \n    \"#61646FFF\", \"#6F7073FF\", \"#7C7B78FF\", \"#8B8779FF\", \"#9B9477FF\", \"#ACA174FF\",\n    \"#BCAF6FFF\", \"#CEBC68FF\", \"#E0CB5EFF\", \"#F2DA50FF\", \"#FFEA46FF\"\n]\n# Define visualization parameters.\nvis_params = {\n  'region': region,\n  'dimensions': 600,\n  'crs': 'EPSG:3857',\n  'framesPerSecond': 10,\n  'min': 0.0,\n  'max': 9000.0,\n  'palette': magma\n}\nimg_collection = img_collection.map(clip_and_get_day_of_year)\nimg_dates = img_collection.aggregate_array('system:index').getInfo()\nstart_yr = int(img_dates[0][:4])\nend_yr = int(img_dates[-1][:4])\nyears = range(start_yr, end_yr + 1, 1)\nindex_array = []\nfor year in years:\n    print('Downloading: ', year)\n    date_start = f'{str(year)}-01-01'\n    date_end = f'{str(year)}-12-31'\n    # Get a collection of distinct images by 'doy'.\n    distinct_doy = img_collection.filterDate(date_start, date_end)\n\n    # Define a filter that identifies which images from the complete\n    # collection match the DOY from the distinct DOY collection.\n    filtered = ee.Filter.equals(leftField = 'doy', rightField = 'doy')\n\n    # Define a join.\n    joined = ee.Join.saveAll('doy_matches')\n\n    # Apply the join and convert the resulting FeatureCollection to an ImageCollection.\n    join_img_collection = ee.ImageCollection(joined.apply(distinct_doy, img_collection, filtered))\n\n    img_composite = join_img_collection.map(match_day_of_year_and_reduce)\n    index_array = index_array + img_composite.aggregate_array('system:index').getInfo()\n    \n    temp_file_name = f\"animations/temp_ndvi_{str(year)}.gif\"\n    \n    geemap.download_ee_video(img_composite, vis_params, temp_file_name)\n# filepaths\nfp_in = \"animations/temp_*.gif\"\nfp_out = \"animations/ndvi.gif\"\n\n# https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif\nimg, *imgs = [Image.open(f) for f in sorted(glob.glob(fp_in))]\nimg.save(fp = fp_out, \n         format = 'GIF',\n         append_images = imgs,\n         save_all = True, \n         duration = 100, \n         loop = 1)\ngeemap.add_text_to_gif(\n    in_gif = fp_out, \n    out_gif = fp_out,\n    xy = ('75%', '1%'),\n    text_sequence = index_array,\n    font_type = 'arial.ttf',\n    font_size = 20,\n    font_color='white',\n    add_progress_bar = True,\n    progress_bar_color = 'white',\n    progress_bar_height = 5,\n    duration = 100, # milliseconds per frame so 200 is 5 fps, 100 is 10 fps etc\n    loop = 0\n)\n\n\n\n\n",
    "preview": "posts/2022-01-23-cropmosaiks-animated-ndvi/ndvi.gif",
    "last_modified": "2022-05-29T16:39:55+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-22-cropmosaiks-team-graphics/",
    "title": "Creating Team Graphics",
    "description": "This notebook explores how the MEDS capstone group CropMOSAIKS used Google Earth Engine Python API to create static images of the median NDVI across Africa in 2013. The graphic are used in presentations for the group.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": "https://github.com/cropmosaiks/NDVI_Images"
      }
    ],
    "date": "2022-01-22",
    "categories": [
      "Python",
      "Spatial Analysis",
      "Remote Sensing",
      "Google Earth Engine"
    ],
    "contents": "\nNDVI Static Images\nThe following code is adapted from a Google Earth Engine JavaScript tutorial. The code was translated into python, made static, and reproduced with several color palettes. The python code is found on the CropMOSAIKS GitHub.\nimport ee\nfrom IPython.display import Image\nee.Initialize()\nndvi = ee.ImageCollection('MODIS/006/MOD13A2').select('NDVI')\nmask = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('wld_rgn', 'Africa'))\nregion = ee.Geometry.Polygon(\n  [\n      [\n          [-19.698368046353494, 38.1446395611524],\n          [-19.698368046353494, -36.16300755581617],\n          [53.229366328646506, -36.16300755581617],\n          [53.229366328646506, 38.1446395611524]\n      ]\n  ]\n)\nndvi = ndvi.filterDate('2013-01-01', '2014-01-01')\nndvi = ndvi.reduce(ee.Reducer.median())\nndvi = ndvi.clip(mask)\nnatural = [\n    'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n    '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n    '012E01', '011D01', '011301'\n]\nviridis = [\n    \"#440154FF\", \"#48186AFF\", \"#472D7BFF\", \"#424086FF\", \"#3B528BFF\", \"#33638DFF\", \n    \"#2C728EFF\", \"#26828EFF\", \"#21908CFF\", \"#1F9F88FF\", \"#27AD81FF\", \"#3EBC74FF\",\n    \"#5DC863FF\", \"#82D34DFF\", \"#AADC32FF\", \"#D5E21AFF\", \"#FDE725FF\"\n]\nmagma = [\n    \"#000004FF\", \"#0B0724FF\", \"#210C4AFF\", \"#3D0965FF\", \"#56106EFF\", \"#71196EFF\", \n    \"#89226AFF\", \"#A32C61FF\", \"#BB3754FF\", \"#D14545FF\", \"#E35932FF\", \"#F1721EFF\",\n    \"#F98C0AFF\", \"#FCAA0FFF\", \"#F9C932FF\", \"#F2E865FF\", \"#FCFFA4FF\"\n]\ncividis = [\n    \"#00204DFF\", \"#002C69FF\", \"#05366EFF\", \"#2D426CFF\", \"#414D6BFF\", \"#52596CFF\", \n    \"#61646FFF\", \"#6F7073FF\", \"#7C7B78FF\", \"#8B8779FF\", \"#9B9477FF\", \"#ACA174FF\",\n    \"#BCAF6FFF\", \"#CEBC68FF\", \"#E0CB5EFF\", \"#F2DA50FF\", \"#FFEA46FF\"\n  ]\n#  # Closer to True color\nvisParams = {\n  'min': 0.0,\n  'max': 9000.0,\n  'dimensions': 512,\n  'region': region, \n  'opacity': 1,\n  'palette': natural\n}\nurl = ndvi.getThumbUrl(visParams)\nImage(url=url, embed=True, format = 'png')\n\n# # Viridis\nvisParams = {\n  'min': 0.0,\n  'max': 9000.0,\n  'dimensions': 512,\n  'region': region, \n  'opacity': 1,\n  'palette': viridis\n}\nurl = ndvi.getThumbUrl(visParams)\nImage(url=url, embed=True, format = 'png')\n\nvisParams = {\n  'min': 0.0,\n  'max': 9000.0,\n  'dimensions': 512,\n  'region': region, \n  'opacity': 1,\n  'palette': magma\n}\nurl = ndvi.getThumbUrl(visParams)\nImage(url=url, embed=True, format = 'png')\n\nvisParams = {\n  'min': 0.0,\n  'max': 9000.0,\n  'dimensions': 512,\n  'region': region, \n  'opacity': 1,\n  'palette': cividis\n}\nurl = ndvi.getThumbUrl(visParams)\nImage(url=url, embed=True, format = 'png')\n\n\n\n\n",
    "preview": "posts/2022-01-22-cropmosaiks-team-graphics/output_13_0.png",
    "last_modified": "2022-05-29T16:39:55+00:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 512
  },
  {
    "path": "posts/2022-01-22-channel-islands-sst/",
    "title": "Channel Islands SST",
    "description": "This notebook explores how to plot satellite sea surface temerature (SST) in the Southern California Bight around the Northern Channel Islands. This shows how to plot a static plot as well as how to animate it over time.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2022-01-21",
    "categories": [
      "R",
      "Ocean Data",
      "Remote Sensing",
      "Spatial Analysis",
      "ENSO"
    ],
    "contents": "\nLibraries\n\n\nhide\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(rerddap)\nlibrary(marmap)\nlibrary(raster)\nlibrary(cowplot)\nlibrary(ggspatial)\nlibrary(gganimate)\nlibrary(gifski)\n\n\n\nGeographic Data\n\n\nhide\n\nca <- read_sf(\"CA.gpkg\")\nSite_Info <- read_csv(\"Site_Info.csv\")\n\n\n\nGriddap Query\n\n\nhide\n\nlat_min <- 33.35\nlat_max <- 34.5\nlon_min <- -120.75\nlon_max <- -118.95\n\nlat <- c(lat_min, lat_max)\nlon <- c(lon_min, lon_max)\n\ntm <- c(\n  \"2009-01-02T12:00:00Z\",\n  '2009-12-30T12:00:00Z'\n)\n\nSST <- 'jplMURSST41'\nfield <- 'analysed_sst'\nmurSST_west <- griddap(\n  x = SST,\n  latitude = lat,\n  longitude = lon,\n  time = tm,\n  fields = field\n)\nsst <- tibble(murSST_west$data) %>%\n  group_by(lat, lon) %>%\n  summarise(sst = mean(analysed_sst))\n\n\n\nBathymetry Querry\n\n\nhide\n\nca_bath <- marmap::getNOAA.bathy(\n  lon1 = lon_min - 1,\n  lon2 = lon_max + 1,\n  lat1 = lat_min - 1,\n  lat2 = lat_max + 1,\n  resolution = 1\n) %>% \n  marmap::as.raster() %>% \n  raster::rasterToPoints() %>% \n  base::as.data.frame() \n\n\n\nMake California Map for Inset\n\n\nhide\n\nbox <- sf::st_polygon(\n  x = list(\n    rbind(\n      c(lon_min, lat_max), \n      c(lon_max, lat_max), \n      c(lon_max, lat_min), \n      c(lon_min, lat_min),\n      c(lon_min, lat_max)\n    )\n  )\n) %>% st_sfc(crs = 4326)\n\nC <- ggplot() +\n  geom_sf(data = ca, fill = \"white\", size = 1) +\n  geom_sf(data = box, fill = NA, size = 1, color = 'red') +\n  theme_void() +\n  theme(panel.border = element_rect(fill = NA),\n        panel.background = element_rect(fill = alpha(\"white\", .5)))\n\n\n\nPlot Static Map\n\n\nhide\n\nmain.plot <- ggplot() +\n  geom_raster(data = sst, aes(x = lon, y = lat, fill = sst), interpolate = T) +\n  scale_fill_viridis_c(option = 'viridis', guide = guide_colorbar(\n    direction = \"horizontal\",frame.colour = \"black\",\n    title.position = \"top\", barheight = unit(.25, 'cm'))) +\n  geom_sf(data = ca, fill = \"grey70\", color = \"grey40\", size = .1) +\n  geom_contour(data = ca_bath, aes(x = x, y = y, z = layer),\n    breaks = seq(min(ca_bath$layer), max(ca_bath$layer), by = 5),\n    color = \"black\", alpha = .01, size = 1) +\n  scale_x_continuous(limits = lon, expand = c(0,0), breaks = c(-120, -119.5)) +\n  scale_y_continuous(limits = lat, expand = c(0,0), breaks = c(33.75, 34.25)) +\n  geom_point(data = Site_Info, aes(x = Longitude, y = Latitude),\n             color = '#d55b23', show.legend = F, inherit.aes = F) +\n  scale_color_viridis_d(option = 'magma', begin = .2, end = .8, limits = force) +\n  labs(fill = \"SST (\\u00B0C)\", x = NULL, y = NULL) +\n  annotation_scale(location = \"bl\") +\n  annotation_north_arrow(which_north = TRUE, location = \"tr\", pad_x = unit(2.75, \"cm\"),\n                         height = unit(1, \"cm\"), width = unit(1, \"cm\")) +\n  theme_classic() +\n  theme(legend.position = c(0.125, 0.12),\n        legend.title = element_text(face = 'bold'),\n        legend.text = element_text(face = 'bold'),\n        legend.background = element_rect(fill = alpha(\"white\", .25), colour = 'black'),\n        axis.ticks.length = unit(-0.25, \"cm\"),\n        axis.ticks = element_line(color = \"black\", size = 2),\n        axis.text.y = element_text(hjust = .5, margin = margin(0,-.7,0,-.5, unit = 'cm'),\n                                   face = 'bold', color = \"white\", angle = 270),\n        axis.text.x = element_text(vjust = 5, margin = margin(-0.5,0,0.5,0, unit = 'cm'),\n                                   face = 'bold', color = \"white\"),\n        panel.border = element_rect(color = \"black\", size = 2, fill = NA)\n  )\n\nggdraw() +\n  draw_plot(main.plot) +\n  draw_plot(C, x = 0.72, y = .77, width = .2, height = .2)\n\n\n\n\nPlot Animated Map\nShowing 2014-2016 to highlight the 2015-2016 El Nino.\n\n\nhide\n\nredo <- FALSE\n\nif (!file.exists(\"sst.gif\") | redo){\n  tm <- c(\n    \"2014-01-01T12:00:00Z\",\n    '2016-12-30T12:00:00Z'\n  )\n  \n  murSST_west <- griddap(\n    x = SST, \n    latitude = lat, \n    longitude = lon,\n    time = tm, \n    fields = field\n  )$data %>% \n    mutate(date_time = lubridate::as_datetime(time),\n           date = lubridate::date(date_time)) %>% \n    group_by(date, lon, lat) %>% \n    summarise(sst = analysed_sst)\n  \n  p1 <- ggplot() +\n    geom_raster(data = murSST_west, aes(x = lon, y = lat, fill = sst), interpolate = T) +\n    scale_fill_viridis_c(option = 'viridis', guide = guide_colorbar(\n      direction = \"horizontal\",frame.colour = \"black\",\n      title.position = \"top\", barheight = unit(.25, 'cm'))) +\n    geom_sf(data = ca, fill = \"grey70\", color = \"grey40\", size = .1) +\n    geom_contour(data = ca_bath, aes(x = x, y = y, z = layer), \n                 breaks = seq(min(ca_bath$layer), max(ca_bath$layer), by = 5), \n                 color = \"black\", alpha = .01, size = 1) +\n    scale_x_continuous(limits = lon, expand = c(0,0), breaks = c(-120, -119.5)) +\n    scale_y_continuous(limits = lat, expand = c(0,0), breaks = c(33.75, 34.25)) +\n    scale_color_viridis_d(option = 'magma', begin = .2, end = .8, limits = force) +\n    labs(fill = \"SST (\\u00B0C)\", \n         title = \"{frame_time}\",\n         x = NULL, y = NULL) +\n    annotation_scale(location = \"bl\") +\n    annotation_north_arrow(which_north = TRUE, location = \"tr\", #pad_x = unit(2.75, \"cm\"),\n                           height = unit(1, \"cm\"), width = unit(1, \"cm\")) +\n    theme_classic() +\n    theme(legend.position = c(0.125, 0.12),\n          legend.title = element_text(face = 'bold'),\n          legend.text = element_text(face = 'bold'),\n          legend.background = element_rect(fill = alpha(\"white\", .25), colour = 'black'),\n          axis.ticks.length = unit(-0.25, \"cm\"),\n          axis.ticks = element_line(color = \"black\", size = 2),\n          axis.text.y = element_text(hjust = .5, margin = margin(0,-.7,0,-.5, unit = 'cm'),\n                                     face = 'bold', color = \"white\", angle = 270),\n          axis.text.x = element_text(vjust = 5, margin = margin(-0.5,0,0.5,0, unit = 'cm'),\n                                     face = 'bold', color = \"white\"),\n          panel.border = element_rect(color = \"black\", size = 2, fill = NA)) +\n    gganimate::transition_time(date)\n  \n  gganimate::animate(p1, width = 720, height = 480,\n                     # renderer = av_renderer(),\n                     nframes = 100, fps = 10)\n  \n  anim_save(filename = \"sst.gif\", animation = last_animation())\n}\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-01-22-channel-islands-sst/sst.gif",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-05-species-density-sst-lagsst/",
    "title": "Effect of ENSO on Kelp Forest Species\n",
    "description": "This analysis takes a shotgun approach to investigate which species are sensitive to either warm-water or cold-water events by using a series of species-level dynamic linear models with lag SST anomalies as predictor variables. These models will provide a broad overview of which species are affected, how they are affected, and whether or not the affect is statistically significant. This will inform our understanding of how species distribution and population ranges might be affected by changing oceanographic conditions due to climate change.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-05",
    "categories": [
      "R",
      "Ecology",
      "Ocean Data",
      "ENSO"
    ],
    "contents": "\n\nContents\nMotivation\nData\nAnalysis\nResults\nCold-water species\nWarm-water species\n\nFuture Work\nBetter Models\nMore Protocols/Species\nIntegrate Program Data\n\nCode Availability\nLiterature Cited\n\nMotivation\nCalifornia’s Northern Channel Islands sit at the transition between two biogeographic provinces, the cold-water North Pacific and the warm-water Gulf of California. San Miguel and Santa Rosa Islands have species representative of the North Pacific province while Anacapa and Santa Barbara Islands sit firmly in the Gulf of California province. Santa Cruz Island lies in the transition zone with the western end of the island favoring North Pacific species and the eastern end favoring Gulf of California species. This makes the Northern Channel Islands a unique place to study how species distribution and recruitment respond to ocean temperatures.\n\n      Figure 1. The Southern California Bight (SCB) shown with a composite sea\n      surface temperatures (SST) color gradient for 2009. This SST is typical of\n      the region and illustrates the transition between the North Pacific and Gulf\n      of California biogegeographic provinces (source).\n\nThe El Niño Southern Oscillation (ENSO) is measured primarily by the Oceanic Niño Index which calculates a SST anomaly value value each month going back to 1950. These values are derived from SST in the equatorial pacific (\\(\\pm5^{\\circ}\\) latitude, \\(120^{\\circ}-170^{\\circ}W\\) longitude). The patterns of SST in this region are responsible for driving certain global weather and climate patterns. Despite the distance from this region, ENSO has a large effect on the oceanographic conditions of the SCB and the Channel Islands. This effect includes an influence on local SST as well as the strength and direction of ocean currents. These effects are known to influence the distribution and abundance of certain marine species (Day 2018, Freedman 2020).\nThis analysis takes a shotgun approach to investigate which species are sensitive to either warm-water or cold-water events by using a series of species-level dynamic linear models with lag SST anomalies as predictor variables. These models will provide a broad overview of which species are affected, how they are affected, and whether or not the affect is statistically significant. This will inform our understanding of how species distribution and population ranges might be affected by changing oceanographic conditions due to climate change.\nData\nThe Kelp Forest Monitoring (KFM) program at Channel Islands National Park has been collecting abundance and size distribution data of more than 170 species since 1982 (sampled annually). The park currently samples 33 sites at the five islands in the National Park. This data is publicly available and was requested and issued under a scientific research and collecting permit (permit #: CHIS-2020-SCI-0006). These data are collected using scuba surveys where the biologists attempt to minimize their impact on the reef by using non-invasive sampling methods. This means that animals are counted in situ and no rocks are overturned or species removed. This inevitably biases the data towards adult populations of certain invertebrates that utilize crevice habitat during development as they will be hidden from direct observation.\nThe fish data is collected in (#\\(/m^3\\)). In order to assure that all data are in the same units, only measurements of invertebrate and algae density (#\\(/m^2\\)) are retained. This takes the total number of species with density data from 151 down to 35. Using species density, we can identify trends in abundance observed over multiple El Niño (warm-water) and La Niña (cold-water) events. Using a dynamic linear model with lag independent variables we can see the effect of sea surface temperature anomalies during and in the years following these events on species density.\nBelow we read in ENSO data and calculate yearly mean anomalies before making 5 new columns with corresponding lag values. We then load in the species density data and filter out fish and 2 species which are exceedingly rare and do not have enough data to model. This leaves us with 33 species to model, all with the same units.\n\n\nhide\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(arrow)\nlibrary(plotly)\n\n# Load and Tidy Data\noni_yearly <-  # Read in Oceanic Nino Index region 3.4 data\n  read.table(\n    \"https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/detrend.nino34.ascii.txt\",\n    header = T) %>%\n  # rename YR for joining tables\n  dplyr::rename(SurveyYear = YR) %>% \n  dplyr::group_by(SurveyYear) %>% \n  # Calculate yearly SST Anom\n  dplyr::summarise(SST_Anom = mean(ANOM)) %>% \n  # Create lagged SST\n  dplyr::mutate(SST_Anom_1 = lag(SST_Anom, n = 1),\n                SST_Anom_2 = lag(SST_Anom, n = 2),\n                SST_Anom_3 = lag(SST_Anom, n = 3),\n                SST_Anom_4 = lag(SST_Anom, n = 4),\n                SST_Anom_5 = lag(SST_Anom, n = 5))\n\n\ndensity <-  # Read in Density data\n  arrow::read_feather(\"Density.feather\") %>%\n  dplyr::filter(Classification != \"Fish\",\n                !CommonName %in% c(\"wakame, adult\", \n                                   \"wakame, juvenile\", \n                                   \"white abalone\")) %>% \n  dplyr::left_join(oni_yearly)\n\n\n\nAnalysis\nThis analysis will use a general model formula of lm(Mean_Density ~ SST_Anom + SST_Anom_1 + SST_Anom_2 + SST_Anom_3 + SST_Anom_4 + SST_Anom_5) applied to density data grouped by each species scientific name. These five lag periods will show how a species is affected within the same year, and for the next five years following anomalous warm or cold water. We expect recruitment of certain species will either increases or decreases following warmer than usual or colder than usual ocean temperatures. Given what we know about the life cycle of many marine species, it is reasonable to assume these effects would not be detected by non-invasive sampling techniques for 1-5 years following anomalous temperatures.\nThis analysis is simplified to detect trends in a large number of species and does not take into account the effect of biogeography. This also does not consider the impact of marine protected areas (MPAs) which potentially buffer the ecological community from the effects of abiotic factors such as SST. MPAs could also have an effect from increased predation to some of these species, potentially limiting their abundance.\nThe cumulative effect on all species will be plotted. Then the results will be filtered to only include significant affects and sorted by the estimated slope of their relationship. The two species with the most positive slope and the two with the most negative slope will have their coefficients plotted over the lag periods.\n\n\nhide\n\nResults_lag <- density %>% \n  dplyr::group_by(ScientificName) %>%\n  dplyr::summarise(\n    generics::tidy(\n      stats::lm(\n        Mean_Density ~ SST_Anom + SST_Anom_1 + SST_Anom_2 + SST_Anom_3 + SST_Anom_4 + SST_Anom_5\n        ))) %>% \n  dplyr::filter(term != \"(Intercept)\") %>% \n  dplyr::mutate(significant = ifelse(p.value <= .05, \"yes\", \"no\"))\n\n\n\nResults\n\n\nhide\n\nResults_filtered <- Results_lag %>%\n  dplyr::filter(p.value <= .05) %>% \n  dplyr::arrange(estimate) %>%\n  dplyr::mutate(statistic = round(statistic, 3),\n                p.value = round(p.value, 3),\n                p.value = ifelse(p.value < 0.001, \"< 0.001\", as.character(p.value))) \ncold_sp <- Results_filtered %>% \n  arrange(estimate) %>% \n  distinct(ScientificName) %>% \n  head(3) %>% \n  pull()\n\nwarm_sp <- Results_filtered %>% \n  arrange(desc(estimate)) %>% \n  distinct(ScientificName) %>% \n  head(3) %>% \n  pull()\n\ncold <- Results_lag %>% \n  group_by(ScientificName) %>% \n  mutate(cum_est = cumsum(estimate)) %>% \n  filter(ScientificName %in% cold_sp)\n\nwarm <- Results_lag %>% \n  group_by(ScientificName) %>% \n  mutate(cum_est = cumsum(estimate)) %>% \n  filter(ScientificName %in% warm_sp)\n\nplt <- function(.data = cold) {\n  ggplot(data = .data, aes(x = term, y = cum_est)) +\n    geom_line(aes(color = ScientificName, group = ScientificName), size = 1) +\n    geom_point(aes(shape = significant), size = 4) +\n    scale_y_continuous(expand = c(0, 1)) +\n    scale_x_discrete(labels = c(\"Current Year\", \"+1\", \"+2\", \"+3\", \"+4\", \"+5\")) +\n    scale_color_viridis_d(option = \"D\", end = .75, direction = -1) +\n    labs(x = \"Years relative to SST anomaly exposure\", y = \"Cumulative effect (sum of coefficients)\",\n         shape = \"P-Val < .05?\", color = \"Species\") +\n    theme_classic() \n}\n\n\n\nCold-water species\nThe two species with the largest negative cumulative effect were identified as Strongylocentrotus purpuratus (purple sea urchin) and Strongylocentrotus franciscanus (red sea urchin). Purple sea urchin seem to have the most extreme negative coefficients, suggesting that this species thrives under cold-water conditions. Red sea urchins have a similar though less extreme response to cold-water conditions. In both urchin species, the effect is significant in the same year, as well as the year following cold-water conditions. This makes sense, because despite non-invasive sampling, juvenile sea urchins as small as 1 mm have been recorded by KFM observers.\nPatiria miniata (bat star) had the third most negative cumulative effect. This species is particularly affected by warm water, which tends to cause a form of sea star wasting disease (SSWD). Bat star populations take years to rebound following SSWD events. This slow recovery is seen in the cumulative effect which is statistically significant for the current period and for the following four years.\n\n\nhide\n\nplt()\n\n\n\n\n      Figure 2. The cumulative effect that SST anomalies have on the three species\n      which have the largest negative slope estimates of all statistically significant\n      results. The effect accumulates from the current year to the following 5 years.\n\nWarm-water species\nSargassum horneri (devil weed) had the single most positive cumulative effect of any species. This is an invasive algae that has spread rapidly at the Channel Islands since its introduction to Catalina island in 2006 (Miller 2007). Predicting what factors contribute to the spread of S. horneri will help scientist understand where to expect it next and what impacts it might have on the ecologic communities it settles in. Only the current year is seen to be positive, suggesting that S. horneri juveniles are largely responsible for the effect.\nMegastaea undosa (wavy turban snail) is an important grazing snail that is known to spawn in warm-water conditions which means that ENSO may be helpful in setting harvest quotas (Zacharias 2006). The effect is only significant 2, 3, and 4 years following a warm water event. I would argue that these snails spawn late in the year when the water is warmest and the sampling season is coming to a close. The following year, the juvenile snails are most likely missed as they are too cryptic. Then during the 2nd, 3rd, and 4th year, these animals are largest enough to be observed reliably.\nLytechinus anamesus likely follow the same pattern as M. undosa. I would be interested to investigate MPA effects on these lobster and sheephead snack-sized sea urchin (<40 mm typically).\n\n\nhide\n\nplt(.data = warm)\n\n\n\n\n      Figure 3. The cumulative effect that SST anomalies have on the three species\n      which have the largest positive slope estimates of all statistically significant\n      results. The effect accumulates from the current year to the following 5 years.\n\nFuture Work\nBetter Models\nInclude biogeogrphic regions to see how species abundance is affected by region. I would also like to include MPA status to identify if there are any buffer effects provided by the MPA, or impacts to prey species from increased preddation.\nMore Protocols/Species\nThis analysis only uses 2 out of 13 sampling protocols of the KFM program. The analysis could be expanded to include data from the two fish surveys, as well as percent cover data from random point contacts survey.\nIntegrate Program Data\nThis analysis could be made more robust by including data from the Partnership for Interdisciplinary Study of Coastal Oceans (PISCO), which conducts similar monitoring efforts around the Channel Islands.\nCode Availability\nGitHub Repository with all code and data available at this link.\nLiterature Cited\nCostello, M.J., Tsai, P., Wong, P.S. et al. Marine biogeographic realms and species endemicity. Nat Commun 8, 1057 (2017). https://doi.org/10.1038/s41467-017-01121-2\nDay, P. B., Stuart-Smith, R. D., Edgar, G. J. & Bates, A. E. Species’ thermal ranges predict changes in reef fish community structure during 8 years of extreme temperature variation. Divers. Distrib. 24, 1036–1046 (2018).\nFreedman, R.M., Brown, J.A., Caldow, C. et al. Marine protected areas do not prevent marine heatwave-induced fish community structure changes in a temperate transition zone. Sci Rep 10, 21081 (2020). https://doi.org/10.1038/s41598-020-77885-3\nHorta e Costa, B. Tropicalization of fish assemblages in temperate biogeographic transition zones. Mar. Ecol. Prog. Ser. 504, 241–252 (2014).\nMiller, K. A., Engle, J. M., Uwai, S., & Kawai, H. (2007). First report of the asian seaweed sargassum filicinum harvey (fucales) in california, usa. Biological Invasions, (9), 609-613.\nWernberg, T. S. et al. Climate-driven regime shift of a temperate marine ecosystem. Science 353, 169–172 (2016).\nZacharias, Mark, and David J. Kushner. 2006. “Sea temperature and wave height as predictors of population size structure and density of Megastraea (Lithopoma) undosa: Implications for fishery management.” Bulletin of Marine Science 79.1: 71-82.\n\n\n\n",
    "preview": "posts/2021-12-05-species-density-sst-lagsst/eds-222-final-project-species-density-sst_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 960
  },
  {
    "path": "posts/2021-12-04-effect-of-enso-on-coral-reefs/",
    "title": "Effect of ENSO on Coral Reefs",
    "description": "\"Our group discovered the Scott Reef and Rowley Shoals Coral Bleaching Data dataset while searching the DataOne repository for “coral bleaching.” This particular dataset focuses on long-term monitoring data from 1994 to 2017 at reef slope habitats off the coast of northwestern Australia.\"",
    "author": [
      {
        "name": "Cullen Molitor, Desik Somasundaram, Ryan Munnikhuis, and Julia Parish",
        "url": "https://github.com/desik23/eds-213-group-project"
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "R",
      "Ocean Data",
      "Ecology",
      "ENSO"
    ],
    "contents": "\n\n\nhide\n\nlibrary(metajam)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(vegan)\nlibrary(zoo)\n\n\n\nEDS 213 Group Project\nOur group discovered the Scott Reef and Rowley Shoals Coral Bleaching Data dataset while searching the DataOne repository for “coral bleaching.” This particular dataset focuses on long-term monitoring data from 1994 to 2017 at reef slope habitats off the coast of northwestern Australia. Data Source: https://search.dataone.org/view/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fmetadata%2Feml%2Fedi%2F952%2F1. The metadata includes the reef system, date range, taxonomy, habitat type, coral coverage, and dataset methods.\nDownload data\n\n\nhide\n\n# assign data url to access coral data from DataOne then download\ndata_url <- \"https://cn.dataone.org/cn/v2/resolve/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F952%2F1%2Ff6212784c45d0a077f2c863868d22c4b\"\n# If data is akready up to date, this will throw an error\n# error=T in header will move past this issue\ndownload_d1_data(data_url, dir_name = \"data\", path = \".\")\n\n\n[1] \"./data\"\n\nLoad and tidy data\n\n\nhide\n\n# assign data path\ndata_path <- \"data\"\n# Read in data with metajam\ncoral_list <- read_d1_files(data_path)\n# Pick out the data read in above and clean it up\ncorals <- coral_list$data %>% \n  janitor::clean_names() %>% \n  dplyr::mutate(year_decimal = format(date_decimal(year_decimal), \"%Y-%m-%d\"),\n                month = month(year_decimal),\n                year = year(year_decimal)) %>% \n  dplyr::rename(date = year_decimal)\n# Read in  ONI to be joined to coral data\noni <- read.table(\n  \"https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/detrend.nino34.ascii.txt\",\n  header = T) %>%\n  dplyr::mutate(date = as.Date(ISOdate(YR, MON, 1)),\n                date_start = as.Date(ISOdate(YR, MON, 1)),\n                date_end = lubridate::ceiling_date(date_start, \"month\")) %>%\n  dplyr::rename(oni_anomaly = ANOM,\n                month = MON,\n                year = YR) %>% \n  dplyr::select(year, month, oni_anomaly, date_start, date_end) %>% \n  dplyr::mutate(roll_3_month_mean = zoo::rollmean(x = oni_anomaly, k = 3, fill = NA, align = \"right\")) %>% \n  dplyr::filter(date_start > lubridate::ymd(\"1994-09-01\"))\n# Calculate the Simpson's diversity index \n# join corals and ONI data\ncoral_oni <- corals %>% \n  tidyr::pivot_longer(cols = 5:18, names_to = \"species\", values_to = \"cover\") %>% \n  dplyr::group_by(system, reef, year, month, location) %>% \n  dplyr::summarise(simpson_index = vegan::diversity(cover, index = \"simpson\")) %>% \n  dplyr::left_join(oni %>% dplyr::select(-date_start, -date_end)) %>% \n  dplyr::mutate(date = lubridate::make_date(year = year, month = month, day = 1))\n\n\n\nPlot\n\n\nhide\n\n# Plot the coral diversity data with ONI color bar\nggplot2::ggplot() +\n  ggplot2::geom_rect(\n    data = oni,\n    aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = .3, fill = oni_anomaly)) +\n  ggplot2::scale_fill_viridis_c(\n    option = \"plasma\",\n    guide = guide_colorbar(direction = \"horizontal\", title.position = \"top\",\n                           order = 2, barheight = unit(.2, \"cm\"))) +\n  ggplot2::scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\", expand = expansion(mult = c(0,0))) +\n  ggplot2::geom_line(data = coral_oni, size = 1,\n                     aes(x = date, y = simpson_index, color = location)) +\n  ggplot2::scale_color_viridis_d() +\n  ggplot2::guides(color = guide_legend(order = 1)) +\n  ggplot2::labs(fill = \"Oceanic Ni\\u00f1o Index\", color = \"Sites\", \n                x = \"Date\", y = \"Simpsons Diversity Index\") +\n  ggplot2::theme_minimal()\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-12-04-effect-of-enso-on-coral-reefs/effect-of-enso-on-coral-reefs_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 768
  },
  {
    "path": "posts/2021-12-04-modeling-enso-and-wave-height/",
    "title": "Modeling ENSO and Wave Height",
    "description": "We are interested in investigating the effect of El Niño on our local oceanographic conditions. We used linear regression models to plot and compare equatorial sea surface temperature anomalies with local Santa Barbara temperature and wave height. Typically temperature anamolies are calcuated from a 30 year base period. Since we only had ~20 years of consistent data, we calculated monthly means since 1997 and used these as base values from which to compare monthly means for each year.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": "https://github.com/mk-waves/mk-waves-tutorial"
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "Python",
      "Ocean Data",
      "Remote Sensing",
      "ENSO"
    ],
    "contents": "\n\n↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\nClick the binder icon above to begin a code along tutorial. A remote python session will launch in your browser with the tutorial Be patient, sometimes it takes a while for the session to initialize the first few times.\nHW3 Making Waves: CDIP Dataset Tutorial and Use Case Examples\nView and evaluating wave height and temperature from the Coastal Data Information Program\nEDS 220, Fall 2021\nAuthors\nCullen Molitor, UC Santa Barbara (cullen_molitor@bren.ucsb.edu) https://cullen-molitor.github.io/\nJake Eisaguirre, UC Santa Barbara (eisaguirre@bren.ucsb.edu) https://jake-eisaguirre.github.io/\nMarie Rivers, UC Santa Barbara (mrivers@bren.ucsb.edu) https://marierivers.github.io/\nTable of Contents\n1. Purpose\n2. Dataset Description\n3. Dataset Input/Output\n4. Metadata\n5. Visualize Data\n6. Use Case Examples\n7. References\n\nNotebook Purpose\nThis notebook was created to provide an introduction to NetCDF4 files and data from the Coastal Data Information Program (CDIP) at the Scripps Institute of Oceanography (SIO). This tutorial is based on a python API which loads netCDF files. We also provide methods for using sea surface temperature and swell height to compare correlations with El Nino/Southern Oscillation (ENSO) behavior at a specific location.\nWhile these methods are applicable to other CDIP wave buoy stations, we chose the Harvest Buoy (CDIP site) to get information close to our local Santa Barbara, CA coast. The metadata is documented on the CDIP informational site.\nWe are also interested to see how the El Niño Southern Oscillation (ENSO) effects our local waters in Santa Barbara California. To do this, we use the primary measurement of the strength of ENSO, the Oceanic Niño Index, which calculates a monthly anomaly value for region 3.4 in the equatorial pacific. Region 3.4 was chosen as it is the first zone to indicate ENSO and commonly used in ENSO research.\n\n\n\n\nBuoy Dataset Description\nFile format: NetCDFData retrieval source: CDIP portal and CDIP Python API\nOperator: CDIPCollaborators: CleanSeasFunding: California Division of Boating and Waterways (CDBW) & US Army Corps of Engineer (USACE)\nData Availability: CDIP data and products are free available for public useLicense: These data may be redistributed and used without restriction\nGlobal coverage of stations:\nHarvest buoy period of record: October 1991 - present\nMeasured wave parameters\nWave height is measured as the distance between the trough and the crest of a wave. Wave period measurements represent the time between two consecutive crests. Wave direction is the compass angle (0-360 degrees clockwise from true North) that the waves are coming from. Ocean waves never have just a single direction or period. Therefore, measurements of peak period (Tp) and peak direction (Dp) are reported. The peak period is the most common period between consecutive waves, and peak direction is the most common direction.\nHs = wave height, 30-minute average of the ⅓ highest waves at a sensor\nTp = peak period (most common period between consecutive waves)\nDp = peak direction (most common direction)\nCDIP documentation notes that statistical estimates of wave parameters have been developed by analyzing time-series measurements of a natural sea state. For example, significant wave height was designed to correspond to wave height estimates from experienced observer and is reported as the mean of the largest 1/3 (33%) of waves recorded during a sampling period. Ocean conditions constantly change and significant wave heights are statistical measures rather than measures corresponding to any specific wave. A given sampling period will have many waves smaller than Hs and some larger than Hs. Statistically, the largest wave in a 1,000 wave sample is likely to be ~1.8 times the reported significant wave height.\n\n\n\nEquipment\nCDIP uses Waverider (Mark III) directional buoys manufactured by Datawell that are equipped with accelerometers that measure wave height by recording the vertical acceleration of the buoy as it rises and falls with passing waves. Wave direction is recorded by horizontal accelerometers that measure north/south and east/west displacements. These buoys effectively measures waves with periods ranging from 1.6 to 30 seconds, with an error less than 3%. Temperature is measured by these buoys with a sensor is located approximately 18 inches below the watersurface. These buoys have a diameter of 0.9 meters. Available equipment documentation did not provide frequency of measurements.\nTemporal notes\nWave calcs use ~30 minute data samples\nTime assigned to the data is start time\nAll data collected is archived by UTC time\nData Quality\nhigh quality publicly released data excludes all records flagged by quality control procedures\nactivity log documents deployments, transmission problems, maintenance issues, and battery/power failures\n\nBuoy Dataset Input/Output\nImport required packages\n\n\n\nimport netCDF4\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport time\nimport calendar\nimport pandas as pd\nfrom matplotlib import gridspec\nfrom matplotlib import cm\nimport matplotlib as mpl\nSet parameters\nnames of any directories where data are stored\nranges of years over which data are valid\nany thresholds or latitude/longitude ranges to be used later (e.g. dimensions of NINO3.4 region, threshold SSTA values for El Nino, etc.)\nThe harvest buoy, CDIP station number of 071, was selected for our regional analysis of the Santa Barbara Coast.\nLink to other buoy stations\n# Type\nstn = '071'\nRead in the data\nwe use the netCDF4 module to read in the archived buoy data.\n# CDIP Archived Dataset URL\ndata_url = 'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive/' + stn + 'p1/' + stn + 'p1_historic.nc'\ndata = netCDF4.Dataset(data_url)\n\nMetadata\nThe CDIP metadata includes general information about the Harvest buoy such as current status, location, instrument description and most recent measurement values. The metadata also includes the availability of parameters for each month that the station has been in operation. The code below further explores metadata associated with this dataset\nThe type function in Python was used to display the file type of the data\n# return the file type\ntype(data)\nnetCDF4._netCDF4.Dataset\nThe summary Python function was used to display summary text of the data\n# return a summary of the dataset\ndata.summary\n'Directional wave and sea surface temperature measurements collected in situ by Datawell Waverider buoys located near HARVEST, CA from 1991/10/22 to 2019/06/17. This dataset includes publicly-released data only, excluding all records flagged bad by quality control procedures. A total of 372110 wave samples were analyzed for this area, where the water depth is approximately 183 to 549 meters.'\nThe command .variables.keys() was used to return all variables included in the dataset.\n# return all variables included in the dataset\nprint(data.variables.keys())\ndict_keys(['sourceFilename', 'waveTime', 'waveTimeBounds', 'waveFlagPrimary', 'waveFlagSecondary', 'waveHs', 'waveTp', 'waveTa', 'waveDp', 'wavePeakPSD', 'waveTz', 'waveSourceIndex', 'waveFrequency', 'waveFrequencyBounds', 'waveFrequencyFlagPrimary', 'waveFrequencyFlagSecondary', 'waveBandwidth', 'sstTime', 'sstTimeBounds', 'sstFlagPrimary', 'sstFlagSecondary', 'sstSeaSurfaceTemperature', 'sstSourceIndex', 'sstReferenceTemp', 'gpsTime', 'gpsTimeBounds', 'gpsStatusFlags', 'gpsLatitude', 'gpsLongitude', 'gpsSourceIndex', 'dwrTime', 'dwrTimeBounds', 'dwrSourceIndex', 'dwrBatteryLevel', 'dwrZAccelerometerOffset', 'dwrXAccelerometerOffset', 'dwrYAccelerometerOffset', 'dwrOrientation', 'dwrInclination', 'dwrBatteryWeeksOfLife', 'metaDeployLatitude', 'metaDeployLongitude', 'metaWaterDepth', 'metaDeclination', 'metaStationName', 'metaStationLatitude', 'metaStationLongitude', 'metaPlatform', 'metaInstrumentation', 'metaGridMapping', 'waveEnergyDensity', 'waveMeanDirection', 'waveA1Value', 'waveB1Value', 'waveA2Value', 'waveB2Value', 'waveCheckFactor', 'waveSpread', 'waveM2Value', 'waveN2Value'])\nThe Harvest buoy collects data in the following categories:\nSignificant Wave Height\nSwell Height\nSwell Period\nSwell Direction\nWind Wave Height\nWind Wave Period\nWind Wave Direction\nWave Steepness\nAverage Wave Period\nWater Temperature\nFor this tutorial we are interested in:\nSignificant Wave Height (waveHs)\nSea Surface Temperature (sstSeaSurfaceTemperature)\nBy using a print statement with the dataframe name and variable in brackets, you can view the long name, units, minimum values, and maximum values of a variable\n# learn more about a variable including long name, units, valid min/max values\nprint(data['sstSeaSurfaceTemperature'])\n<class 'netCDF4._netCDF4.Variable'>\nfloat32 sstSeaSurfaceTemperature(sstTime)\n    long_name: sea surface temperature\n    units: Celsius\n    _FillValue: -999.99\n    standard_name: sea_surface_temperature\n    coordinates: metaStationLatitude metaStationLongitude\n    grid_mapping: metaGridMapping\n    valid_min: -5.0\n    valid_max: 46.15\n    ancillary_variables: sstFlagPrimary sstFlagSecondary\n    ncei_name: SEA SURFACE TEMPERATURE\n    cell_methods: sstTime: point\nunlimited dimensions: \ncurrent shape = (372015,)\nfilling off\n# Other Variables\n# data.variables\n# Hs = data.variables['waveHs']\n# Tp = data.variables['waveTp']\n# Dp = data.variables['waveDp'] \nClean data\nThanks to the data processing methods used by CDIP, the downloaded datafiles did not require much actual cleaning. Since the sea surface temperature and wave height variables had different timestamps associated with each measurements we pulled out these variables for use in creating new dataframes that were used as an intermediate step in our overall analysis\nExample code found here\n# Get SST timestamp variable \nsst_time_var = data.variables['sstTime']\n\n# Get SST variable \nsst = data.variables['sstSeaSurfaceTemperature'][:]\n\n# Get wave height timestamp variable\nwave_time_var = data.variables['waveTime']\n\n# Get wave height variable \nwave = data.variables['waveHs'][:]\nWe used the cftime Python library for decoding time units and variable values in a netCDF file conforming to the Climate and Forecasting (CF) netCDF conventions.\ncftime documentation\nFirst we created dataframes for the sea surface temperature sst_df and wave height wave_df data. We then aggregated this data by month and joined the dataframes to create one dataframe of buoy variables.\n# Use num2date on sst_time_var\nsst_time = netCDF4.num2date(sst_time_var[:], sst_time_var.units, only_use_cftime_datetimes=False)\n# Make an empty pandas dataframe\nsst_df = pd.DataFrame()\n\n# Fill it with SST and the date time it was collected\nsst_df['sst'] = sst\nsst_df['date_time'] = sst_time\nThe code below creates columns for the date, month, and year of each observation timestamp. These values will later be used to calculate monthly means.\n# Make date_time column a pandas date_time\nsst_df['date_time'] = pd.to_datetime(sst_df['date_time']) \n\n# Pull out date from datetime\nsst_df['date'] = sst_df['date_time'].dt.date\n\n# Pull out month from datetime\nsst_df['month'] = sst_df['date_time'].dt.month\n\n# Pull out year from datetime\nsst_df['year'] = sst_df['date_time'].dt.year\n# Use num2date on wave_time_var\nwave_time = netCDF4.num2date(wave_time_var[:], wave_time_var.units, only_use_cftime_datetimes=False)\n# Make an empty pandas dataframe\nwave_df = pd.DataFrame()\n\n# Fill it with SST and the date time it was collected\nwave_df['wave'] = wave\nwave_df['date_time'] = wave_time\n# Make date_time column a pandas date_time\nwave_df['date_time'] = pd.to_datetime(wave_df['date_time']) \n\n# Pull out date from datetime\nwave_df['date'] = wave_df['date_time'].dt.date\n\n# Pull out month from datetime\nwave_df['month'] = wave_df['date_time'].dt.month\n\n# Pull out year from datetime\nwave_df['year'] = wave_df['date_time'].dt.year\nThe next two code chunks let you view the newly created sea surface temperature and wave height dataframes\n# Inspect data\nsst_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nsst\n\n\ndate_time\n\n\ndate\n\n\nmonth\n\n\nyear\n\n\n0\n\n\n16.00\n\n\n1991-10-22 08:21:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n1\n\n\n16.00\n\n\n1991-10-22 08:51:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n2\n\n\n15.90\n\n\n1991-10-22 09:21:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n3\n\n\n15.90\n\n\n1991-10-22 09:51:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n4\n\n\n15.85\n\n\n1991-10-22 10:21:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\nwave_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nwave\n\n\ndate_time\n\n\ndate\n\n\nmonth\n\n\nyear\n\n\n0\n\n\n1.50\n\n\n1991-10-22 07:52:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n1\n\n\n1.48\n\n\n1991-10-22 08:22:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n2\n\n\n1.54\n\n\n1991-10-22 08:52:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n3\n\n\n1.56\n\n\n1991-10-22 09:22:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\n4\n\n\n1.59\n\n\n1991-10-22 09:52:00\n\n\n1991-10-22\n\n\n10\n\n\n1991\n\n\nENSO Dataset\nEl Nino/Southern Oscillation (ENSO) data was obtained from the National Oceanic and Atmospheric Administration (NOAA) Climate Prediction Center. The code below goes through the steps to download and visualize this data then manipulate it to use in conjunction with the buoy dataset. We used Nino Region 3.4 for our analysis.\nRead in ENSO data\nERSST5 Extended Reconstructed Sea Surface Temperature (SST) V5Nino Regions\n\n\n\nWe use sep = '\\s{2,}' because the data linked above is seperated by two or more spaces, not a comma or other typical delimiter.\npath = \"https://www.cpc.ncep.noaa.gov/data/indices/ersst5.nino.mth.91-20.ascii\"\nenso = pd.read_csv(path, sep = '\\s{2,}', engine = 'python')\nenso.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nYR\n\n\nMON\n\n\nNINO1+2\n\n\nANOM\n\n\nNINO3\n\n\nANOM.1\n\n\nNINO4\n\n\nANOM.2\n\n\nNINO3.4\n\n\nANOM.3\n\n\n0\n\n\n1950\n\n\n1\n\n\n23.01\n\n\n-1.55\n\n\n23.56\n\n\n-2.10\n\n\n26.94\n\n\n-1.38\n\n\n24.55\n\n\n-1.99\n\n\n1\n\n\n1950\n\n\n2\n\n\n24.32\n\n\n-1.78\n\n\n24.89\n\n\n-1.52\n\n\n26.67\n\n\n-1.53\n\n\n25.06\n\n\n-1.69\n\n\n2\n\n\n1950\n\n\n3\n\n\n25.11\n\n\n-1.38\n\n\n26.36\n\n\n-0.84\n\n\n26.52\n\n\n-1.80\n\n\n25.87\n\n\n-1.42\n\n\n3\n\n\n1950\n\n\n4\n\n\n23.63\n\n\n-1.90\n\n\n26.44\n\n\n-1.14\n\n\n26.90\n\n\n-1.73\n\n\n26.28\n\n\n-1.54\n\n\n4\n\n\n1950\n\n\n5\n\n\n22.68\n\n\n-1.74\n\n\n25.69\n\n\n-1.57\n\n\n27.73\n\n\n-1.18\n\n\n26.18\n\n\n-1.75\n\n\n# drop unused columns from ENSO data\nenso = enso.drop(enso.columns[[2, 3, 4, 5, 6, 7, 8]], axis=1)\n\n# rename columns\nenso = enso.rename(columns={\"YR\":\"year\", \"MON\":\"month\", \"ANOM.3\": \"enso_anom\"})\nenso.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nyear\n\n\nmonth\n\n\nenso_anom\n\n\n0\n\n\n1950\n\n\n1\n\n\n-1.99\n\n\n1\n\n\n1950\n\n\n2\n\n\n-1.69\n\n\n2\n\n\n1950\n\n\n3\n\n\n-1.42\n\n\n3\n\n\n1950\n\n\n4\n\n\n-1.54\n\n\n4\n\n\n1950\n\n\n5\n\n\n-1.75\n\n\n\nVisualize data\nHarvet buoy visualization\nWe started by visualizing the full dataset to inform our further analysis. Based on the data gaps shown in the plot, we decided to begin our analysis at 1997.\n# Plot here to see what the data looks like\nf, (pHs, pSst) = plt.subplots(2, 1, sharex=True, figsize=(15,10)) \npSst.plot(sst_df.date, sst_df.sst, linewidth = 0.5)\npHs.plot(wave_df.date, wave_df.wave, linewidth = 0.5)\nplt.title(\"Harvest Buoy\", fontsize=30, y = 2.3)\npHs.set_ylabel('Wave Height, m', fontsize=18)\npSst.set_ylabel('SST, C', fontsize=18)\nText(0, 0.5, 'SST, C')\npngENSO anomaly visualization\nThe below plot shows the Oceanic Nino Index anaomaly vvalues from 1950 to the most current available data. The area under/over the curve to the x-axis is colored by the intensity of the anomaly. Anomaly values over 0.5°C for 5 consecutive 3 month seasonal averages are considered to be El Niño. Anomaly values under -0.5°C for 5 consecutive 3 month seasonal averages are considered to be La Niña.\nNext we assign() an arbitrary day value of 1 in order to make a date with pd.to_datetime() function\nenso = enso.assign(day = 1)\nenso['date'] = pd.to_datetime(enso[['year', 'month', 'day']])\n# Choose color gradient here: https://matplotlib.org/stable/tutorials/colors/colormaps.html \n\ncolourmap = cm.get_cmap('magma')\n\nxx = enso.date\nyy =  enso.enso_anom\n\nplt.figure(figsize = (15, 5))\nplt.plot(xx,yy, color = 'none')\n\nnormalize = mpl.colors.Normalize(vmin=yy.min(), vmax=yy.max())\nnpts = len(enso)\nfor i in range(npts - 1):\n    plt.fill_between([xx[i], xx[i+1]],\n                     [yy[i], yy[i+1]],\n                     color=colourmap(normalize(yy[i]))\n                     ,alpha=0.6)\nplt.show()\npng\nUse Cases Examples\nObservational Data vs Modeled Data\nThe CDIP buoy network is a valuable resource for marine scientists, coastal managers, and mariners. The historic data (observational) is archived and is of great use for understanding past patterns and provides context for understanding the oceanographic conditions of our local ocean. Real time data (observational) gives an better idea of what to expect as wave energy moves past the buoy and towards our shoreline. It is also incredibly useful for mariners navigating these sometimes harsh and volatile waters. Forecasted data (modeled) is perhaps the most practical use case for mariners, as they not only need to know the current conditions but they need to know how those conditions are changing in order to navigate safely.\nCDIP provide what they call ‘nowcasted’ data and ‘forecasted’ data. - Nowcasted provides insight at a higher resolution for the upcoming 6-hour period. These predictions tend to be more accurate and are generally fairly trustworthy. - Forecasted data provide a look further into the future, but with a greater amount of uncertainty\nOur interest\nWe are interested in investigating the effect of El Niño on our local oceanographic conditions. We used linear regression models to plot and compare equatorial sea surface temperature anomalies with local Santa Barbara temperature and wave height. Typically temperature anomalies are calculated from a 30 year base period. Since we only had ~20 years of consistent data, we calculated monthly means since 1997 and used these as base values from which to compare monthly means for each year. The results of this analysis are useful to scientist, marine managers, marine conservation groups and coastal communities.\nOther General Use Cases\nModeling coastal erosion\nInvestigating storm frequency and intensity over time\nInvestigating changes in SST over time\nDetecting upwelling events\nThe code chunks below take the sea surface temperature and wave height dataframes created from the buoy data and group values by month and year so that monthly averages can be calculated. The two datasets are then grouped into a single buoy dataframe, buoy_df.\n# Filter data to be greater than 1997 due to missing values before that time\nsst_df = sst_df[sst_df['year'] > 1997]\n\nwave_df = wave_df[wave_df['year'] > 1997]\n# Group by date and summarise with mean SST and Wave Height\nsst_monthly = sst_df.groupby(['month', 'year']).agg({'sst': 'mean'})\n\nwave_monthly = wave_df.groupby(['month', 'year']).agg({'wave': 'mean'})\n# Inspect Data\nprint(sst_monthly.head())\nprint(wave_monthly.head())\n                  sst\nmonth year           \n1     1999  13.194914\n      2000  13.023686\n      2001  13.454234\n      2002  12.896236\n      2003  14.313378\n                wave\nmonth year          \n1     1999  2.467315\n      2000  2.126410\n      2001  2.971848\n      2002  2.609128\n      2003  2.432630\n# join monthly sst and wave data into a buoy dataframe\nbuoy_df = sst_monthly.join(wave_monthly)\nbuoy_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n\nsst\n\n\nwave\n\n\nmonth\n\n\nyear\n\n\n\n\n\n\n1\n\n\n1999\n\n\n13.194914\n\n\n2.467315\n\n\n2000\n\n\n13.023686\n\n\n2.126410\n\n\n2001\n\n\n13.454234\n\n\n2.971848\n\n\n2002\n\n\n12.896236\n\n\n2.609128\n\n\n2003\n\n\n14.313378\n\n\n2.432630\n\n\nNext we used reset_index to ungroup the buoy data\nbuoy_df = buoy_df.reset_index()\nbuoy_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nmonth\n\n\nyear\n\n\nsst\n\n\nwave\n\n\n0\n\n\n1\n\n\n1999\n\n\n13.194914\n\n\n2.467315\n\n\n1\n\n\n1\n\n\n2000\n\n\n13.023686\n\n\n2.126410\n\n\n2\n\n\n1\n\n\n2001\n\n\n13.454234\n\n\n2.971848\n\n\n3\n\n\n1\n\n\n2002\n\n\n12.896236\n\n\n2.609128\n\n\n4\n\n\n1\n\n\n2003\n\n\n14.313378\n\n\n2.432630\n\n\nHere we calculate anomalies for buoy sea surface temperature and wave height values.\n# calculate the anomalies\nbuoy_df = buoy_df.groupby(['month', 'year']).agg({'sst': 'mean', 'wave': 'mean'}) - \\\n    buoy_df.groupby(['month']).agg({'sst': 'mean', 'wave': 'mean'}) \nbuoy_df.head()\n\n# since this line of code was so long, use used a `\\` to break up the line. You can also split up lines at commas\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n\nsst\n\n\nwave\n\n\nmonth\n\n\nyear\n\n\n\n\n\n\n1\n\n\n1999\n\n\n-0.458099\n\n\n-0.088380\n\n\n2000\n\n\n-0.629327\n\n\n-0.429286\n\n\n2001\n\n\n-0.198779\n\n\n0.416153\n\n\n2002\n\n\n-0.756777\n\n\n0.053433\n\n\n2003\n\n\n0.660365\n\n\n-0.123065\n\n\nNote: You need to reset the index each time you group the data\nbuoy_df = buoy_df.reset_index()\nbuoy_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nmonth\n\n\nyear\n\n\nsst\n\n\nwave\n\n\n0\n\n\n1\n\n\n1999\n\n\n-0.458099\n\n\n-0.088380\n\n\n1\n\n\n1\n\n\n2000\n\n\n-0.629327\n\n\n-0.429286\n\n\n2\n\n\n1\n\n\n2001\n\n\n-0.198779\n\n\n0.416153\n\n\n3\n\n\n1\n\n\n2002\n\n\n-0.756777\n\n\n0.053433\n\n\n4\n\n\n1\n\n\n2003\n\n\n0.660365\n\n\n-0.123065\n\n\n# rename columns\nbuoy_df = buoy_df.rename(columns = {'sst':'buoy_sst_anom', 'wave':'buoy_wave_anom'})\n\n# Assign arbitrary day value\nbuoy_df = buoy_df.assign(day = 1) \n\n# set datetime \nbuoy_df['date'] = pd.to_datetime(buoy_df[['year', 'month', 'day']])\n\n# sort values\nbuoy_df = buoy_df.sort_values(by=['date'])\n\n# set the index for plotting\nbuoy_df = buoy_df.set_index('date')\n# Inspect\nbuoy_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nmonth\n\n\nyear\n\n\nbuoy_sst_anom\n\n\nbuoy_wave_anom\n\n\nday\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n1998-03-01\n\n\n3\n\n\n1998\n\n\n1.834283\n\n\n0.105816\n\n\n1\n\n\n1998-04-01\n\n\n4\n\n\n1998\n\n\n0.914924\n\n\n-0.130027\n\n\n1\n\n\n1998-05-01\n\n\n5\n\n\n1998\n\n\n0.674051\n\n\n-0.090042\n\n\n1\n\n\n1998-06-01\n\n\n6\n\n\n1998\n\n\n0.614153\n\n\n-0.300137\n\n\n1\n\n\n1998-07-01\n\n\n7\n\n\n1998\n\n\n1.214789\n\n\n0.132242\n\n\n1\n\n\nJoining enso and buoy data: Here we join the buoy dataframe and ENSO dataframe by date and create a basic plot of each anomaly.\nbuoy_df = buoy_df.drop([\"month\", \"year\", \"day\"], axis=1).reset_index()\nenso = enso.drop([\"year\", \"month\", \"day\"], axis = 1)\nanom_df = buoy_df.set_index('date').join(enso.set_index('date')).reset_index()\n# Plot sst and wave height anomaly from buoy along with ENSO sst anomaly\nf, (bwa, bsa, esa ) = plt.subplots(3, 1, sharex=True, figsize=(15,10))\n\nbwa.plot(anom_df.date, anom_df.buoy_wave_anom)\nbwa.fill_between(anom_df.date, anom_df.buoy_wave_anom, 0, alpha=0.30)\nbwa.axhline(0,color='red')\nbwa.set_ylabel('Buoy Wave Height Anomaly')\n\nbsa.plot(anom_df.date, anom_df.buoy_sst_anom)\nbsa.fill_between(anom_df.date, anom_df.buoy_sst_anom,  0, alpha=0.30)\nbsa.axhline(0,color='red')\nbsa.set_ylabel('Buoy SST Anomaly')\n\nesa.plot(anom_df.date, anom_df.enso_anom)\nesa.fill_between(anom_df.date, anom_df.enso_anom,  0, alpha=0.30)\nesa.axhline(0,color='red')\nesa.set_ylabel('ENSO Anomaly')\nText(0, 0.5, 'ENSO Anomaly')\npngModel\nWe used the statsmodels package to develop linear regression models of the anomaly dataframes created in the sections above. We modeled anomalies for buoy sea surface temperature vs. ENSO sea surface temperature, buoy wave height vs. ENSO sea surface temperature, and buoy wave height vs. buoy sea surface temperature.\nimport statsmodels.api as sm\nBuoy sea surface tempurature anomaly vs. ENSO sea surface temperature anomaly\n# Simple linear model: buoy sst anomaly vs enso sst anomaly\nmodel = sm.OLS(anom_df.buoy_sst_anom, anom_df.enso_anom)\n\n# Model results\nresults = model.fit()\n\n# Model summary\nprint(results.summary())\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:          buoy_sst_anom   R-squared (uncentered):                   0.282\nModel:                            OLS   Adj. R-squared (uncentered):              0.279\nMethod:                 Least Squares   F-statistic:                              98.58\nDate:                Tue, 07 Dec 2021   Prob (F-statistic):                    8.24e-20\nTime:                        18:09:44   Log-Likelihood:                         -312.83\nNo. Observations:                 252   AIC:                                      627.7\nDf Residuals:                     251   BIC:                                      631.2\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nenso_anom      0.5964      0.060      9.929      0.000       0.478       0.715\n==============================================================================\nOmnibus:                       11.182   Durbin-Watson:                   0.808\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               12.718\nSkew:                           0.402   Prob(JB):                      0.00173\nKurtosis:                       3.751   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nBuoy wave height anomaly vs. ENSO sea surface temperature anomaly\n# Simple linear model: buoy wave anomaly vs enso sst anomaly\nmodel = sm.OLS(anom_df.buoy_wave_anom, anom_df.enso_anom)\n\n# Model results\nresults = model.fit()\n\n# Model summary\nprint(results.summary())\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:         buoy_wave_anom   R-squared (uncentered):                   0.000\nModel:                            OLS   Adj. R-squared (uncentered):             -0.004\nMethod:                 Least Squares   F-statistic:                           0.006389\nDate:                Tue, 07 Dec 2021   Prob (F-statistic):                       0.936\nTime:                        18:09:44   Log-Likelihood:                         -11.128\nNo. Observations:                 252   AIC:                                      24.26\nDf Residuals:                     251   BIC:                                      27.79\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nenso_anom      0.0015      0.018      0.080      0.936      -0.034       0.037\n==============================================================================\nOmnibus:                        1.416   Durbin-Watson:                   1.619\nProb(Omnibus):                  0.493   Jarque-Bera (JB):                1.162\nSkew:                           0.011   Prob(JB):                        0.559\nKurtosis:                       3.332   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nBuoy wave height anomaly vs. buoy sea surface temperature anomaly\n# Simple linear model: buoy wave anomaly vs buoy sst anomaly\nmodel = sm.OLS(anom_df.buoy_wave_anom, anom_df.buoy_sst_anom)\n\n# Model results\nresults = model.fit()\n\n# Model summary\nprint(results.summary())\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:         buoy_wave_anom   R-squared (uncentered):                   0.035\nModel:                            OLS   Adj. R-squared (uncentered):              0.031\nMethod:                 Least Squares   F-statistic:                              9.167\nDate:                Tue, 07 Dec 2021   Prob (F-statistic):                     0.00272\nTime:                        18:09:44   Log-Likelihood:                         -6.6118\nNo. Observations:                 252   AIC:                                      15.22\nDf Residuals:                     251   BIC:                                      18.75\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nbuoy_sst_anom    -0.0480      0.016     -3.028      0.003      -0.079      -0.017\n==============================================================================\nOmnibus:                        2.296   Durbin-Watson:                   1.645\nProb(Omnibus):                  0.317   Jarque-Bera (JB):                2.133\nSkew:                           0.093   Prob(JB):                        0.344\nKurtosis:                       3.410   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nVisualize model results\nm, b = np.polyfit(anom_df.enso_anom, anom_df.buoy_sst_anom, 1)\nplt.figure(figsize = (10, 5))\nplt.scatter(anom_df.enso_anom, anom_df.buoy_sst_anom)\nplt.plot(anom_df.enso_anom, m * anom_df.enso_anom + b, color = 'red')\nplt.title('Buoy SST Anomaly ~ ENSO SST Anomaly')\nplt.ylabel('Buoy SST Anomaly')\nplt.xlabel('ENSO SST Anomaly')\nText(0.5, 0, 'ENSO SST Anomaly')\npngm, b = np.polyfit(anom_df.enso_anom, anom_df.buoy_wave_anom, 1)\nplt.figure(figsize = (10, 5))\nplt.scatter(anom_df.enso_anom, anom_df.buoy_wave_anom)\nplt.plot(anom_df.enso_anom, m * anom_df.enso_anom + b, color = 'red')\nplt.title('Buoy Wave Anomaly ~ ENSO SST Anomaly')\nplt.ylabel('Buoy Wave Anomaly')\nplt.xlabel('ENSO SST Anomaly')\nText(0.5, 0, 'ENSO SST Anomaly')\npngm, b = np.polyfit(anom_df.buoy_sst_anom, anom_df.buoy_wave_anom, 1)\nplt.figure(figsize = (10, 5))\nplt.scatter(anom_df.buoy_sst_anom, anom_df.buoy_wave_anom)\nplt.plot(anom_df.buoy_sst_anom, m * anom_df.buoy_sst_anom + b, color = 'red')\nplt.title('Buoy Wave Anomaly ~ Buoy SST Anomaly')\nplt.ylabel('Buoy Wave Anomaly')\nplt.xlabel('Buoy SST Anomaly')\nText(0.5, 0, 'Buoy SST Anomaly')\npngResults\nOur model shows a positive correlation between ENSO sea surface temperature anomaly and Santa Barbara sea surface temperature anomaly. The model output indicates that this correlation is statistically significant based on the p-value, but the low \\(R^2\\) value of 0.282 estimates that temperature near the equator only accounts for approximately 28% of the variability in sea surface temperature near Santa Barbara.Buoy height wave anomaly only has a slight positive correlation with ENSO sea surface temperature anomaly, but these results were not statistically significant due to a p-value > 0.05. The model for buoy wave height vs. buoy sea surface temperature was statistically significant (p-value < 0.05, but based on the \\(R^2\\) value, buoy sea surface temperature anomaly accounts for only 3.5% of the variability observed in wave height..\nFuture Work\nFurther analyses with these datasets could look at correlations on a larger spatial scale by comparing results for other buoy stations closer to and further from the equator than Santa Barbara. The CDIP datasets can also be used to model the effect of ENSO on wave direction.\n\nReferences\nReferences used to create this tutorial include:\nCoastal Data Information Program (CDIP): https://cdip.ucsd.edu/\nCoastal Data Information Program (CDIP) data access documentation: https://cdip.ucsd.edu/m/documents/data_access.html\nHarvest bouy, station 071: https://cdip.ucsd.edu/m/products/?stn=071p1\nBuoy Data citation: Data furnished by the Coastal Data Information Program (CDIP), Integrative Oceanography Division, operated by the Scripps Institution of Oceanography, under the sponsorship of the U.S. Army Corps of Engineers and the California Department of Parks and Recreation\nENSO Data citation: Data made publicly available by the National Weather Service’s Climate Prediction Center. The data used: Monthly ERSSTv5 (1991-2020 base period) Niño 3.4 (5°North-5°South)(170-120°West)\n\n\n\n",
    "preview": "posts/2021-12-04-modeling-enso-and-wave-height/output_51_0.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 873,
    "preview_height": 303
  },
  {
    "path": "posts/2021-12-04-precipitation-and-ndvi/",
    "title": "Remote Sensing with Google Earth Engine",
    "description": "Using Google Earth Engine to work with satellite imagery, We find the total precipitation for a coastal region in Northern Califnia as well as calculate the normalized difference vegetation index (NDVI), a simple graphical indicator that can be used to analyze remote sensing measurements. We use ERA5 Daily Aggregates data for precipitation and Landsat 8 data for NDVI.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "Python",
      "Spatial Analysis",
      "Google Earth Engine",
      "Remote Sensing"
    ],
    "contents": "\nPurpose\nThis analysis was done in order to get familiar with the Google Earth Engine Python API. To run this code, an account with Google Earth Engine is required and student licences are available for free.\n1. Choose A Study Region\nFor this project I am choosing Arcata California. This is where I attended college, and I chose to go there for it’s rural location and its proximity to a variety of outdoor recreation areas. Arcata is a situated along the northern coast of California. The coastal zone surrounding the town includes beautiful coast redwoods (Sequoia sempervirens), Humboldt Bay, and low-lying areas around Humboldt Bay. Going east from the coastal zone, you quickly gain elevation and enter the southern Cascade mountain range and the Trinity Alps. This region contains the volcanoes Mt. Shasta and Mt Lassen, formed by the Cascadia subduction zone. Arcata’s climate falls into the temperate rainforest category, and is dominated by a rainy season and a dry season. Arcata’s population is also somewhat seasonal thanks to the college population leaving for the summer months.\n2. Locate A Precipitation Dataset\nTake a look through the Google Earth Engine data catalog.\nI chose the the ERA5 Daily Aggregates dataset because my other choice Global Precipitation Measurement (GPM) v6 kept exceeding my user memory limit. I also investigated many other datasets (test_1 to test_5 files) but I did not really like how a few of them looked in comparison. I think the ERA5 Daily Aggregates suited my needs quite well and showed the scope of the storm event I was looking at quite well. It feels a bit like cheating, but I really did try to use other datasets. This dataset also gives reasonable spatial resolution.\n# Import packages\nimport ee\nimport geemap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import Image\nee.Initialize()\ngdat = ee.ImageCollection('ECMWF/ERA5/DAILY')\ngdat_prop = gdat.propertyNames().getInfo()\npr = gdat.select('total_precipitation')\nprdflt = pr.filter(ee.Filter.date('2019-02-24', '2019-02-28')).sum();\nprdflt.getInfo()\n{'type': 'Image',\n 'bands': [{'id': 'total_precipitation',\n   'data_type': {'type': 'PixelType', 'precision': 'double'},\n   'crs': 'EPSG:4326',\n   'crs_transform': [1, 0, 0, 0, 1, 0]}]}\n3. Plot a Time Series of Precipitation\nLet’s do a new type of analysis on the data: the creation of a time series.\nar_lon = -124\nar_lat = 40.8\nar_poi = ee.Geometry.Point(ar_lon, ar_lat)\nscale = 1000   # scale in m\nar_pr_ts = pr.getRegion(ar_poi, scale).getInfo()\ndf = pd.DataFrame(ar_pr_ts)\nprint(df)\n              0          1          2              3                    4\n0            id  longitude   latitude           time  total_precipitation\n1      19790102 -123.99895  40.796989   284083200000                    0\n2      19790103 -123.99895  40.796989   284169600000                    0\n3      19790104 -123.99895  40.796989   284256000000             0.002583\n4      19790105 -123.99895  40.796989   284342400000                    0\n...         ...        ...        ...            ...                  ...\n15161  20200705 -123.99895  40.796989  1593907200000             0.000057\n15162  20200706 -123.99895  40.796989  1593993600000             0.000298\n15163  20200707 -123.99895  40.796989  1594080000000             0.000005\n15164  20200708 -123.99895  40.796989  1594166400000              0.00015\n15165  20200709 -123.99895  40.796989  1594252800000             0.000015\n\n[15166 rows x 5 columns]\nheaders = df.loc[0]     # Assign the first entry in the data frame to a variable called \"headers\"\nprint(headers) \n0                     id\n1              longitude\n2               latitude\n3                   time\n4    total_precipitation\nName: 0, dtype: object\ndf = pd.DataFrame(df.values[1:], columns=headers)      \n# Make a new data frame out of the old one, but assigning the names we just retrieved as actual column headers\nprint(df)  \n0            id  longitude   latitude           time total_precipitation\n0      19790102 -123.99895  40.796989   284083200000                   0\n1      19790103 -123.99895  40.796989   284169600000                   0\n2      19790104 -123.99895  40.796989   284256000000            0.002583\n3      19790105 -123.99895  40.796989   284342400000                   0\n4      19790106 -123.99895  40.796989   284428800000                   0\n...         ...        ...        ...            ...                 ...\n15160  20200705 -123.99895  40.796989  1593907200000            0.000057\n15161  20200706 -123.99895  40.796989  1593993600000            0.000298\n15162  20200707 -123.99895  40.796989  1594080000000            0.000005\n15163  20200708 -123.99895  40.796989  1594166400000             0.00015\n15164  20200709 -123.99895  40.796989  1594252800000            0.000015\n\n[15165 rows x 5 columns]\ndf['datetime'] = pd.to_datetime(df['time'], unit = 'ms')\ndf\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nid\n\n\nlongitude\n\n\nlatitude\n\n\ntime\n\n\ntotal_precipitation\n\n\ndatetime\n\n\n0\n\n\n19790102\n\n\n-123.99895\n\n\n40.796989\n\n\n284083200000\n\n\n0\n\n\n1979-01-02\n\n\n1\n\n\n19790103\n\n\n-123.99895\n\n\n40.796989\n\n\n284169600000\n\n\n0\n\n\n1979-01-03\n\n\n2\n\n\n19790104\n\n\n-123.99895\n\n\n40.796989\n\n\n284256000000\n\n\n0.002583\n\n\n1979-01-04\n\n\n3\n\n\n19790105\n\n\n-123.99895\n\n\n40.796989\n\n\n284342400000\n\n\n0\n\n\n1979-01-05\n\n\n4\n\n\n19790106\n\n\n-123.99895\n\n\n40.796989\n\n\n284428800000\n\n\n0\n\n\n1979-01-06\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n15160\n\n\n20200705\n\n\n-123.99895\n\n\n40.796989\n\n\n1593907200000\n\n\n0.000057\n\n\n2020-07-05\n\n\n15161\n\n\n20200706\n\n\n-123.99895\n\n\n40.796989\n\n\n1593993600000\n\n\n0.000298\n\n\n2020-07-06\n\n\n15162\n\n\n20200707\n\n\n-123.99895\n\n\n40.796989\n\n\n1594080000000\n\n\n0.000005\n\n\n2020-07-07\n\n\n15163\n\n\n20200708\n\n\n-123.99895\n\n\n40.796989\n\n\n1594166400000\n\n\n0.00015\n\n\n2020-07-08\n\n\n15164\n\n\n20200709\n\n\n-123.99895\n\n\n40.796989\n\n\n1594252800000\n\n\n0.000015\n\n\n2020-07-09\n\n\n15165 rows × 6 columns\n\n\nplt.figure(figsize = (10, 6), dpi = 300)    # create a new figure, set size and resolution (dpi)\nplt.plot(df['datetime'],df['total_precipitation'])   # add data to the plot\nplt.title('Arcata Rainfall', fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Precip (m)', fontsize=14)\nplt.ylim(0, 0.12)\n(0.0, 0.12)\n\n4. Make Some Maps of Interesting Times\nI chose to take the sum of the time period I examined in order to look at how much precipitation there was over the Mad River watershed. The sum of the values also lets you see the total amount of precipitation over the entire time period of the storm event. My map shows heavy precipitation over the Pacific, making landfall in northern California and Oregon, while staying to the north and largely avoiding Nevada.\n# Base map\nMap = geemap.Map(center=[40.8,-124], zoom=6)\n# Min and Max\nVIS_PREC = {\n    'min':0,\n    'max':.02,\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']\n}\n# Add layer\nMap.addLayer(prdflt, VIS_PREC,'total precipitation',opacity=0.3)\n# Plot map\nMap\nMap(center=[40.8, -124], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=…\npt = ee.Geometry.Point([-124.3652, 40.2922])\nroi = pt.buffer(1600000)\n# Create a URL to the styled image for a region around Mendocino County CA.\nurl = prdflt.getThumbUrl({\n    'min': 0, 'max': .02, 'dimensions': 512, 'region': roi, 'opacity': 0.3,\n    'palette': ['#FFFFFF', '#00FFFF', '#0080FF', '#DA00FF', '#FFA400', '#FF0000']})\nImage(url=url, embed=True, format = 'png')\n\n5. Plot Landsat NDVI for the Region\nThe precipitation analysis discussed above provides useful context for identifying interesting weather events for a particular region. Now let’s see what (if any!) effect changes in weather patterns have had on the landscape, using Landsat imagery.\nI was able to see a slight difference in the two images. It is unclear to me if the long term average includes more cloud cover and is therefore less bright/vivid, or if the rain increased the vegetation. If I had to guess I would say that there was not a very noticeable change because I chose an area that is already very wet and productive.\nland_8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA')\npt = ee.Geometry.Point([-124, 40.8])   # Point corresponding to Arcata, CA\nland_8_pt = land_8.filterBounds(pt)\nland_8_least_cld = land_8_pt.filter('CLOUD_COVER < 20').mean();\nred = land_8_least_cld.select('B4')\nnir = land_8_least_cld.select('B5')\nndvi=(nir.subtract(red)).divide((nir.add(red))).rename('NDVI')\nndvi\n<ee.image.Image at 0x22d22b1b280>\nndviParams = {'min': -1, \n              'max': 1, \n              'palette': ['blue', 'white', 'green']\n             }\n#  Interactive only, no saved output\nMap_NDVI = geemap.Map(center=[40.8, -124], zoom=8)\nMap_NDVI.addLayer(ndvi, ndviParams,'NDVI')\nMap_NDVI\nMap(center=[40.8, -124], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=…\npt = ee.Geometry.Point([-124.3652, 40.2922])\nroi = pt.buffer(120000)\n# Create a URL to the styled image for a region around Mendocino County CA.\nurl = ndvi.getThumbUrl({\n    'min': -1, 'max': 1, 'dimensions': 512, 'region': roi,\n    'palette': ['blue', 'white', 'green']})\nImage(url=url, embed=True, format = 'png')\n\nland_8_least_cld_flt = land_8_pt.filter('CLOUD_COVER < 20')\nland_8_least_cld_flt_dt = land_8_least_cld_flt.filter(ee.Filter.date('2019-01-01', '2019-04-28')).mean();\nred_2 = land_8_least_cld_flt_dt.select('B4')\nnir_2 = land_8_least_cld_flt_dt.select('B5')\nndvi_2 = (nir_2.subtract(red_2)).divide((nir_2.add(red_2))).rename('NDVI')\nndvi_2\n<ee.image.Image at 0x22d220f6820>\n#  Interactive only, no saved output\nMap_NDVI_flt = geemap.Map(center=[40.8, -124], zoom=8)\nMap_NDVI_flt.addLayer(ndvi_2, ndviParams,'NDVI')\nMap_NDVI_flt\nMap(center=[40.8, -124], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=…\npt = ee.Geometry.Point([-124.3652, 40.2922])\nroi = pt.buffer(120000)\n# Create a URL to the styled image for a region around Mendocino County CA.\nurl = ndvi_2.getThumbUrl({\n    'min': -1, 'max': 1, 'dimensions': 512, 'region': roi,\n    'palette': ['blue', 'white', 'green']})\nImage(url=url, embed=True, format = 'png')\n\n\n\n\n",
    "preview": "posts/2021-12-04-precipitation-and-ndvi/output_42_0.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 512,
    "preview_height": 394
  },
  {
    "path": "posts/2021-12-03-houston-blackout-analysis-2021/",
    "title": "Houston Blackout Analysis (2021)",
    "description": "This analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite.The task is to answer the questions: How many residential buildings were without power on 2021-02-16? Is there a socioeconomic metric that predicts being affected by the power outage?",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "R",
      "Remote Sensing",
      "Spatial Analysis"
    ],
    "contents": "\nLoad libraries\n\n\nhide\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(stars)\nlibrary(rosm)\nlibrary(tmap)\n\n\n\nFunction to load the DNB dataset from VNP46A1 granules\n\n\nhide\n\nread_dnb <- function(file_name) {\n  # Reads the \"DNB_At_Sensor_Radiance_500m\" dataset from a VNP46A1 granule into a STARS object.\n  # Then read the sinolsoidal tile x/y positions and adjust the STARS dimensions (extent+delta)\n\n  # The name of the dataset holding the nightlight band in the granule\n  dataset_name <- \"//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m\"\n\n  # From the metadata, we pull out a string containing the horizontal and vertical tile index\n  h_string <- gdal_metadata(file_name)[199]\n  v_string <- gdal_metadata(file_name)[219]\n  \n  # We parse the h/v string to pull out the integer number of h and v\n  tile_h <- as.integer(str_split(h_string, \"=\", simplify = TRUE)[[2]])\n  tile_v <- as.integer(str_split(v_string, \"=\", simplify = TRUE)[[2]])\n\n  # From the h/v tile grid position, we get the offset and the extent\n  west <- (10 * tile_h) - 180\n  north <- 90 - (10 * tile_v)\n  east <- west + 10\n  south <- north - 10\n\n  # A tile is 10 degrees and has 2400x2400 grid cells\n  delta <- 10 / 2400\n\n  # Reading the dataset\n  dnb <- read_stars(file_name, sub = dataset_name)\n\n  # Setting the CRS and applying offsets and deltas\n  st_crs(dnb) <- st_crs(4326)\n  st_dimensions(dnb)$x$delta <- delta\n  st_dimensions(dnb)$x$offset <- west\n  st_dimensions(dnb)$y$delta <- -delta\n  st_dimensions(dnb)$y$offset <- north\n  \n  return(dnb)\n}\n\n\n\nRead in day night band (DNB) data\n\n\nhide\n\nFeb_07_v5 <- read_dnb(file_name = \"data/VNP46A1.A2021038.h08v05.001.2021039064328.h5\")\n\n\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \n\nhide\n\nFeb_07_v6 <- read_dnb(file_name = \"data/VNP46A1.A2021038.h08v06.001.2021039064329.h5\")\n\n\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \n\nhide\n\nFeb_16_v5 <- read_dnb(file_name = \"data/VNP46A1.A2021047.h08v05.001.2021048091106.h5\")\n\n\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \n\nhide\n\nFeb_16_v6 <- read_dnb(file_name = \"data/VNP46A1.A2021047.h08v06.001.2021048091105.h5\")\n\n\n//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m, \n\nCombine adjacent tiles for each date\n\n\nhide\n\nFeb_07_v5_v6 <- st_mosaic(Feb_07_v5, Feb_07_v6)\nFeb_16_v5_v6 <- st_mosaic(Feb_16_v5, Feb_16_v6)\n\n\n\nCreate blackout Mask\n\n\nhide\n\n## Take the difference of light data from before the storm and after the storm to make a mask of values with a difference greater than 200 nW cm-2 sr-1\ndiff <- (Feb_07_v5_v6 - Feb_16_v5_v6) > 200\n\n## Convert values with a difference of less than 200 to NA\ndiff[diff == F] <- NA\n\n\n\nVectorize blackout mask\n\n\nhide\n\nblackout_mask <- st_as_sf(diff)\n\n## Fix invalid geometries \nblackout_mask_fixed <- st_make_valid(blackout_mask)\n\nrm(diff, blackout_mask)\ngc()\n\n\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells  3047944 162.8    4503170  240.5   4503170  240.5\nVcells 27482570 209.7  132454492 1010.6 156217946 1191.9\n\nCrop the vectorized blackout mask to the region of interest\n\n\nhide\n\n## Set region of interest\nhouston <- st_polygon(\n  list(\n    rbind(\n      c(-96.5, 29), \n      c(-96.5, 30.5), \n      c(-94.5, 30.5), \n      c(-94.5, 29), \n      c(-96.5, 29)\n    )\n  )\n) %>% \n  st_sfc(crs = 4326)\n\n## Crop night lights data\nintersects <- st_intersects(blackout_mask_fixed, houston, sparse = FALSE)\nblackout_cropped <- blackout_mask_fixed[intersects,]\n\n## Transform cropped blackout mask back to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\nblackout_cropped_NAD83 <- st_transform(blackout_cropped, 3083)\n\nrm(blackout_cropped)\ngc()\n\n\n           used  (Mb) gc trigger  (Mb)  max used   (Mb)\nNcells  3020703 161.4    4503170 240.5   4503170  240.5\nVcells 27554636 210.3  105963594 808.5 156217946 1191.9\n\nSanity check plot\n\n\nhide\n\nggplot() +\n  geom_sf(data = blackout_cropped_NAD83) +\n  theme_classic()\n\n\n\n\nRoads data\n\n\nhide\n\nquery <- \n  \"SELECT * \n   FROM gis_osm_roads_free_1 \n   WHERE fclass='motorway'\"\n\nhighways <- \n  st_read(\n    \"data/gis_osm_roads_free_1.gpkg\", \n    query = query) %>% \n  st_transform(crs = 3083) %>% \n  st_buffer(dist = 200) %>% \n  st_union()\n\n\nReading query `SELECT * \n   FROM gis_osm_roads_free_1 \n   WHERE fclass='motorway'' from data source `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\nhide\n\ncat(\"\\n\\n\\nAfter Transforming\\n\\n\")\n\n\n\n\n\nAfter Transforming\n\nhide\n\nhighways\n\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1837420 ymin: 7218299 xmax: 2040621 ymax: 7387049\nProjected CRS: NAD83 / Texas Centric Albers Equal Area\n\nBasic highways plot\n\n\nhide\n\nggplot() +\n  geom_sf(data = highways) +\n  theme_classic()\n\n\n\n\nBuildings data\n\n\nhide\n\nquery <- \n  \"SELECT * \n   FROM gis_osm_buildings_a_free_1\n   WHERE (type IS NULL AND name IS NULL)\n   OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nbuildings <- \n  st_read(\n    \"data/gis_osm_buildings_a_free_1.gpkg\", \n    query = query) %>% \n  st_transform(crs = 3083) \n\n\nReading query `SELECT * \n   FROM gis_osm_buildings_a_free_1\n   WHERE (type IS NULL AND name IS NULL)\n   OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')' from data source `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\nhide\n\ncat(\"\\n\\n\\nAfter Transforming\\n\\n\")\n\n\n\n\n\nAfter Transforming\n\nhide\n\nbuildings\n\n\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1838180 ymin: 7216470 xmax: 2027040 ymax: 7386914\nProjected CRS: NAD83 / Texas Centric Albers Equal Area\nFirst 10 features:\n     osm_id code   fclass name       type\n1  15289727 1500 building <NA>       <NA>\n2  15289869 1500 building <NA>       <NA>\n3  15299261 1500 building <NA> apartments\n4  15331425 1500 building <NA>       <NA>\n5  15349970 1500 building <NA>       <NA>\n6  20868178 1500 building <NA>       <NA>\n7  20871848 1500 building <NA>       <NA>\n8  20871948 1500 building <NA>       <NA>\n9  20876080 1500 building <NA>       <NA>\n10 20877241 1500 building <NA>       <NA>\n                             geom\n1  MULTIPOLYGON (((1948074 728...\n2  MULTIPOLYGON (((1927251 732...\n3  MULTIPOLYGON (((1984491 730...\n4  MULTIPOLYGON (((1932443 731...\n5  MULTIPOLYGON (((1925238 732...\n6  MULTIPOLYGON (((1922763 727...\n7  MULTIPOLYGON (((1922899 727...\n8  MULTIPOLYGON (((1922788 727...\n9  MULTIPOLYGON (((1922699 727...\n10 MULTIPOLYGON (((1922807 727...\n\nCensus data\n\n\nhide\n\n# st_layers(\"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\nacs_geoms <- \n  st_read(\n    \"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n    layer = \"ACS_2019_5YR_TRACT_48_TEXAS\"\n  )\n\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\nhide\n\nacs_income <- \n  st_read(\n    \"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n    layer = \"X19_INCOME\"\n  ) %>% \n  select(GEOID, B19013e1) %>% \n  rename(GEOID_Data = GEOID,\n         median_income = B19013e1)\n\n\nReading layer `X19_INCOME' from data source \n  `C:\\Users\\Cullen\\OneDrive\\Documents\\MEDS\\cullen-molitor.github.io\\_posts\\2021-12-03-houston-blackout-analysis-2021\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n\n\nhide\n\nacs_geoms_med <- left_join(acs_geoms, acs_income)  %>% \n  st_transform(crs = 3083) \n\n\n\nMerge datasets\n\n\nhide\n\nblackout_no_hwy <- st_difference(blackout_cropped_NAD83, highways)\nrm(highways)\ngc()\n\n\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells  7126649 380.7   20004878 1068.4  20004878 1068.4\nVcells 56415877 430.5  105963594  808.5 156217946 1191.9\n\n\n\nhide\n\nhouston_res_wo_power <- buildings[blackout_no_hwy, op = st_intersects]\nnumber_houses_wo_power <- length(houston_res_wo_power$osm_id)\n\n\n\nThe number of buildings that were left without power is 157411.\n\n\nhide\n\nacs_building <- st_join(houston_res_wo_power, acs_geoms_med, join = st_intersects)\n\n\n\n\n\nhide\n\nacs_polygon <- st_join(blackout_no_hwy, acs_geoms_med, join = st_intersects)\n\n\n\n\n\nhide\n\nhouston_bbox <- st_bbox(houston)\nhouston_map <- osm.raster(houston_bbox)\n\n\n\nArea and Median Incomes of Residences Affected by Houston Blackout in February, 2021\n\n\nhide\n\ntm_shape(houston_map) +\n  tm_rgb(alpha = .75) +\n  # tm_shape(acs_geoms_med,\n  #          border.alpha = 0) +\n  # tm_polygons(col = \"median_income\") +\n  tm_shape(blackout_no_hwy) +\n  tm_polygons(border.alpha = 1) +\n  tm_shape(acs_polygon) + \n  tm_fill(\"median_income\", \n          n = 5, \n          style = \"pretty\",\n          title = \"Median Income ($)\") +\n  tm_compass()+\n  tm_scale_bar()\n\n\n\n\nThis map was created by Amber McEldowney and Cullen Molitor 2021-10-24. Sources: Socioeconomic data: U.S. Census Bureau’s American Community Survey for Texas census tracts in 2019  Light data: NASA’s Level-1 and Atmosphere Archive & Distribution System Distributed Active Archive Center (LAADS DAAC)  Spatial & Buildings Data: OpenStreetMap\n\n\nhide\n\nrm(blackout_cropped_NAD83, acs_geoms, acs_income, buildings)\ngc()\n\n\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells  4929805 263.3   16003903  854.8  20004878 1068.4\nVcells 46664452 356.1  152763574 1165.5 156217946 1191.9\n\nHistogram of Median Income for Houses Affected by Blackout\n\n\nhide\n\nMedian_Income <- acs_polygon$median_income\nhist(Median_Income,\nmain=\"Median Income in Regions Affected by Blackout\",\nxlab=\"Median Income ($)\",\nylab=\"Number of Households\",\ncol=\"grey\",\nfreq=FALSE\n)\n\n\n\n\nHistogram of Median Income in Houston\n\n\nhide\n\nhouston_nad <- houston %>%\n  st_as_sf() %>%\n  st_transform(houston, crs = 3083)\n\n# acs_geoms_med_bb <- st_join(acs_geoms_med, houston_nad, join = st_intersects)\n\nintersects <- st_intersects(acs_geoms_med, houston_nad, sparse = FALSE)\nacs_geoms_med_bb <- acs_geoms_med[intersects,]\n\nMedian_Income_Houston <- acs_geoms_med_bb$median_income\nhist(Median_Income_Houston,\nmain=\"Median Income in Houston\",\nxlab=\"Median Income ($)\",\nylab=\"Number of Households\",\ncol=\"grey\",\nfreq=FALSE\n)\n\n\n\n\nWe thought it would be interesting to compare the median incomes of households affected by the blackout, to median incomes in Houston in general, but because the blackouts seem to have occured in a more metropolitan area, it is likely the incomes are skewed higher for that area, and it does not give a clear indication of whether median income had an effect on whether or not a household experienced a blackout.\n\n\n\n",
    "preview": "posts/2021-12-03-houston-blackout-analysis-2021/houston-blackout-analysis-2021_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-12-03-iowas-wind-power-potential/",
    "title": "Iowa's Wind Power Potential",
    "description": "This analysis is used to determine locations in Iowa that would be suitible for wind turbine power plant installation as well as to calculate the hypothetical potential wind energy production capacity of these areas. This type of analysis can be used as a baseline in determining the maximum cost and benefits for potential renewable energy projects.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "Python",
      "SQL",
      "Spatial Analysis"
    ],
    "contents": "\nImport Modules\nimport sqlalchemy as sa\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport math\n%matplotlib inline\nDatabase Connection\npg_uri_template = 'postgresql+psycopg2://{user}:{pwd}@{host}/{db_name}'\ndb_uri = pg_uri_template.format(\n    drivername='postgresql+psycopg2',\n    host = '{ip address}',\n    user = '{username}',\n    pwd = '{pwd}',\n    db_name = 'osmiowa'\n)\nengine = sa.create_engine(db_uri)\nData Details\nDatabase Name: - osmiowa\nDatabase Tables: - planet_osm_line - planet_osm_point - planet_osm_polygon - planet_osm_roads - spatial_ref_sys - wind - wind_cells\nGeometry: - Projected in EPSG:26975 - Geodetic CRS: NAD83 - Units: meters - Coordinate system: Cartesian 2D CS - Axes: easting, northing (X,Y) - Orientations: east, north\nQuery Database\nResidential Scenario 1 (3H)\nsql_buildings_residential_1 = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 450) as way\nFROM \n    planet_osm_polygon \nWHERE \n    building IN ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\nOR \n    landuse = 'residential'\nOR \n    place = 'town'\n\"\"\"\nResidential Scenario 2 (10H)\nsql_buildings_residential_2 = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 1500) as way \nFROM \n    planet_osm_polygon \nWHERE \n    building \nIN \n    ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\nOR \n    landuse = 'residential'\nOR\n    place = 'town'\n\"\"\"\nNon-residential (3H)\nsql_buildings_nonresidential = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 450) as way\nFROM \n    planet_osm_polygon \nWHERE \n    building \nNOT IN \n    ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\n\"\"\"\nAirports\nsql_airports = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 7500) as way\nFROM \n    planet_osm_polygon \nWHERE \n    aeroway IS NOT NULL\n\"\"\"\nMilitary Bases\nsql_military = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 0) as way \nFROM \n    planet_osm_polygon \nWHERE \n    military IS NOT NULL\nOR \n    landuse = 'military'\n\"\"\"\nRailways and Roads\nsql_railways_n_roads = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 300) as way\nFROM \n    planet_osm_line \nWHERE \n    (railway IS NOT NULL\n     AND railway \n     NOT IN ('abandoned', 'disused', 'razed', 'dismantled'))\nOR \n    (highway IS NOT NULL\nAND \n    highway \nIN \n    ('motorway', 'motorway_link', 'trunk', 'trunk_link', 'road',\n    'primary', 'primary_link', 'secondary', 'secondary_link'))\n\"\"\"\nNature Reserves, Parks, and Wetlands\nsql_nature = \\\n\"\"\"\nSELECT \n    way \nFROM \n    planet_osm_polygon \nWHERE \n    leisure \nIN \n    ('nature_reserve', 'park')\nOR\n    \"natural\" \nIN \n    ('wetland')\n\"\"\"\nRivers\nsql_rivers = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 150) as way \nFROM \n    planet_osm_line \nWHERE \n    waterway \nIN \n    ('river')\n\"\"\"\nLakes\nsql_lakes = \\\n\"\"\"\nSELECT\n    way\nFROM \n    planet_osm_polygon\nWHERE \n    water \nIN \n    ('lake', 'reservoir', 'pond')\n\"\"\"\nPower Lines\nsql_powerlines = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 300) as way\nFROM \n    planet_osm_line \nWHERE \n    power IS NOT NULL\n\"\"\"\nPower Plants\nsql_powerplants = \\\n\"\"\"\nSELECT \n    ST_BUFFER(way, 150) as way\nFROM \n    planet_osm_polygon \nWHERE \n    power IS NOT NULL\n\"\"\"\nWind Turbines\nsql_turbines = \\\n\"\"\"\nSELECT\n    ST_BUFFER(way, 680) as way\nFROM \n    planet_osm_point \nWHERE \n    \"generator:source\" IS NOT NULL\nAND \n    \"generator:source\" IN ('wind')\n\"\"\"\nMerge Subqueries\nScenario 1\nsql_siting_constraints = \\\nf\"\"\"\n{sql_buildings_residential_1} \nUNION\n{sql_buildings_nonresidential} \nUNION\n{sql_airports} \nUNION\n{sql_military} \nUNION\n{sql_railways_n_roads} \nUNION\n{sql_nature} \nUNION\n{sql_rivers} \nUNION\n{sql_lakes} \nUNION\n{sql_powerlines}\nUNION\n{sql_powerplants}\nUNION\n{sql_turbines}\n\"\"\"\nsiting_constraints = gpd.read_postgis(sql_siting_constraints, con = engine, geom_col = 'way')\nScenario 2\nsql_siting_constraints_2 = \\\nf\"\"\"\n{sql_buildings_residential_2} \nUNION\n{sql_buildings_nonresidential} \nUNION\n{sql_airports} \nUNION\n{sql_military} \nUNION\n{sql_railways_n_roads} \nUNION\n{sql_nature} \nUNION\n{sql_rivers} \nUNION\n{sql_lakes} \nUNION\n{sql_powerlines}\nUNION\n{sql_powerplants}\nUNION\n{sql_turbines}\n\"\"\"\nsiting_constraints_2 = gpd.read_postgis(sql_siting_constraints_2, con = engine, geom_col = 'way')\nfig, ax = plt.subplots(figsize=(8, 8))\nsiting_constraints.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\nax.grid(True, color = 'dimgray')\nax.set_title('Siting Constraints (Scenario 1)', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs=\"EPSG:26975\")\n\nfig, ax = plt.subplots(figsize=(8, 8))\nsiting_constraints_2.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\nax.grid(True, color = 'dimgray')\nax.set_title('Siting Constraints (Scenario 2)', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs=\"EPSG:26975\")\n\nWind Data\nThe table wind_cells_10000 contains 10 km2 square polygons with associated average annual wind speeds (m s-1), arranged to cover Iowa in a ragged grid.\nsql_wind_speeds = \\\n\"\"\"\nSELECT * \nFROM \n    wind_cells_10000 \nWHERE \n    wind_speed IS NOT NULL\n\"\"\"\nwind_speeds = gpd.read_postgis(sql_wind_speeds, con = engine, geom_col = 'geom')\nResults\nScenario #1\nSubtract Siting Constraints from Wind Cells\nsuitable_cells = wind_speeds.overlay(siting_constraints, how = 'difference', keep_geom_type = False)\nPlot Suitable Wind Cells\nfig, ax = plt.subplots(figsize=(8, 8))\nsuitable_cells.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\nax.grid(True, color = 'dimgray')\nax.set_title('Suitable Areas (Scenario 1)', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs=\"EPSG:26975\")\n\nFind Number of Turbines\nrotor_5x = math.pi * (136 * 5) ** 2\nsuitable_cells[\"area\"] = suitable_cells['geom'].area\nsuitable_cells[\"number_wind_turbines\"] = suitable_cells['geom'].area / rotor_5x\ntotal_wind_turbines = suitable_cells[\"number_wind_turbines\"].sum()\nprint(\"The total number of turbines in scenario 1 is:\", round(total_wind_turbines))\nThe total number of turbines in scenario 1 is: 57286\nPower production\nsuitable_cells['energy_per_turbine'] = 2.6 * suitable_cells['wind_speed'] - 5\nsuitable_cells['power_per_cell'] = suitable_cells['energy_per_turbine'] * suitable_cells['number_wind_turbines']\nTotal Power Production\ntotal_scenario_1 = suitable_cells['power_per_cell'].sum()\nprint(\"The total power produced in scenario 1 is:\", round(total_scenario_1), \"GWH/year\")\nThe total power produced in scenario 1 is: 1064159 GWH/year\nScenario #2\nSubtract Siting Constraints from Wind Cells\nsuitable_cells_2 = wind_speeds.overlay(siting_constraints_2, how = 'difference', keep_geom_type = False)\nPlot Suitable Wind Cells\nfig, ax = plt.subplots(figsize=(8, 8))\nsuitable_cells_2.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .5)\nax.grid(True, color = 'dimgray')\nax.set_title('Suitable Areas (Scenario 2)', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs=\"EPSG:26975\")\n\nFind Number of Turbines\nrotor_5x = math.pi * (136 * 5) ** 2\nsuitable_cells_2[\"area\"] = suitable_cells_2['geom'].area\nsuitable_cells_2[\"number_wind_turbines\"] = suitable_cells_2['geom'].area / rotor_5x\ntotal_wind_turbines_2 = suitable_cells_2[\"number_wind_turbines\"].sum()\nprint(\"The total number of turbines in scenario 2 is:\", round(total_wind_turbines_2))\nThe total number of turbines in scenario 2 is: 52055\nPower Production\nsuitable_cells_2['energy_per_turbine'] = 2.6 * suitable_cells_2['wind_speed'] - 5\nsuitable_cells_2['power_per_cell'] = suitable_cells_2['energy_per_turbine'] * suitable_cells_2['number_wind_turbines']\nwind_speeds\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nid\n\n\ngeom\n\n\nwind_speed\n\n\n0\n\n\n1\n\n\nPOLYGON ((1256222.769 1212179.582, 1266222.769…\n\n\n9.336039\n\n\n1\n\n\n2\n\n\nPOLYGON ((1266222.769 1212179.582, 1276222.769…\n\n\n9.097315\n\n\n2\n\n\n3\n\n\nPOLYGON ((1276222.769 1212179.582, 1286222.769…\n\n\n8.984566\n\n\n3\n\n\n4\n\n\nPOLYGON ((1286222.769 1212179.582, 1296222.769…\n\n\n9.266137\n\n\n4\n\n\n5\n\n\nPOLYGON ((1296222.769 1212179.582, 1306222.769…\n\n\n9.296747\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n1437\n\n\n1438\n\n\nPOLYGON ((1686222.769 902179.582, 1696222.769 …\n\n\n8.678687\n\n\n1438\n\n\n1439\n\n\nPOLYGON ((1426222.769 892179.582, 1436222.769 …\n\n\n9.114730\n\n\n1439\n\n\n1440\n\n\nPOLYGON ((1656222.769 892179.582, 1666222.769 …\n\n\n8.237385\n\n\n1440\n\n\n1441\n\n\nPOLYGON ((1666222.769 892179.582, 1676222.769 …\n\n\n8.423034\n\n\n1441\n\n\n1442\n\n\nPOLYGON ((1666222.769 882179.582, 1676222.769 …\n\n\n8.109468\n\n\n1442 rows × 3 columns\n\n\nTotal Power Production\ntotal_scenario_2 = suitable_cells_2['power_per_cell'].sum()\nprint(\"The total power produced in scenario 2 is:\", round(total_scenario_2), \" GWH/year\")\nThe total power produced in scenario 2 is: 967009  GWH/year\n\n\n\n",
    "preview": "posts/2021-12-03-iowas-wind-power-potential/output_58_0.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 496,
    "preview_height": 346
  },
  {
    "path": "posts/2021-12-03-protecting-whales-from-ships/",
    "title": "Protecting Whales From Ships",
    "description": "A vessel speed reduction zone is a potential solution to reducing ship strikes with whales near Dominica. The goal of this   analysis is to look at sperm whale sighting data, and AIS vessel data, to determine a vessel speed reduction zone, and the effects implementing a vessel speed reduction zone would have on shipping activities in the zone.",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [
      "Python",
      "Spatial Analysis"
    ],
    "contents": "\nSetup\nImport Modules\nimport geopandas as gpd \nimport rasterio as rio\nfrom rasterio.plot import show\nimport numpy as np\nimport pandas as pd\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport contextily as ctx\nimport shapely\nfrom shapely.geometry import Polygon\n%matplotlib inline\nSet Parameters and constants\nshoreline_shapefile  = 'data/dominica/dma_admn_adm0_py_s1_dominode_v2.shp'\nwhales_CSV           = 'data/sightings2005_2018.csv'\nvessels_CSV          = 'data/station1249.csv'\nten_kn_in_ms = 5.14\nprojected_EPSG = 2002   # Dominica 1945 / British West Indies Grid\ngeodetic_EPSG  = 4326   # WGS 84 (use as default CRS for incoming latlon)\nLoad Data\nDominica Shape File\ndominica_shp = gpd.read_file(shoreline_shapefile)\ndominica_shp.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nADM0_PCODE\n\n\nADM0_EN\n\n\ngeometry\n\n\n0\n\n\nDM\n\n\nDominica\n\n\nPOLYGON ((-61.43023 15.63952, -61.43019 15.639…\n\n\nSet CRS to EPSG 2002\ndominica_shp = dominica_shp.to_crs(projected_EPSG)\ndominica_shp.crs\n<Projected CRS: EPSG:2002>\nName: Dominica 1945 / British West Indies Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Dominica - onshore.\n- bounds: (-61.55, 15.14, -61.2, 15.69)\nCoordinate Operation:\n- name: British West Indies Grid\n- method: Transverse Mercator\nDatum: Dominica 1945\n- Ellipsoid: Clarke 1880 (RGS)\n- Prime Meridian: Greenwich\nPlot Dominica\nfig, ax = plt.subplots(figsize=(5, 5))\nax.grid(True, color = 'dimgray')\nax.set(ylim=(1.67e6,1.74e6), xlim=(445000, 490000))\nax.ticklabel_format(scilimits =  [-5, 5])\ndominica_shp.plot(ax = ax, edgecolor = \"k\", facecolor=\"None\")\nctx.add_basemap(ax, crs=projected_EPSG)\n\nLoad Data\nWhale Sightings\nwhales = gpd.read_file(whales_CSV)\nwhales\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nfield_1\n\n\nGPStime\n\n\nLat\n\n\nLong\n\n\ngeometry\n\n\n0\n\n\n0\n\n\n2005-01-15 07:43:27\n\n\n15.36977117\n\n\n-61.49328433\n\n\nNone\n\n\n1\n\n\n1\n\n\n2005-01-15 08:07:13\n\n\n15.3834075\n\n\n-61.503702\n\n\nNone\n\n\n2\n\n\n2\n\n\n2005-01-15 08:31:17\n\n\n15.38106333\n\n\n-61.50486067\n\n\nNone\n\n\n3\n\n\n3\n\n\n2005-01-15 09:19:10\n\n\n15.33532083\n\n\n-61.46858117\n\n\nNone\n\n\n4\n\n\n4\n\n\n2005-01-15 10:08:00\n\n\n15.294224\n\n\n-61.45318517\n\n\nNone\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n4888\n\n\n4888\n\n\n2018-05-25 12:01:25\n\n\n15.39195\n\n\n-61.572\n\n\nNone\n\n\n4889\n\n\n4889\n\n\n2018-05-25 13:08:29\n\n\n15.4189\n\n\n-61.5833\n\n\nNone\n\n\n4890\n\n\n4890\n\n\n2018-05-25 14:50:00\n\n\n15.443483\n\n\n-61.60995\n\n\nNone\n\n\n4891\n\n\n4891\n\n\n2018-05-25 15:57:34\n\n\n15.499866\n\n\n-61.638333\n\n\nNone\n\n\n4892\n\n\n4892\n\n\n2018-05-25 16:17:10\n\n\n15.494783\n\n\n-61.6482\n\n\nNone\n\n\n4893 rows × 5 columns\n\n\nCreate GeoDataFrame\npoints = gpd.points_from_xy(whales['Long'], whales['Lat'], crs = geodetic_EPSG)\nwhales_gdf = gpd.GeoDataFrame(whales, geometry = points)\nwhales_gdf = whales_gdf.to_crs(projected_EPSG)\nPlot Whale Sightings\nfig, ax = plt.subplots(figsize = (10, 10))\nax.set_aspect('equal')\nax.grid(True, color = 'dimgray')\nax.set(xlim=(4e5, 5e5), ylim=(1.5e6, 1.8e6))\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\nwhales_gdf.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .25)\nax.set_title('Whale Sightings', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs = projected_EPSG)\n\nCreate Grid\nxmin, ymin, xmax, ymax = whales_gdf.total_bounds\ncell_size = 2000\nxs = list(np.arange(xmin, xmax + cell_size, cell_size))\nys = list(np.arange(ymin, ymax + cell_size, cell_size))\ndef make_cell(x, y, cell_size):\n    ring = [\n        (x, y),\n        (x + cell_size, y),\n        (x + cell_size, y + cell_size),\n        (x, y + cell_size)\n    ]\n    cell = shapely.geometry.Polygon(ring)\n    return cell\ncells = []\nfor x in xs:\n    for y in ys:\n        cell = make_cell(x, y, cell_size)\n        cells.append(cell)\ngrid = gpd.GeoDataFrame({'geometry': cells}, crs = geodetic_EPSG)\ngrid = grid.set_geometry(crs = projected_EPSG, col = grid['geometry'])\nPlot Grid\ngrid.plot(facecolor = \"none\", figsize = (10, 10), linewidth = 1)\n<AxesSubplot:>\n\nExtract Whale Habitat\nwhale_grid = grid.sjoin(whales_gdf, how=\"inner\")\nPlot whale sighting grid cells\nwhale_grid.plot(facecolor = \"none\", figsize = (10, 10), linewidth = .75)\n<AxesSubplot:>\n\nSummarise Whale Counts\ngrid['count'] = whale_grid.groupby(whale_grid.index).count()['index_right']\ngrid = grid[grid['count'] > 20]\nPlot Whale Habitat\ngrid.plot(facecolor = \"none\", figsize = (10, 10), linewidth = .75)\n<AxesSubplot:>\n\nCreate Unary Union\ngrid_union = grid.unary_union\ngrid_union\n\nCreate Vessel Speed Reduction Zone\nThe vessel speed reduction zone is determined by the whale habitat areas in which more than 20 whales were observed per 2,000 m².\ngrid_convex = grid_union.convex_hull\ngrid_convex\n\nMake GeoDataFrame\nwhale_habitat = gpd.GeoDataFrame(index = [0], geometry = [grid_convex], crs = projected_EPSG)\nPlot\nfig, ax = plt.subplots(figsize = (10, 10))\nax.grid(True, color = 'dimgray')\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\nwhale_habitat.plot(ax = ax, edgecolor = \"purple\", facecolor = 'none')\nax.set_title('Whale Habitat', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs = projected_EPSG)\n\nwhale_habitat_diff = whale_habitat.difference(dominica_shp)\nwhale_habitat = gpd.GeoDataFrame(geometry = whale_habitat_diff)\nwhale_habitat\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\ngeometry\n\n\n0\n\n\nPOLYGON ((456480.652 1682792.746, 454480.652 1…\n\n\nfig, ax = plt.subplots(figsize = (10, 10))\nax.grid(True, color = 'dimgray')\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\nwhale_habitat.plot(ax = ax, edgecolor = \"purple\", facecolor = 'none')\nax.set_title('Whale Habitat', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs = projected_EPSG)\n\nwhale_habitat\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\ngeometry\n\n\n0\n\n\nPOLYGON ((456480.652 1682792.746, 454480.652 1…\n\n\nLoad AIS Vessel Data\nvessels = gpd.read_file(vessels_CSV)\npoints = gpd.points_from_xy(vessels['LON'], vessels['LAT'], crs = geodetic_EPSG)\nvessels_gdf = gpd.GeoDataFrame(vessels, geometry = points)\nvessels_gdf = vessels_gdf.to_crs(projected_EPSG)\nvessels_gdf['TIMESTAMP'] = pd.to_datetime(vessels_gdf['TIMESTAMP'])\nvessels_gdf\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\n0\n\n\n0\n\n\n233092000\n\n\n-61.84788\n\n\n15.23238\n\n\n2015-05-22 13:53:26\n\n\nPOINT (-61.84788 15.23238)\n\n\n1\n\n\n1\n\n\n255803280\n\n\n-61.74397\n\n\n15.96114\n\n\n2015-05-22 13:52:57\n\n\nPOINT (-61.74397 15.96114)\n\n\n2\n\n\n2\n\n\n329002300\n\n\n-61.38968\n\n\n15.29744\n\n\n2015-05-22 13:52:32\n\n\nPOINT (-61.38968 15.29744)\n\n\n3\n\n\n3\n\n\n257674000\n\n\n-61.54395\n\n\n16.2334\n\n\n2015-05-22 13:52:24\n\n\nPOINT (-61.54395 16.23340)\n\n\n4\n\n\n4\n\n\n636092006\n\n\n-61.52401\n\n\n15.81954\n\n\n2015-05-22 13:51:23\n\n\nPOINT (-61.52401 15.81954)\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n617257\n\n\n238722\n\n\n256525000\n\n\n-61.40679\n\n\n15.36907\n\n\n2015-05-21 21:34:59\n\n\nPOINT (-61.40679 15.36907)\n\n\n617258\n\n\n238723\n\n\n311077100\n\n\n-61.37539\n\n\n15.27406\n\n\n2015-05-21 21:34:55\n\n\nPOINT (-61.37539 15.27406)\n\n\n617259\n\n\n238724\n\n\n377907247\n\n\n-61.39461\n\n\n15.30672\n\n\n2015-05-21 21:34:46\n\n\nPOINT (-61.39461 15.30672)\n\n\n617260\n\n\n238725\n\n\n253365000\n\n\n-61.49001\n\n\n16.14007\n\n\n2015-05-21 21:34:46\n\n\nPOINT (-61.49001 16.14007)\n\n\n617261\n\n\n238726\n\n\n329002300\n\n\n-61.48073\n\n\n15.44751\n\n\n2015-05-21 21:34:45\n\n\nPOINT (-61.48073 15.44751)\n\n\n617262 rows × 6 columns\n\n\nSelect Vessels Inside Whale Habitat\nvessels_whale = vessels_gdf.sjoin(whale_habitat, how = 'inner')\nvessels_whale\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\n2\n\n\n2\n\n\n329002300\n\n\n-61.38968\n\n\n15.29744\n\n\n2015-05-22 13:52:32\n\n\nPOINT (464555.392 1690588.725)\n\n\n0\n\n\n7\n\n\n7\n\n\n338143127\n\n\n-61.39575\n\n\n15.33418\n\n\n2015-05-22 13:50:54\n\n\nPOINT (463892.452 1694650.397)\n\n\n0\n\n\n13\n\n\n13\n\n\n329002300\n\n\n-61.38968\n\n\n15.29745\n\n\n2015-05-22 13:48:32\n\n\nPOINT (464555.389 1690589.831)\n\n\n0\n\n\n15\n\n\n15\n\n\n338143015\n\n\n-61.39558\n\n\n15.33423\n\n\n2015-05-22 13:47:31\n\n\nPOINT (463910.683 1694655.978)\n\n\n0\n\n\n16\n\n\n16\n\n\n338143127\n\n\n-61.39757\n\n\n15.33139\n\n\n2015-05-22 13:47:25\n\n\nPOINT (463697.964 1694341.275)\n\n\n0\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n617252\n\n\n238717\n\n\n329002300\n\n\n-61.4885\n\n\n15.4706\n\n\n2015-05-21 21:37:45\n\n\nPOINT (453901.647 1709712.916)\n\n\n0\n\n\n617253\n\n\n238718\n\n\n338143015\n\n\n-61.39553\n\n\n15.33448\n\n\n2015-05-21 21:37:14\n\n\nPOINT (463915.972 1694683.643)\n\n\n0\n\n\n617255\n\n\n238720\n\n\n338143127\n\n\n-61.39563\n\n\n15.33468\n\n\n2015-05-21 21:35:12\n\n\nPOINT (463905.177 1694705.734)\n\n\n0\n\n\n617259\n\n\n238724\n\n\n377907247\n\n\n-61.39461\n\n\n15.30672\n\n\n2015-05-21 21:34:46\n\n\nPOINT (464023.288 1691613.624)\n\n\n0\n\n\n617261\n\n\n238726\n\n\n329002300\n\n\n-61.48073\n\n\n15.44751\n\n\n2015-05-21 21:34:45\n\n\nPOINT (454741.236 1707161.130)\n\n\n0\n\n\n167402 rows × 7 columns\n\n\nPlot Vessel Data\nfig, ax = plt.subplots(figsize = (10, 10))\nax.grid(True, color = 'dimgray')\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\nvessels_whale.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .1)\nax.set_title('Vessel Data Inside Whale Habitat', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs = projected_EPSG)\n\nCalculate Vessel Speeds\nvessels_whale_sorted = vessels_whale.sort_values(['MMSI', 'TIMESTAMP'])\nvessels_whale_shifted = vessels_whale_sorted.shift(periods = 1)\nvw_sorted_shifted = vessels_whale_sorted.join(vessels_whale_shifted, lsuffix = \"_original\", rsuffix = \"_copy\")\nvw_matched = vw_sorted_shifted[vw_sorted_shifted['MMSI_original'] == vw_sorted_shifted['MMSI_copy']]\nvw_matched = vw_matched.set_geometry(col = vw_matched['geometry_original'], crs = projected_EPSG)\nvw_matched['distance_m'] = vw_matched['geometry_original'].distance(vw_matched['geometry_copy']) \nvw_matched['time_s'] = vw_matched['TIMESTAMP_original'] - vw_matched['TIMESTAMP_copy']\nvw_matched['time_s'] = vw_matched['time_s'].dt.total_seconds() \nvw_matched['speed_ms'] = vw_matched['distance_m'] / vw_matched['time_s']\nvw_matched['10_kn_time'] = vw_matched['distance_m'] / ten_kn_in_ms\nvw_matched['time_diff'] = vw_matched['10_kn_time'].sub(vw_matched['time_s'])\nvw_matched = vw_matched[vw_matched['time_diff'] > 0]\nPlot Vessels travelling over 10 knots\nfig, ax = plt.subplots(figsize = (10, 10))\nax.grid(True, color = 'dimgray')\nax.set(xlim=(4.4e5, 4.85e5), ylim=(1.675e6, 1.73e6))\ndominica_shp.plot(ax = ax, color='None', edgecolor = \"k\")\nvw_matched.plot(ax = ax, markersize = .1, color = \"purple\", alpha = .25)\nax.set_title('Vessel Data Inside Whale Habitat', fontsize=12)\nax.ticklabel_format(scilimits =  [-5, 5])\nctx.add_basemap(ax, crs = projected_EPSG)\n\nImpact of a 10 knot Speed Reduction Zone\nWe calculated that ship vessel traffic would be slowed by 2 minutes per mile on average. Over the course of the year, this equated to approximately 28 days of increased shipping time.\nnum_days = vw_matched['time_diff'].sum() / 60 / 60 /24\nnum_days\n27.9423247732646\nmean_time_lost = vw_matched['time_diff'].mean() / 60\nmean_time_lost\n1.8906563139508044\n\n\n\n",
    "preview": "posts/2021-12-03-protecting-whales-from-ships/output_74_0.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 499,
    "preview_height": 604
  },
  {
    "path": "posts/2021-11-29-cartography-in-r/",
    "title": "Cartography in R",
    "description": "Objectives:   \n1. Learn the (very) basics of map design \n2. Learn how to load geospatial data  \n3. Learn how to inspect geospatial data  \n4. Plot geospatial data",
    "author": [
      {
        "name": "Cullen Molitor",
        "url": {}
      }
    ],
    "date": "2021-11-29",
    "categories": [
      "R",
      "Ocean Data",
      "Remote Sensing",
      "Spatial Analysis"
    ],
    "contents": "\nPurpose\nThis notebook demonstrates how to work with geospatial data using the R software environment to make maps. I chose to plot the bathymetry around New Zealand. This project uses the R package marmap to querry NOAA databases to get the digital elevation model data. I then use a shapefile of the New Zealand coastlines to get better delineation between ocean and land.\nLoad libraries\n\n\nhide\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(here)\nlibrary(marmap)\nlibrary(ggspatial)\n\n\n\nLoad Data\nUse marmap::getNOAA.bathy() to load New Zealand bathymetry and elevation data directly from NOAA database and convert bathy object to dataframe for ggplot.\n\n\nhide\n\nnz_df <- marmap::getNOAA.bathy(\n  lon1 = 162,\n  lon2 = 180,\n  lat1 = -33,\n  lat2 = -50,\n  resolution = 1\n) %>% \n  marmap::as.raster() %>% \n  raster::rasterToPoints() %>% \n  base::as.data.frame()%>%\n              filter(layer <= 0)\n\n\n\nRead in New Zealand coastlines with sf::read_sf() to read the shapefile containing the New Zealand coastline.\n\n\nhide\n\nnz_coast <- sf::read_sf(\n    \"data/nz-coastlines-and-islands-polygons-topo-150k.shp\"\n  )\n\n\n\nPlot Data\nUse ggplot() to plot New Zealand coastline, bathymetric raster, and depth contours.\n\n\nhide\n\nggplot() +\n  geom_raster(data = nz_df,\n              aes(x = x, y = y, fill = layer)) +\n  geom_contour(data = nz_df,\n               aes(x = x, y = y, z = layer), color = \"grey20\", size = .1) +\n  geom_sf(data = nz_coast,  fill = \"grey40\", \n          color = \"black\", size = .15) +\n  coord_sf(xlim = c(162, 180), ylim = c(-50, -33), expand = c(0, 0)) + \n  labs(fill = \"Depth (m)\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       title = \"Bathymetric Map of New Zealand\",\n       caption = \n\"This map highlights the bathymetric data surrounding the coast of New Zealand.\nNote that the scale on map varies by more than 10%, scale bar may be inaccurate.\nNA bathymetry values are reflected in white and can be seen in shallow coastal areas.\nSource: (A) NOAA National Geophysical Data Center. 2009: ETOPO1 1 Arc-Minute \nGlobal Relief Model. NOAA National Centers for Environmental Information. Accessed [2021-10-01]. \n(B) New Zealand Coastline shapefile downloaded from https://data.linz.govt.nz/data/. Accessed [2021-10-01].\nMap created by Cullen Molitor, Shelby Smith, and Amber McEldowney at the UCSB Bren School [2021-10-04].\") +\n  ggspatial::annotation_scale(location = \"br\") +\n  ggspatial::annotation_north_arrow(which_north=TRUE, location = \"tl\" ) +\n  theme_classic()+\n  theme(panel.border = element_rect(fill = NA, size = 2, color = \"black\"),\n        plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-29-cartography-in-r/cartography.png",
    "last_modified": "2022-05-29T16:39:54+00:00",
    "input_file": {},
    "preview_width": 1721,
    "preview_height": 1774
  }
]
